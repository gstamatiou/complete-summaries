\documentclass[../../../main_math.tex]{subfiles}


\begin{document}
\changecolor{NC}
\begin{multicols}{2}[\section{Numerical calculus}]
  \subsection{Initial value problems}
  \begin{definition}
    An initial-value problem is said to be \emph{well-posed in the Hadamard sense} (or simply \emph{well-posed}) if it has existence and uniqueness of solutions and it has continuos dependence on initial conditions and parameters.
  \end{definition}
  \subsubsection{One-step methods}
  Consider the ivp
  \begin{equation}\label{NC:ivp}
    \left\{
    \begin{aligned}
      \vf{x}'     & = \vf{f}(t,\vf{x}) \\
      \vf{x}(t_0) & = \vf{x}_0
    \end{aligned}
    \right.
  \end{equation}
  For $n\in\NN\cup\{0\}$ let $t_{n+1}:=t_{n}+h$, where $h>0$ is called \emph{step size}. We would like to create a sequence $(\vf{\tilde{x}}_n)$ (\emph{mesh-points}) that approximates (in some sense) $\vf{{x}}_n:=\vf{{x}}(t_n)$ from a first iterate $\vf{\tilde{x}}_0:=\vf{x}_0$. In this section we will describe several algorithms that intend to do so. We will denote $\vf{f}_n:=\vf{f}(t_n,\vf{x}_{n})$ and $\vf{\tilde{f}}_n:=\vf{f}(t_n,\vf{\tilde{x}}_{n})$. Note that solving \mcref{NC:ivp} is equivalent to solve the integral problem:
  $$\vf{x}(t)=\vf{x}_0+\int_0^t\vf{f}(t,\vf{x}(s))\dd{s}$$
  Choosing different numerical-integration methods for approximating this latter integral will lead to different methods for solving the ivp.
  \begin{definition}
    A numerical method is called \emph{explicit} if the $n$-th iterate can be computed directly in terms of some of the previous iterates. A method is called \emph{implicit} if the $n$-th iterate depends implicitly on itself.
  \end{definition}
  \begin{definition}
    A \emph{one-step method} $\vf\Phi$ for the approximation of \mcref{NC:ivp} can be cast in the concise form
    \begin{equation}\label{NC:onestep}
      \vf{\tilde{x}}_{n+1}=\vf\Phi(t_n,\vf{\tilde{x}}_{n},\vf{\tilde{x}}_{n+1},\vf{f},h)=\vf{\tilde{x}}_{n}+h\vf\phi(t_n,\vf{\tilde{x}}_{n},\vf{\tilde{x}}_{n+1},\vf{f},h)
    \end{equation}
    The remarkable fact is that the $n$-th iterate only depends on the previous one.
    The function $\vf\phi$ is called \emph{incremental function}. From here we can define the \emph{local truncation errors} as
    $$\vf\tau_{n}(h)=\frac{\vf{x}_{n+1}-\vf{x}_{n}-h\vf\phi(t_n,\vf{x}_{n},\vf{x}_{n+1},\vf{f},h)}{h}$$
    We define $\tau(h)$ as: $$\tau(h)=\sup_{n\geq 1}\norm{\vf\tau_n(h)}$$
    Finally, we define the \emph{global truncation error} as:
    $$\vf{e}_n=\vf{x}_n-\vf{\tilde{x}}_n$$
    We can also define the interates $\vf{\tilde{x}}_n^{\vf{*}}$ as defined by:
    $$\vf{\tilde{x}}_n^{\vf{*}}=\vf{x}_{n}+h\vf\phi(t_n,\vf{x}_{n},\vf{x}_{n+1},\vf{f},h)$$
  \end{definition}
  \begin{remark}
    In reality in \mcref{NC:onestep} we should add a term of the form $h^{q}\vf{\varepsilon}_nK$ with $K>0$, $q\in\NN$ and $\norm{\vf{\varepsilon}}\leq 1$ on account of the approximation errors due to the float-precision arithmetic. But from here on, we should omit it in order to simplify the notation.
  \end{remark}
  \begin{figure}[H]
    \centering
    \includestandalone[mode=image|tex,width=0.7\linewidth]{Images/errors}
    \caption{Geometrical interpretation of the local and global truncation errors}
    \label{NC:errors_fig}
  \end{figure}
  \begin{definition}[Euler method]\label{NC:euler}
    Consider the ivp of \mcref{NC:ivp}. The \emph{forward Euler method} or \emph{explicit Euler method} is defined as:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+h\vf{\tilde{f}}_n$$
    The \emph{backward Euler method} or \emph{implicit Euler method} is defined as:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+h\vf{\tilde{f}}_{n+1}$$
    Note that the forward method is explicit, whereas the backward method is \emph{implicit}.
  \end{definition}
  \begin{figure}[H]
    \centering
    \includestandalone[mode=image|tex,width=0.7\linewidth]{Images/euler}
    \caption{Explicit Euler method for approximating the ivp $\{x'=x, x(0)=1\}$ with different number of steps.}
    \label{NC:euler_fig}
  \end{figure}
  \begin{definition}[Trapezoidal method]
    Consider the ivp of \mcref{NC:ivp}. The \emph{Trapezoidal method} is defined as:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+\frac{h}{2}\left(\vf{\tilde{f}}_n+\vf{\tilde{f}}_{n+1}\right)$$
  \end{definition}
  \begin{definition}[Heun method]
    Consider the ivp of \mcref{NC:ivp}. The \emph{Heun method} is defined as:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+\frac{h}{2}\left(\vf{\tilde{f}}_n+\vf{f}(t_{n+1},\vf{\tilde{x}}_{n}+h\vf{\tilde{f}}_n)\right)$$
  \end{definition}
  \begin{definition}[Taylor method]
    Consider the ivp of \mcref{NC:ivp} and suppose that $\vf{f}\in\mathcal{C}^r(\RR\times\RR^d)$. The \emph{Taylor method of order $r$} is the method constructed from the Taylor series of the solution $\vf{x}(t)$. Thus the Taylor method of order $r$ is:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+\sum_{k=1}^r\frac{h^k}{k!}\vf{x}_n^{(k)}$$
    We should then substitute each unknown derivative $\vf{x}_n^{(k)}$ a function of $\vf{f}_n$.
    For example the Taylor method of order 2 would be:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+h\vf{\tilde{f}}_n+\frac{h^2}{2}\left(\vf{{f}}_t(t_n,\vf{\tilde{x}}_{n})+\vf{D}_2\vf{f}(\vf{\tilde{f}}_n)\right)$$
    Note that the Taylor method of order 1 is precisely the \mnameref{NC:euler}.
  \end{definition}
  \begin{definition}
    A one-step method for the approximation of \mcref{NC:ivp} is said to be \emph{consistent} if $\displaystyle\lim_{h\to 0}\tau(h)=0$. Moreover, we say that the algorithm has \emph{order of consistency} (\emph{order of accuracy} or simply \emph{order}) $p$ if $\tau(h)=\O{h^p}$.
  \end{definition}
  \begin{definition}
    A one-step method for the approximation of \mcref{NC:ivp} is \emph{convergent} if $$\lim_{h\to 0}\sup_{n\geq 1}\norm{\vf{e}_n}=0$$
    Moreover, we say that the algorithm has \emph{order of convergency} $p$ if $\norm{\vf{e}_n}=\O{h^p}$.
  \end{definition}
  \begin{remark}
    Note that in a consistent method the difference equation for the method approaches the ode as the step size goes to zero, whereas in a convergent method is the solution to the difference equation that approaches the solution to the ode as the step size goes to zero.
  \end{remark}
  \begin{theorem}\label{NC:errorLipschitz}
    Consider a consistent one-step method such that its incremental function $\vf\phi$ is Lipschitz continuous (with constant $L$) with respect to $\vf{x}$. Then:
    $$\norm{\vf{e}_{n+1}}\leq \frac{\exp{L(t_{n+1}-t_0)}-1}{L}\tau(h)$$
  \end{theorem}
  \begin{proof}
    \begin{align*}
      \norm{\vf{e}_{n+1}} & \leq \norm{\vf{x}_{n+1}-\vf{\tilde{x}}_{n+1}^{\vf{*}}}+\norm{\vf{\tilde{x}}_{n+1}^{\vf{*}}-\vf{\tilde{x}}_{n+1}}                  \\
      \begin{split}
        & \leq h\norm{\vf\tau_{n+1}(h)}+\norm{\vf{e}_n}+\\&\hspace{2cm}+h\norm{\vf\phi(t_n,\vf{x}_{n},\vf{f},h)-\vf\phi(t_n,\vf{\tilde{x}}_{n},\vf{{f}},h)}
      \end{split} \\
                          & \leq h\norm{\vf\tau_{n+1}(h)}+(1+hL)\norm{\vf{e}_n}
    \end{align*}
    Iterating the process (note that $\vf{e}_0=\vf{0}$) we have:
    \begin{align*}
      \norm{\vf{e}_{n+1}} & \leq h[1+(1+hL)+\cdots+{(1+hL)}^n]\tau(h)    \\
                          & = \frac{{(1+hL)}^{n+1}-1}{L}\tau(h)          \\
                          & \leq \frac{\exp{L(t_{n+1}-t_0)}-1}{L}\tau(h)
    \end{align*}
    where the last inequality follows from $1+x\leq\exp{x}$.
  \end{proof}
  \begin{corollary}
    Consider a one-step method with order of consistency $p$ such that its incremental function $\vf\phi$ is Lipschitz continuous with respect to $\vf{x}$. Then, if $t_n\leq T$ for a fixed $T\in\RR$, the accuracy of the method has also order $p$.
  \end{corollary}
  \begin{lemma}
    Euler method has order of consistency 1, whereas Heun method has order of consistency 2.
  \end{lemma}
  \begin{proof}
    Using the Taylor series expansion of $\vf{x}(t)$ we have that:
    $$\frac{\vf{x}(t+h)-\vf{x}(t)-h\vf{f}(t,\vf{x})}{h}=\frac{h\vf{x}''(t)}{2}$$
    Hence, Euler method has order 1. For the Heun method we will describe a general procedure for constructing methods of arbitrary order. Let
    \begin{gather*}
      \vf{k}_1=\vf{f}_n\quad \vf{k}_2=\vf{f}(t_n+c_2h,\vf{x}_n+ha_{21}\vf{k}_1)\\
      \vf{x}_{n+1}=\vf{x}_n+h(b_1\vf{k}_1+b_2\vf{k}_2)+\O{h^3}
    \end{gather*}
    Expanding $\vf{k}_2$ we have that:
    $$\vf{k}_2=\vf{f}+c_2h\vf{{f}}_t+a_{21}h\vf{D}_2\vf{f}(\vf{k}_1)+\O{h^2}$$
    So:
    \begin{equation}\label{NC:heun1}
      \vf{x}_{n+1}=\vf{x}_n+(b_1+b_2)h\vf{f}+h^2(b_2c_2\vf{{f}}_t+b_2a_{21}\vf{D}_2\vf{f}(\vf{f}))+\O{h^3}
    \end{equation}
    But from $\vf{x}'=\vf{f}(t,\vf{x})$ we have:
    \begin{equation}\label{NC:heun2}
      \vf{x}_{n+1}=\vf{x}_n+h\vf{f}+\frac{h^2}{2}(\vf{{f}}_t+\vf{D}_2\vf{f}(\vf{f}))+\O{h^3}
    \end{equation}
    Matching coefficients from \mcref{NC:heun1,NC:heun2}, we get the desired result.
  \end{proof}
  \begin{remark}
    For a method of order $s$ (see \mcref{NC:consistencyRK}), just start with $s$ values $\vf{k}_1,\ldots,\vf{k}_s$ of the form: $$\vf{k}_i=\vf{f}(t_n+c_sh,\vf{x}_n+h(a_{s1}\vf{k}_1+\cdots+a_{i(i-1)}\vf{k}_{i-1}))$$
    for $i\geq 2$ and $\vf{k}_1=\vf{f}_n$, and impose:
    $$\vf{x}_{n+1}=\vf{x}_n+h\sum_{i=1}^s b_i\vf{k}_i+\O{h^{s+1}}$$
  \end{remark}
  \subsubsection{Runge-Kutta methods}
  \begin{definition}
    The family of \emph{$s$-stage Runge-Kutta methods} (or \emph{RK methods}) is defined by $$\vf\phi(t,\vf{x},\vf{f},h)=\vf{x}+h\sum_{i=1}^sb_i\vf{k}_i$$
    where the stages $\vf{k}_i\in\RR^d$ are the solutions to the coupled system of (generally nonlinear) equations
    $$\vf{k}_i=\vf{f}(t+c_ih,\vf{x}+h\sum_{j=1}^{s}a_{ij}\vf{k}_j)\quad i=1,\ldots,s$$
    where $c_i:=\sum_{j=1}^{s}a_{ij}$ for $i=1,\ldots,s$. Denoting $\vf{c}=(c_i)$, $\vf{b}=(b_i)$ and $\vf{A}=(a_{ij})$ we can construct the \emph{Butcher tableau}
    \begin{center}
      \renewcommand{\arraystretch}{1.25}
      \begin{tabular}{c|c}
        $\vf{c}$ & $\vf{A}$             \\
        \hline
                 & $\transpose{\vf{b}}$
      \end{tabular}
    \end{center}
    to summarize the information about the method. The method is explicit if $a_{ij}=0$ $\forall j\geq i$. Otherwise it is implicit.
  \end{definition}
  \begin{lemma}\label{NC:consistencyRK}
    A Runge-Kutta method is consistent if and only if $\sum_{i=1}^sb_i=1$. If moreover, $\sum_{i=1}^sb_ic_i=\frac{1}{2}$, then it has order of consistency 2. And if the conditions $\sum_{i=1}^sb_i{c_i}^2=\frac{1}{3}$ and $\sum_{i=1}^sb_i\sum_{j=1}^sa_{ij}c_j=\frac{1}{6}$ are also satisfied, then the consistency is of order 3.
  \end{lemma}
  \begin{proof}
    In the following equations we omit the evaluation at $(t_n,\vf{x}_n)$.
    On the one hand we have:
    \begin{align*}
      \vf{x}'   & =\vf{f}                                                                                                                                         \\
      \vf{x}''  & =\vf{f}_t+\vf{f}_{\vf{x}}\vf{f}=:\vf{F}                                                                                                         \\
      \vf{x}''' & =\vf{f}_{tt}+2\vf{f}_{\vf{x}t}\vf{f}+{\vf{f}_{\vf{xx}}}\vf{f}^2+\vf{f}_{\vf{x}}(\vf{f}_t+{\vf{f}_{\vf{x}}}\vf{f})=:\vf{G}+\vf{f}_{\vf{x}}\vf{F}
    \end{align*}
    Note that here $\vf{f}_{\vf{xx}}\in\mathcal{L}(\RR^d,\mathcal{L}(\RR^d,\RR^d))$. That is, is a vector of matrices. And the vector product ${\vf{f}_{\vf{xx}}}\vf{f}^2$ is do it ${(\vf{f}_{\vf{xx}})}_i\vf{f}$, for each $i=1,\ldots,d$, which result in $d$-column vectors that form a matrix that gets multiplied by $\vf{f}$.
    And on the other hand:
    \begin{align*}
      \begin{split}
        \vf{k}_i & =\vf{f}+c_i h\vf{f}_t+\vf{f}_{\vf{x}}\left(h\sum_{j=1}^{s}a_{ij}\vf{k}_j\right)+\frac{{c_i}^2 h^2}{2}\vf{f}_{tt}+\\
        &+c_i h^2\vf{f}_{\vf{x}t}\left(\sum_{j=1}^{s}a_{ij}\vf{k}_j\right)+\frac{h^2}{2}\vf{f}_{\vf{xx}}{\left(\sum_{j=1}^{s}a_{ij}\vf{k}_j\right)}^2\!+\O{h^3}
      \end{split} \\
       & =\vf{f}+c_i h\vf{F}+\frac{{c_i}^2}{2}h^2\vf{G}+h^2\left(\sum_{j=1}^{s}a_{ij}c_j\right)\vf{f}_{\vf{x}}\vf{F}+\O{h^3}                                       \\
    \end{align*}
    Therefore:
    $$\vf\tau_{n}(h)=\vf{f}+\frac{1}{2}h\vf{F}+\frac{1}{6}h^2\left(\vf{G}+\vf{f}_{\vf{x}}\vf{F}\right)+\O{h^3}-\sum_{i=1}^sb_i\vf{k}_i$$
    Matching coefficients we get the desired result.
  \end{proof}
  \begin{lemma}
    The consistency order $p$ of an $s$-stage Runge-Kutta method is bounded by $p\leq 2 s$. If the Runge-Kutta method is explicit, then $p\leq s$.
  \end{lemma}
  \begin{remark}
    Looking at \mcref{NC:stages-orderRK} we see why the RK4, i.e. the RK method with 4 stages, is so widely known.
  \end{remark}
  \begin{table}[H]
    \centering
    \begin{tabular}{c|cccccccc}
      order & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8  \\
      \hline
      $s$   & 1 & 2 & 3 & 4 & 6 & 7 & 9 & 11
    \end{tabular}
    \caption{Number of stages of an explicit RK method needed for a given order of consistency}
    \label{NC:stages-orderRK}
  \end{table}
  \subsubsection{Step-size control for Runge-Kutta methods}
  \begin{theorem}\label{NC:errorControl}
    Let $\vf{f}:[t_0,t_n]\times\RR^d\rightarrow\RR^d$ be a function of class $\mathcal{C}^{N+1}$ with respect to the second variable and let $\vf{\tilde{x}}(t)$ be the numerical solution to the ivp \mcref{NC:ivp} obtained by a one-step method of order $p\leq N$ with step-size $h$. Then, $\vf{\tilde{x}}(t)$ has an asymptotic expansion of:
    $$\vf{\tilde{x}}(t)=\vf{x}(t)+\vf{e}_p(t)h^p+\cdots+\vf{e}_N(t)h^N+E_{N+1}(t,h)h^{N+1}$$
    with $\vf{e}_k(t_0)=0$ $\forall k\geq p$. This is valid $\forall t\in[t_0,t_n]$ and all $h>0$. Moreover the functions $\vf{e}_k$ are differentiable and independent of $h$ and $\norm{E_{N+1}(t,\cdot)}_\infty<\infty$ $\forall t\in[t_0,t_n]$.
  \end{theorem}
  % \begin{proof}
  %   In order to simplify the notation, we will only prove it for $N=p+1$. Let $\vf{\phi}$ be the incremental function. Since the method is of order $p\leq N$, we have:
  %   $$\vf{x}(t+h)-\vf{x}(t)-h\vf\phi(t,\vf{x},\vf{f},h)=\vf{d}_{p+1}h^{p+1}+\O{h^{p+2}}$$
  %   We will show that there is a differentiable function $\vf{e}_{p}$ such that:
  %   $$\vf{x}(t+h)-\vf{x}(t)-h\vf\phi(t,\vf{x},\vf{f},h)=\vf{e}_{p}(t)h^{p}+\O{h^{p+1}}$$
  %   Consider $\vf{\hat{x}}(t):=\vf{\tilde{x}}(t)-\vf{e}_p(t)h^p$ where the choice of $\vf{e}_p$ is still left open. It is easy to see that $\vf{\hat{x}}(t)$ can be chosen as the result of another one-step method: $$\vf{\hat{x}}(t+h)=\vf{\hat{x}}(t)+h\vf{\hat\phi}(t,\vf{\hat{x}},\vf{f},h)$$
  %   where: $$\vf{\hat\phi}(t,\vf{{x}},\vf{f},h)=\vf{\phi}(t,\vf{{x}}+\vf{e}_p(t)h^p,\vf{f},h)-(\vf{e}_p(t+h)-\vf{e}_p(t))h^{p-1}$$
  %   Taylor expanding to find the local truncation error of this new method:
  %   \begin{multline*}
  %     \vf{x}(t+h)-\vf{x}(t)-h\vf{\hat\phi}(t,\vf{x},\vf{f},h)=\\
  %     =(\vf{d}_{p+1}-\vf{f}_{\vf{x}}\vf{e}_p(t)-{\vf{e}_p}'(t))h^{p+1}+\O{h^{p+2}}
  %   \end{multline*}
  %   So if we take $\vf{e}_p$ to be the solution of the ivp:
  %   $$\begin{cases}
  %       {\vf{e}_p}'=\vf{d}_{p+1}-\vf{f}_{\vf{x}}\vf{e}_p(t) \\
  %       \vf{e}_p(t_0)=0
  %     \end{cases}$$
  %   Then we have that this new method is of order $p+1$ and by \mcref{NC:errorLipschitz} we have that:
  %   $$\vf{\hat{x}}(t)-\vf{x}(t)=\vf{\tilde{x}}(t)-\vf{x}(t)-\vf{e}_p(t)h^p=\O{h^{p+1}}$$
  %   A repetition of these arguments with  $\vf{\hat\phi}$ in place of $\vf{\phi}$ completes the proof.
  % \end{proof}
  \begin{theorem}[Richardson extrapolation]
    Consider the ivp of \mcref{NC:ivp} and let $\vf{\tilde{x}}(t;h)$ be the numerical solution obtained by a one-step method of order $p\leq N$ with step-size $h$. Then:
    $$\vf{x}(t)=\vf{\tilde{x}}(t;h/2)-\frac{\vf{\tilde{x}}(t;h)-\vf{\tilde{x}}(t;h/2)}{2^p-1}+\O{h^{p+1}}$$
  \end{theorem}
  \begin{proof}
    By \mcref{NC:errorControl} we have that:
    \begin{align*}
      \vf{x}(t;h)   & =\vf{x}(t)+\vf{e}_p(t)h^p+\O{h^{p+1}}                          \\
      \vf{x}(t;h/2) & =\vf{x}(t)+\vf{e}_p(t){\left(\frac{h}{2}\right)}^p+\O{h^{p+1}}
    \end{align*}
    Substracting the two equations we have:
    $$\vf{e}_p(t){\left(\frac{h}{2}\right)}^p=\frac{\vf{\tilde{x}}(t;h)-\vf{\tilde{x}}(t;h/2)}{2^p-1}+\O{h^{p+1}}$$
  \end{proof}
  \begin{theorem}[Runge-Kutta-Fehlberg method]
    Consider two RK methods of orders $p$ and $p+1$ with incremental functions $\vf{\hat\phi}$ and $\vf{\phi}$ respectively such that their Butcher tableaus have the same $(a_{ij})$ coefficients (and therefore the same $(c_{ij})$ coefficients):
    \begin{center}
      \renewcommand{\arraystretch}{1.25}
      \begin{tabular}{c|c}
        $\vf{c}$ & $\vf{A}$                   \\
        \hline
                 & $\transpose{\vf{b}}$       \\
                 & $\transpose{\vf{\hat{b}}}$
      \end{tabular}
    \end{center}
    These methods $\vf{\phi}$ and $\vf{\hat\phi}$ are called \emph{embedded methods}\footnote{Usually the notation $\mathrm{RK}p(q)s$ is used to refer for a method of order $p$ with an embedded method of order $q< p$ and a total of $s$ stages.}. Denote by $\vf{\tilde{x}}$ and $\vf{\hat{x}}$ the numerical solutions using the respective incremental functions.
    Then, given a tolerance $\epsilon$ and a older step-size $h$, we would like to choose a new step size $h_\mathrm{new}$ for which our approximate solutions differ no more than $\epsilon$ between them. At the time $t_n$ we have:
    \begin{align*}
      \vf{\tilde{x}}_{n+1} & =\vf{\tilde{x}}_n+h\vf{\phi}(t_n,\vf{\tilde{x}}_n,\vf{f},h)   \\
      \vf{\hat{x}}_{n+1}   & =\vf{\tilde{x}}_n+h\vf{\hat\phi}(t_n,\vf{\hat{x}}_n,\vf{f},h)
    \end{align*}
    To obtain this we have to choose the new step-size
    \begin{equation}\label{NC:RFF}
      h_{\mathrm{new}}\simeq h\sqrt[p+1]{\frac{\epsilon}{\norm{\vf{\hat{x}}(t_n+h)-\vf{\tilde{x}}(t_n+h)}}}
    \end{equation}
    If this new step-size was not successful, i.e. we have $$\norm{\vf{\hat{x}}(t_{n+1}+h_\mathrm{new})-\vf{\tilde{x}}(t_{n+1}+h_\mathrm{new})}>\epsilon$$ we will have to repeat the last step with an another step-size $h_{\mathrm{new}}^*<h_{\mathrm{new}}$.
  \end{theorem}
  \begin{proof}
    We assume that the $n$-th iteration was successful, i.e.:
    $$\norm{\vf{\hat{x}}(t_n+h)-\vf{\tilde{x}}(t_n+h)}\leq\epsilon$$
    From the hypothesis and \mcref{NC:errorControl} we have:
    $$\norm{\vf{\hat{x}}(t_n+h)-\vf{\tilde{x}}(t_n+h)}\leq \vf{c}(\vf{x}(t_n))h^{p+1}$$
    Moreover, up to errors of first order, we have $\vf{c}(\vf{x}(t_n))\approx\vf{c}(\vf{x}(t_{n}+h))$. Finally imposing
    $$\norm{\vf{c}(\vf{x}(t_{n}+h)){h_\mathrm{new}}^{p+1}}\lesssim\epsilon$$
    yields to:
    $$\norm{\vf{\hat{x}}(t_n+h)-\vf{\tilde{x}}(t_n+h)}{\left(\frac{h_{\mathrm{new}}}{h}\right)}^{p+1}\lesssim\epsilon$$
  \end{proof}
  \begin{remark}
    Note that there exist RK embedded methods by the following argument. Start with a RK method of order $p+1$ that has $s$ stages. Then we can construct a RK method of order $p$ with $s$ stages by copying the coefficients $\vf{A}$ and $\vf{c}$ and adjusting the coefficients $\vf{b}$ in a proper way to make it ``less'' consistent.
  \end{remark}
  \begin{remark}
    In practise in order to avoid many unsuccessful steps, instead of the new step in \mcref{NC:RFF} we use the following:
    $$h_{\mathrm{new}}\simeq\alpha h\sqrt[p+1]{\frac{\epsilon}{\norm{\vf{\hat{x}}(t_n+h)-\vf{\tilde{x}}(t_n+h)}}}$$
    with $\alpha\simeq 0.9$.
    Furthermore, in order to avoid rapid oscillations of the stepsize, $h$ should not, however, be changed by more than a factor of 2 to 5 from one step to the next.
  \end{remark}
  \subsubsection{Stability of Runge-Kutta methods}
  \begin{definition}
    Consider a RK method applied to the ivp $y'=\lambda y$. We can express it as:
    $$
      \tilde{y}_{n+1}=g(h\lambda)\tilde{y}_n
    $$
    for some function $g:\RR\rightarrow\RR$. This function is called \emph{stability function}. Given $h$ and $\lambda$, the method is said to be \emph{stable} if $\abs{g(h\lambda)}\leq 1$ and \emph{absolutely stable} if $\abs{g(h\lambda)}< 1$. Given $\lambda<0$, the method is said to be \emph{conditionally absolutely stable} for a fixed $h$ if there exists a function $\Lambda(\lambda):\CC\to\RR_{>0}$ such that $g(h\lambda)<1\iff h<\Lambda(\lambda)$. If the condition $g(h \lambda)<1$ holds $\forall h\in\RR_{>0}$, we say that the method is \emph{unconditionally absolutely stable}. If $\abs{g(h\lambda)}> 1$, the method is said to be \emph{unstable}.
  \end{definition}
  \begin{definition}
    Consider a RK method with stability function $g$. We define the \emph{stability region} of the method as the set:
    $$
      \mathcal{A}:=\{z\in\CC:\abs{g(z)}< 1\}
    $$
    We say that the method is \emph{A-stable} if $\{\Re(z)<0\}\subseteq \mathcal{A}$.
  \end{definition}
  \begin{remark}
    The motivation behind this definition of stability is on the \emph{stiff equations}, which are differential equations for which certain numerical methods for solving the equation are numerically unstable, unless the step size is taken to be extremely small. A-stable methods do not exhibit the instability problems.
  \end{remark}
  \begin{theorem}[Lax theorem]
    Consider a RK consistent and stable method. Then, the method is convergent.
  \end{theorem}
  \begin{proof}
    We will prove it only for the \emph{test problem} $y'=\lambda y$. We have:
    \begin{multline*}
      e_{n+1}={y}_{n+1}-{\tilde{y}}_{n+1}={y}_{n+1}-g(h\lambda){\tilde{y}}_n=\\
      ={y}_{n+1}-g(h\lambda)({y}_n-e_n)=h{\tau}_n(h)+g(h\lambda)e_n
    \end{multline*}
    Iterating the proces and taking norms we have:
    $$
      \abs{{e}_n}\leq\sum_{j=0}^n{\abs{g(h\lambda)}}^jh\abs{{\tau}_{n-j}(h)}\leq nh\tau(h)\overset{h\to 0}{\longrightarrow}0
    $$
    because $nh=t_n-t_0$ is bounded.
  \end{proof}
  \subsubsection{Multistep method}
  \begin{definition}
    A $k$-step method is a method that uses the previous $k$ steps to compute the next step.
  \end{definition}
  \begin{definition}[Linear multistep method]
    Consider the ivp of \mcref{NC:ivp}. A \emph{linear $k$-step method} is a method of the form:
    \begin{equation}\label{NC:multistep}
      \sum_{j=0}^k\alpha_j\vf{\tilde{y}}_{n+j}=h\sum_{j=0}^{k}\beta_j\vf{\tilde{f}}_{n+j}
    \end{equation}
    that computes the $(n+k)$-th iterate from the previous $k$ iterates. Here $\alpha_j,\beta_j\in\mathbb{R}$ are such that $\alpha_k\ne 0$ and ${\alpha_0}^2+{\beta_0}^2\ne 0$. Observe that we need $k$ initial values in order to use the method. Finally note that if $\beta_k=0$ then the method is explicit.
  \end{definition}
  \begin{remark}
    Since in practise we only have one initial value, we can use a one-step method to compute the first $k$ iterates and then use the $k$-step method.
  \end{remark}
  \begin{definition}
    Consider the multistep method of \mcref{NC:multistep}. We define the \emph{first and second characteristic polynomials} of the method as:
    \begin{gather*}
      \rho(z) =\sum_{j=0}^k\alpha_jz^j                  \qquad
      \sigma (z) =\sum_{j=0}^k\beta_jz^j
    \end{gather*}
  \end{definition}
  \begin{definition}
    A linear $k$-step method is said to be \emph{zero-stable} if there is a constant $C>0$ such that for every $N\in\NN$ sufficiently large and for any two different sets of initial data $\vf{\tilde{y}}_0,\ldots,\vf{\tilde{y}}_{k-1}$ and $\vf{\hat{y}}_0,\ldots,\vf{\hat{y}}_{k-1}$, the two respective sequences $(\vf{\tilde{y}}_n)_{0\leq n\leq N}$ and $(\vf{\hat{y}}_n)_{0\leq n\leq N}$ of iterates satisfy
    $$\max_{0\leq n\leq N}\norm{\vf{\tilde{y}}_n-\vf{\hat{y}}_n}\leq C\max_{0\leq n\leq k-1}\norm{\vf{\tilde{y}}_n-\vf{\hat{y}}_n}$$
    as $h\to 0$.
  \end{definition}
  \begin{definition}
    A linear $k$-step method satisfies the \emph{root condition} if all zeros of its first characteristic polynomial $\rho(z)$ lie inside the closed unit disc, and every zero that lies on the unit circle is simple.
  \end{definition}
  \begin{theorem}
    Consider the ivp of \mcref{NC:ivp} and suppose that $\vf{f}$ is Lipschitz continuous. Then, a linear $k$-step method is zero-stable if and only if it satisfies the root condition.
  \end{theorem}
  \begin{remark}
    This theorem implies that zero-stability of a multi-step method can be determined by merely considering its behavior when applied to the trivial differential equation $y'=0$. It is for this reason that it is called \textit{zero}-stability.
  \end{remark}
  \begin{definition}[Adams method]
    Consider the ivp of \mcref{NC:ivp}. The \emph{Adams method} is a linear multistep method of the form:
    \begin{equation}\label{NC:Adams}
      \vf{\tilde{y}}_{n+k}=\vf{\tilde{y}}_{n+k-1}+h\sum_{j=0}^{k}\beta_j\vf{\tilde{f}}_{n+j}
    \end{equation}
    If $\beta_{k}=0$ then the method is explicit and it is called \emph{Adams-Bashforth method}. If $\beta_{k}\ne 0$ then the method is implicit and it is called \emph{Adams-Moulton method}.

    The coefficients $\beta_j$ are found by integrating the Lagrange interpolating polynomial between $t_{n+k-1}$ and $t_{n+k}$ constructed from the nodes $(t_{n+j},\vf{\tilde{f}}_{n+j})$ for $j=0,\ldots,k-1$, for the Adams-Bashforth method, and for $j=0,\ldots,k$ for the Adams-Moulton method. That is, the respective incremental functions are given by:
    \begin{align*}
      \vf\phi_\mathrm{AB} & =\int_{t_{n+k-1}}^{t_{n+k}}\sum_{j=0}^{k-1}\vf{f}_{n+j}\prod_{\substack{i=0 \\i\ne j}}^{k-1}\frac{t-t_{n+i}}{t_{n+j}-t_{n+i}}\dd{t}\\
      \vf\phi_\mathrm{AM} & =\int_{t_{n+k-1}}^{t_{n+k}}\sum_{j=0}^{k}\vf{f}_{n+j}\prod_{\substack{i=0   \\i\ne j}}^{k}\frac{t-t_{n+i}}{t_{n+j}-t_{n+i}}\dd{t}
    \end{align*}
    In the following table we expose the first three Adams' incremental functions for the explicit and implicit methods:
    \begin{center}
      \renewcommand{\arraystretch}{1.7}
      \begin{tabular}[ht]{c|c|c}
        $k$ & Adams-Bashforth                                                   & Adams-Moulton                                                  \\
        \hline
        1   & $\vf{f}_n$                                                        & $\vf{f}_{n+1}$                                                 \\
        2   & $\displaystyle\frac{3\vf{f}_{n+1}-\vf{f}_n}{2}$                   & $\displaystyle\frac{\vf{f}_{n+1}+\vf{f}_n}{2}$                 \\
        3   & $\displaystyle\frac{23\vf{f}_{n+1}-16\vf{f}_n+5\vf{f}_{n-1}}{12}$ & $\displaystyle\frac{5\vf{f}_{n+2}+8\vf{f}_{n+1}-\vf{f}_n}{12}$ \\
      \end{tabular}
    \end{center}
  \end{definition}
  \begin{definition}
    Consider the $k$-step method of \mcref{NC:multistep}. We define the \emph{local truncation error} of the method as:
    $$
      \vf\tau_n(h)=\frac{\sum_{j=0}^k[\alpha_j\vf{y}_{n+j}-h\beta_j\vf{{f}}_{n+j}]}{h}
    $$
    We define $\tau(h)$ as:
    $$
      \tau(h)=\sup_{n\geq 1}\norm{\vf\tau_n(h)}
    $$
    We say that the method is \emph{consistent} if $\displaystyle\lim_{h\to 0}\tau(h)=0$. Moreover, we say that the algorithm has \emph{order of consistency} or \emph{order of accuracy} $p$ if $\tau(h)=\O{h^p}$.
  \end{definition}
  \begin{remark}
    The global error of the method and the convergency of it are the same as in the one-step case.
  \end{remark}
  \begin{proposition}
    The Adams-Bashforth $k$-step method has order of consistency $k$, whereas the Adams-Moulton method has order of consistency $k+1$.
  \end{proposition}
  \begin{theorem}
    A necessary condition for the convergence of the linear multi-step method of \mcref{NC:multistep} is that it has to be zero-stable and consistent.
  \end{theorem}
  \begin{theorem}[Dahlquist's theorem]
    Let $\vf{f}$ be a Lipschitz continuous function and consider the multistep method of \mcref{NC:multistep}. Suppose the method is consistent. Then, the method is zero-stable if and only if it is convergent. Moreover if the solution $\vf{y}$ is of class $\mathcal{C}^{p+1}$ and the consistency error is $\O{h^p}$, then the global error is $\O{h^{p}}$.
  \end{theorem}
  \subsection{Non-linear systems of equations}
  \begin{definition}[Newton method]
    Let $\vf{F}:\RR^n\rightarrow\RR^n$ be a differentiable field. We would like to find the solutions of $\vf{F}(\vf{x})=\vf{0}$.
    The \emph{Newton method} is a recurrence of the form:
    \begin{equation*}
      \vf{x}_{n+1}=\vf{x}_n-\vf{DF}^{-1}(\vf{x}_n)\vf{F}(\vf{x}_n)
    \end{equation*}
    which starts with an initial guess $\vf{x}_0$.
    Rather than actually computing the inverse of the Jacobian matrix, one may save time and increase numerical stability by solving the system of linear equations
    $$
      \vf{DF}(\vf{x}_n)(\vf{x}_{n+1}-\vf{x}_n)=-\vf{F}(\vf{x}_n)
    $$
    for the unknown $\vf{x}_{n+1}-\vf{x}_n$.
  \end{definition}
  \begin{remark}
    Computing the Jacobian is a difficult and expensive operation. Broyden proposed a method that compute the whole Jacobian only at the first iteration and to do rank-one updates at other iterations.
  \end{remark}
  \begin{definition}[Broyden's method]
    Let $\vf{F}:\RR^n\rightarrow\RR^n$. The \emph{Broyden's method} is a recurrence for the Jacobian matrix of the form:
    $$
      \vf{DF}_{n+1}=\vf{DF}_n+\frac{\vf{F}_n-\vf{DF}_n\Delta\vf{x}_n}{\norm{\Delta\vf{x}_n}^2}\transpose{(\Delta\vf{x}_n)}
    $$
    where $\Delta\vf{x}_n=\vf{x}_{n+1}-\vf{x}_n$ and $\vf{F}_n=\vf{F}(\vf{x}_n)$. We then proceed with the Newton method:
    $$
      \vf{x}_{n+1}=\vf{x}_n-{\vf{DF}_{n}}^{-1}\vf{F}_n
    $$
    A variant to the Broyen's method for computing directly the inverse of the Jacobian matrix is:
    $$
      {\vf{DF}_{n+1}}^{-1}\!={\vf{DF}_n}^{-1}+\frac{\Delta\vf{x}_n-{\vf{DF}_n}^{-1}\Delta\vf{F}_n}{\transpose{(\Delta\vf{x}_n)}{\vf{DF}_n}^{-1}\Delta\vf{F}_n}\transpose{(\Delta\vf{x}_n)}{\vf{DF}_n}^{-1}
    $$
    where $\Delta\vf{F}_n=\vf{F}_{n+1}-\vf{F}_n$.
  \end{definition}
  \subsubsection{Boundary value problems}
  \begin{definition}[Shooting method]
    Suppose we want to solve the boundary value problem:
    \begin{equation*}
      \begin{cases}
        x'' =f(t, x, x') \\
        x(t_0)=\alpha    \\
        x(t_f)=\beta
      \end{cases}
    \end{equation*}
    Let $x(t;s)$ be the solution to the ivp:
    \begin{equation*}
      \begin{cases}
        x'' =f(t, x, x') \\
        x(t_0)=\alpha    \\
        x'(t_0)=s
      \end{cases}
    \end{equation*}
    If $x(t_1;s)=\beta$, $x(t,s)$ is also a solution to the boundary value problem. The \emph{shooting method} is the process of solving the initial value problem for many different values of $a$ until one finds the solution $x(t;s)$ that satisfies the desired boundary conditions. That is, the solutions correspond to roots of:
    $$
      F(s)=x(t_1;s)-\beta
    $$
  \end{definition}
  \begin{remark}
    Given to initial guesses $\tilde\beta_0$ and $\tilde\beta_1$, we can use interpolation with nodes $(\tilde\beta_0, F(\tilde\beta_0))$ and $(\tilde\beta_1, F(\tilde\beta_1))$, find the root of the interpolating polynomial and use it as the new guess. Alternatively, one can use the Newton's method.
  \end{remark}
  %%% It's missing the theorem of convergence of the Newton method
  %%% It's missing the minimization of functions
\end{multicols}
\end{document}