\documentclass[../../../main_math.tex]{subfiles}


\begin{document}
\changecolor{NC}
\begin{multicols}{2}[\section{Numerical calculus}]
  \subsection{Ordinary differential equations}
  \begin{definition}
    An initial-value problem is said to be \emph{well-posed} if it has existence and uniqueness of solutions and it has continuos dependence on initial conditions and parameters.
  \end{definition}
  \subsubsection{One-step methods}
  Consider the ivp
  \begin{equation}\label{NC:ivp}
    \left\{
    \begin{aligned}
      \vf{x}'     & = \vf{f}(t,\vf{x}) \\
      \vf{x}(t_0) & = \vf{x}_0
    \end{aligned}
    \right.
  \end{equation}
  For $n\in\NN\cup\{0\}$ let $t_{n+1}:=t_{n}+h$, where $h>0$ is called \emph{step size}. We would like to create a sequence $(\vf{\tilde{x}}_n)$ that approximates (in some sense) $\vf{{x}}_n:=\vf{{x}}(t_n)$ from a first iterate $\vf{\tilde{x}}_0:=\vf{x}_0$. In this section we will describe several algorithms that intend to do so. We will denote $\vf{f}_n:=\vf{f}(t_n,\vf{x}_{n})$ and $\vf{\tilde{f}}_n:=\vf{f}(t_n,\vf{\tilde{x}}_{n})$. Note that solving \mcref{NC:ivp} is equivalent to solve the integral problem:
  $$\vf{x}(t)=\vf{x}_0+\int_0^t\vf{f}(t,\vf{x}(s))\dd{s}$$
  Choosing a different numerical integration methods for approximating this latter integral will lead to different methods for solving the ivp.
  \begin{definition}
    A numerical method is called \emph{explicit} if the $i$-th iterate can be computed directly in terms of some of the previous iterates. A method is called \emph{implicit} if the $i$-th iterate depends implicitly on itself.
  \end{definition}
  \begin{definition}
    A \emph{one-step explicit method} $\vf\Phi$ for the approximation of \mcref{NC:ivp} can be cast in the concise form $$\vf{\tilde{x}}_{n+1}=\vf\Phi(t_n,\vf{\tilde{x}}_{n},\vf{f},h)=\vf{\tilde{x}}_{n}+h\vf\phi(t_n,\vf{\tilde{x}}_{n},\vf{f},h)$$
    The function $\vf\phi$ is called \emph{incremental function}. From here we can define the \emph{residuals} $\vf\varepsilon$ as
    $$\vf{x}_{n+1}=\vf{x}_{n}+h\vf\phi(t_n,\vf{x}_{n},\vf{f},h)+\vf\varepsilon_{n+1}$$
    and the \emph{local truncation errors} as $h\vf\tau_{n}(h)=\vf\varepsilon_n$. We define $\tau(h)$ as: $$\tau(h)=\max_{n\geq 1}\norm{\vf\tau_n(h)}$$
    Finally, we define the \emph{global truncation error} as:
    $$\vf{e}_n=\vf{x}_n-\vf{\tilde{x}}_n$$
    We can also define the interates $\vf{\tilde{x}^*}_{n}$ as defined by:
    $$\vf{\tilde{x}^*}_{n}=\vf{x}_{n}+h\vf\phi(t_n,\vf{x}_{n},\vf{f},h)$$
  \end{definition}
  \begin{figure}[H]
    \centering
    \includestandalone[mode=image|tex,width=0.7\linewidth]{Images/errors}
    \caption{Geometrical interpretation of the local and global truncation errors}
    \label{NC:errors_fig}
  \end{figure}
  \begin{definition}[Euler method]\label{NC:euler}
    Consider the ivp of \mcref{NC:ivp}. The \emph{forward Euler method} is defined as:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+h\vf{\tilde{f}}_n$$
    The \emph{backward Euler method} is defined as:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+h\vf{\tilde{f}}_{n+1}$$
    Note that the forward method is explicit, whereas the backward method is \emph{implicit}.
  \end{definition}
  \begin{figure}[H]
    \centering
    \includestandalone[mode=image|tex,width=0.7\linewidth]{Images/euler}
    \caption{Euler method for approximating the ivp $\{x'=x, x(0)=1\}$ with different number of steps.}
    \label{NC:euler_fig}
  \end{figure}
  \begin{definition}[Trapezoidal method]
    Consider the ivp of \mcref{NC:ivp}. The \emph{Trapezoidal method} is defined as:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+\frac{h}{2}\left(\vf{\tilde{f}}_n+\vf{\tilde{f}}_{n+1}\right)$$
  \end{definition}
  \begin{definition}[Heun method]
    Consider the ivp of \mcref{NC:ivp}. The \emph{Heun method} is defined as:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+\frac{h}{2}\left(\vf{\tilde{f}}_n+\vf{f}(t_{n+1},\vf{\tilde{x}}_{n}+h\vf{\tilde{f}}_n)\right)$$
  \end{definition}
  \begin{definition}[Taylor method]
    Consider the ivp of \mcref{NC:ivp} and suppose that $\vf{f}\in\mathcal{C}^r(\RR\times\RR^d)$. The \emph{Taylor method of order $r$} is the method constructed from the Taylor series of the solution $\vf{x}(t)$. For the sake of simplicity, suppose that $\vf{x}=x$ is univalued. Thus the Taylor method of order $r$ is::
    $$\tilde{x}_{n+1}=\tilde{x}_{n}+\sum_{k=1}^r\frac{h^k}{k!}(\vf{D}^k{x})(t_n,\tilde{x}_{n})$$
    For example the Taylor method of order 2 would be:
    $$\vf{\tilde{x}}_{n+1}=\vf{\tilde{x}}_{n}+h\vf{\tilde{f}}_n+\frac{h^2}{2}\left(\vf{{f}}_t(t_n,\vf{\tilde{x}}_{n})+\vf{D}_2\vf{f}(\vf{\tilde{f}}_n)\right)$$
    Note that the Taylor method of order 1 is precisely the \mnameref{NC:euler}.
  \end{definition}
  \begin{definition}
    A numerical method is \emph{consistent} if $\displaystyle\lim_{h\to 0}\tau(h)=0$. Moreover, we say that the algorithm has \emph{order of consistency} $p$ if $\tau(h)=\O{h^p}$.
  \end{definition}
  \begin{definition}
    A one-step method for the approximation of \mcref{NC:ivp} is \emph{convergent} if $$\lim_{h\to 0}\max_{n\geq 1}\norm{\vf{e}_n}=0$$
    Moreover, we say that the algorithm has \emph{order of accuracy} $p$ if $\norm{\vf{e}_n}=\O{h^p}$.
  \end{definition}
  \begin{remark}
    Note that in a consistent method the difference equation for the method approaches the ode as the step size goes to zero, whereas in a convergent method is the solution to the difference equation that approaches the solution to the ode as the step size goes to zero.
  \end{remark}
  \begin{theorem}\label{NC:errorLipschitz}
    Consider a consistent one-step method such that its incremental function $\vf\phi$ is Lipschitz continuous (with constant $L$) with respect to $\vf{x}$. Then:
    $$\norm{\vf{e}_{n+1}}\leq \frac{\exp{L(t_{n+1}-t_0)}-1}{L}\tau(h)$$
  \end{theorem}
  \begin{proof}
    \begin{align*}
      \norm{\vf{e}_{n+1}} & \leq \norm{\vf{x}_{n+1}-\vf{\tilde{x}^*}_{n+1}}+\norm{\vf{\tilde{x}^*}_{n+1}-\vf{\tilde{x}}_{n+1}}                                \\
      \begin{split}
        & \leq h\norm{\vf\tau_{n+1}(h)}+\norm{\vf{e}_n}+\\&\hspace{2cm}+h\norm{\vf\phi(t_n,\vf{x}_{n},\vf{f},h)-\vf\phi(t_n,\vf{\tilde{x}}_{n},\vf{{f}},h)}
      \end{split} \\
                          & \leq h\norm{\vf\tau_{n+1}(h)}+(1+hL)\norm{\vf{e}_n}
    \end{align*}
    Iterating the process (note that $\vf{e}_0=\vf{0}$) we have:
    \begin{align*}
      \norm{\vf{e}_{n+1}} & \leq h[1+(1+hL)+\cdots+{(1+hL)}^n]\tau(h)    \\
                          & = \frac{{(1+hL)}^{n+1}-1}{L}\tau(h)          \\
                          & \leq \frac{\exp{L(t_{n+1}-t_0)}-1}{L}\tau(h)
    \end{align*}
    where in the last inequality we have used that $1+x\leq\exp{x}$.
  \end{proof}
  \begin{corollary}
    Consider a one-step method with order of consistency $p$ such that their incremental functions $\vf\phi$ are Lipschitz continuous with respect to $\vf{x}$. Then its accuracy has also order $p$.
  \end{corollary}
  \begin{lemma}
    Euler method has order 1, whereas Heun method has order 2.
  \end{lemma}
  \begin{proof}
    Using the Taylor series expansion of $\vf{x}(t)$ we have that:
    $$\vf{x}(t+h) = \vf{x}(t)+h\vf{f}(t,\vf{x}) + \O{h^2}$$
    Hence, Euler method has order 1. For the Heun method we will describe a general procedure for constructing methods of arbitrary order. Let
    \begin{gather*}
      \vf{k}_1=\vf{f}_n\quad \vf{k}_2=\vf{f}(t_n+c_2h,\vf{x}_n+ha_{21}\vf{k}_1)\\
      \vf{x}_{n+1}=\vf{x}_n+h(b_1\vf{k}_1+b_2\vf{k}_2)+\O{h^3}
    \end{gather*}
    Expanding $\vf{k}_2$ we have that:
    $$\vf{k}_2=\vf{f}+c_2h\vf{{f}}_t+a_{21}h\vf{D}_2\vf{f}(\vf{k}_1)+\O{h^2}$$
    So:
    \begin{equation}\label{NC:heun1}
      \vf{x}_{n+1}=\vf{x}_n+(b_1+b_2)h\vf{f}+h^2(b_2c_2\vf{{f}}_t+b_2a_{21}\vf{D}_2\vf{f}(\vf{f}))+\O{h^3}
    \end{equation}
    But the $\vf{x}'=\vf{f}(t,\vf{x})$ we have:
    \begin{equation}\label{NC:heun2}
      \vf{x}_{n+1}=\vf{x}_n+h\vf{f}+\frac{h^2}{2}(\vf{{f}}_t+\vf{D}_2\vf{f}(\vf{f}))+\O{h^3}
    \end{equation}
    Matching coefficients from \mcref{NC:heun1,NC:heun2}, we get the desired result.
  \end{proof}
  \begin{remark}
    For a method of order $s$ (see \mcref{NC:consistencyRK}), just start with $s$ values $\vf{k}_1,\ldots,\vf{k}_s$ of the form: $$\vf{k}_i=\vf{f}(t_n+c_sh,\vf{x}_n+h(a_{s1}\vf{k}_1+\cdots+a_{i(i-1)}\vf{k}_{i-1}))$$
    for $i\geq 2$ and $\vf{k}_1=\vf{f}_n$, and impose:
    $$\vf{x}_{n+1}=\vf{x}_n+h\sum_{i=1}^s b_i\vf{k}_i+\O{h^{s+1}}$$
  \end{remark}
  \subsubsection{Runge-Kutta methods}
  \begin{definition}
    The family of \emph{$s$-stage Runge-Kutta} (or \emph{RK}) methods is defined by $$\vf\phi(t,\vf{x},\vf{f},h)=\vf{x}+h\sum_{i=1}^sb_i\vf{k}_i$$
    where the stages $\vf{k}_i\in\RR^d$ are the solutions to the coupled system of (generally nonlinear) equations
    $$\vf{k}_i=\vf{f}(t+c_ih,\vf{x}+h\sum_{j=1}^{s}a_{ij}\vf{k}_j)\quad i=1,\ldots,s$$
    where $c_i:=\sum_{j=1}^{s}a_{ij}$ for $i=1,\ldots,s$. Denoting $\vf{c}=(c_i)$, $\vf{b}=(b_i)$ and $\vf{A}=(a_{ij})$ we can construct the \emph{Butcher tableau}:
    \begin{center}
      \renewcommand{\arraystretch}{1.25}
      \begin{tabular}{c|c}
        $\vf{c}$ & $\vf{A}$             \\
        \hline
                 & $\transpose{\vf{b}}$
      \end{tabular}
    \end{center}
  \end{definition}
  \begin{lemma}\label{NC:consistencyRK}
    A Runge-Kutta method is consistent if and only if $\sum_{i=1}^sb_i=1$. If moreover, $\sum_{i=1}^sb_ic_i=\frac{1}{2}$, then it has order 2 consistency. And if the conditions $\sum_{i=1}^sb_i{c_i}^2=\frac{1}{3}$ and $\sum_{i=1}^sb_i\sum_{j=1}^sa_{ij}c_j=\frac{1}{6}$ are also satisfied, then the consistency is of order 3.
  \end{lemma}
  \begin{proof}
    In the following equations we omit the evaluation at $(t_n,\vf{x}_n)$.
    On the one hand we have:
    \begin{align*}
      \vf{x}'   & =\vf{f}                                                                                                                                       \\
      \vf{x}''  & =\vf{f}_t+\vf{f}_{\vf{x}}\vf{f}=:\vf{F}                                                                                                       \\
      \vf{x}''' & =\vf{f}_{tt}+2\vf{f}_{\vf{x}t}\vf{f}+{\vf{f}_{\vf{xx}}}\vf{f}+\vf{f}_{\vf{x}}(\vf{f}_t+{\vf{f}_{\vf{x}}}\vf{f})=:\vf{G}+\vf{f}_{\vf{x}}\vf{F}
    \end{align*}
    And on the other hand:
    \begin{align*}
      \begin{split}
        \vf{k}_i & =\vf{f}+c_i h\vf{f}_t+\vf{f}_{\vf{x}}\left(h\sum_{j=1}^{s}a_{ij}\vf{k}_j\right)+\frac{{c_i}^2 h^2}{2}\vf{f}_{tt}+\\
        &+c_i h^2\vf{f}_{\vf{x}t}\left(\sum_{j=1}^{s}a_{ij}\vf{k}_j\right)+\frac{h^2}{2}\vf{f}_{\vf{xx}}{\left(\sum_{j=1}^{s}a_{ij}\vf{k}_j\right)}^2\!+\O{h^3}
      \end{split} \\
       & =\vf{f}+c_i h\vf{F}+\frac{{c_i}^2}{2}h^2\vf{G}+\left(\sum_{j=1}^{s}a_{ij}c_j\right)\vf{f}_{\vf{x}}\vf{F}+\O{h^3}                                          \\
    \end{align*}
    Therefore:
    $$\vf\tau_{n}(h)=\vf{f}+\frac{1}{2}h\vf{F}+\frac{1}{6}h^2\left(\vf{G}+\vf{f}_{\vf{x}}\vf{F}\right)+\O{h^3}-\sum_{i=1}^sb_i\vf{k}_i$$
    Matching coefficients we get the desired result.
  \end{proof}
  \begin{lemma}
    The (consistency) order $p$ of an $s$-stage Runge-Kutta method is bounded by $p\leq 2 s$. If the Runge-Kutta method is explicit, then $p\leq s$.
  \end{lemma}
  \begin{remark}
    Looking at \mcref{NC:stages-orderRK} we see why the RK4, i.e. the RK method with 4 stages, is widely known.
  \end{remark}
  \begin{table}[H]
    \centering
    \begin{tabular}{c|cccccccc}
      order & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8  \\
      \hline
      $s$   & 1 & 2 & 3 & 4 & 6 & 7 & 9 & 11
    \end{tabular}
    \caption{Number of stages of an explicit RK method needed for a given order of consistency}
    \label{NC:stages-orderRK}
  \end{table}
  \subsubsection{Step-size control for Runge-Kutta methods}
  \begin{theorem}\label{NC:errorControl}
    Let $\vf{f}:[t_0,t_n]\times\RR^d\rightarrow\RR^d$ be a function of class $\mathcal{C}^{N+1}$ with respect to the second variable and let $\vf{\tilde{x}}(t)$ be the numerical solution to the ivp \mcref{NC:ivp} obtained by a one-step method of order $p\leq N$ with step-size $h$. Then, $\vf{\tilde{x}}(t)$ has an asymptotic expansion of:
    $$\vf{\tilde{x}}(t)=\vf{x}(t)+\vf{e}_p(t)h^p+\cdots+\vf{e}_N(t)h^N+E_{N+1}(t,h)h^{N+1}$$
    with $\vf{e}_k(t_0)=0$ $\forall k\geq p$. This is valid $\forall t\in[t_0,t_n]$ and all $h>0$. Moreover the functions $\vf{e}_k$ are differentiable and independent of $h$ and $\norm{E_{N+1}(t,\cdot)}_\infty<\infty$ $\forall t\in[t_0,t_n]$.
  \end{theorem}
  \begin{proof}
    In order to simplify the notation, we will only prove it for $N=p+1$. Let $\vf{\phi}$ be the incremental function. Since the method is of order $p\leq N$, we have:
    $$\vf{x}(t+h)-\vf{x}(t)-h\vf\phi(t,\vf{x},\vf{f},h)=\vf{d}_{p+1}h^{p+1}+\O{h^{p+2}}$$
    We will show that there is a differentiable function $\vf{e}_{p}$ such that:
    $$\vf{x}(t+h)-\vf{x}(t)-h\vf\phi(t,\vf{x},\vf{f},h)=\vf{e}_{p}(t)h^{p}+\O{h^{p+1}}$$
    Consider $\vf{\hat{x}}(t):=\vf{\tilde{x}}(t)-\vf{e}_p(t)h^p$ where the choice of $\vf{e}_p$ is still left open. It is easy to see that $\vf{\hat{x}}(t)$ can be chosen as the result of another one-step method: $$\vf{\hat{x}}(t+h)=\vf{\hat{x}}(t)+h\vf{\hat\phi}(t,\vf{\hat{x}},\vf{f},h)$$
    where: $$\vf{\hat\phi}(t,\vf{{x}},\vf{f},h)=\vf{\phi}(t,\vf{{x}}+\vf{e}_p(t)h^p,\vf{f},h)-(\vf{e}_p(t+h)-\vf{e}_p(t))h^{p-1}$$
    Taylor expanding to find the local truncation error of this new method:
    \begin{multline*}
      \vf{x}(t+h)-\vf{x}(t)-h\vf{\hat\phi}(t,\vf{x},\vf{f},h)=\\
      =(\vf{d}_{p+1}-\vf{f}_{\vf{x}}\vf{e}_p(t)-{\vf{e}_p}'(t))h^{p+1}+\O{h^{p+2}}
    \end{multline*}
    So if we take $\vf{e}_p$ to be the solution of the ivp:
    $$\begin{cases}
        {\vf{e}_p}'=\vf{d}_{p+1}-\vf{f}_{\vf{x}}\vf{e}_p(t) \\
        \vf{e}_p(t_0)=0
      \end{cases}$$
    Then we have that this new method is of order $p+1$ and by \mcref{NC:errorLipschitz} we have that:
    $$\vf{\hat{x}}(t)-\vf{x}(t)=\vf{\tilde{x}}(t)-\vf{x}(t)-\vf{e}_p(t)h^p=\O{h^{p+1}}$$
    A repetition of these arguments with  $\vf{\hat\phi}$ in place of $\vf{\phi}$ completes the proof.
  \end{proof}
  \begin{theorem}[Richardson extrapolation]
    Consider the ivp of \mcref{NC:ivp} and let $\vf{\tilde{x}}(t;h)$ be the numerical solution obtained by a one-step method of order $p\leq N$ with step-size $h$. Then:
    $$\vf{x}(t)=\vf{\tilde{x}}(t;h/2)-\frac{\vf{\tilde{x}}(t;h)-\vf{\tilde{x}}(t;h/2)}{2^p-1}+\O{h^{p+2}}$$
  \end{theorem}
  \begin{proof}
    By \mcref{NC:errorControl} we have that:
    \begin{align*}
      \vf{x}(t;h)   & =\vf{x}(t)+\vf{e}_p(t)h^p+\O{h^{p+1}}                          \\
      \vf{x}(t;h/2) & =\vf{x}(t)+\vf{e}_p(t){\left(\frac{h}{2}\right)}^p+\O{h^{p+1}}
    \end{align*}
    Substracting the two equations we have:
    $$\vf{e}_p(t){\left(\frac{h}{2}\right)}^p=\frac{\vf{\tilde{x}}(t;h)-\vf{\tilde{x}}(t;h/2)}{2^p-1}+\O{h^{p+1}}$$
  \end{proof}
  \begin{theorem}[Runge-Kutta-Fehlberg method]
    Consider two RK methods of orders $p$ and $p+1$ with incremental functions $\vf{\phi}$ and $\vf{\hat\phi}$ respectively such that their Butcher tableaus have the same $(a_{ij})$ coefficients (and therefore the same $(c_{ij})$ coefficients):
    \begin{center}
      \renewcommand{\arraystretch}{1.25}
      \begin{tabular}{c|c}
        $\vf{c}$ & $\vf{A}$                   \\
        \hline
                 & $\transpose{\vf{b}}$       \\
                 & $\transpose{\vf{\hat{b}}}$
      \end{tabular}
    \end{center}
    These methods $\vf{\phi}$ and $\vf{\hat\phi}$ are called \emph{embedded methods}\footnote{Usually the notation $\mathrm{RK}p(q)s$ is used to refer for a method of order $p$ with an embedded method of order $q< p$ and a total of $s$ stages.}. Denote by $\vf{\tilde{x}}$ and $\vf{\hat{x}}$ the numerical solutions using the respective incremental functions. Then, given a tolerance $\epsilon$ and a older step-size $h$, we have to choose the new step-size
    \begin{equation}\label{NC:RFF}
      h_{\mathrm{new}}\simeq h\sqrt[p+1]{\frac{\epsilon}{\norm{\vf{\hat{x}}(t_i+h)-\vf{\tilde{x}}(t_i+h)}}}
    \end{equation}
    If with this new step-size we have $$\norm{\vf{\hat{x}}(t_{i+1}+h_\mathrm{new})-\vf{\tilde{x}}(t_{i+1}+h_\mathrm{new})}>\epsilon$$ we will have to repeat the last step with an another step-size $h_{\mathrm{new}}^*<h_{\mathrm{new}}$.
  \end{theorem}
  \begin{proof}
    Assuming that the $i$-th iteration was successful, i.e.:
    $$\norm{\vf{\hat{x}}(t_i+h)-\vf{\tilde{x}}(t_i+h)}\leq\epsilon$$
    Neglecting terms of higher order, we then have:
    $$\vf{\hat{x}}(t_i+h)-\vf{\tilde{x}}(t_i+h)\approx \vf{C}(\vf{x}(t_i))h^{p+1}$$
    Moreover, up to errors of first order, we have $\vf{C}(\vf{x}(t_i))\approx\vf{C}(\vf{x}(t_{i+1}))$. Finally imposing
    $$\norm{\vf{C}(\vf{x}(t_{i+1})){h_\mathrm{new}}^{p+1}}\leq\epsilon$$
    yields to:
    $$\norm{\vf{\hat{x}}(t_i+h)-\vf{\tilde{x}}(t_i+h)}{\left(\frac{h_{\mathrm{new}}}{h}\right)}^{p+1}\lesssim\epsilon$$
  \end{proof}
  \begin{remark}
    Note that there exists RK embedded methods by the following argument. Start with a RK method of order $p+1$ that has $s$ stages. Then we can construct a RK method of order $p$ with $s$ stages by copying the coefficients $\vf{A}$ and $\vf{c}$ and adjusting the coefficients $\vf{b}$ in a proper way.
  \end{remark}
  \begin{remark}
    In practise in order to avoid many unsuccessful steps, instead of the new step in \mcref{NC:RFF} we use the following:
    $$h_{\mathrm{new}}\simeq\alpha h\sqrt[p+1]{\frac{\epsilon}{\norm{\vf{\hat{x}}(t_i+h)-\vf{\tilde{x}}(t_i+h)}}}$$
    with $\alpha\simeq 0.9$.
    Furthermore, in order to avoid rapid oscillations of the stepsize, $h$ should not, however, be changed by more than a factor of 2 to 5 from one step to the next.
  \end{remark}
\end{multicols}
\end{document}