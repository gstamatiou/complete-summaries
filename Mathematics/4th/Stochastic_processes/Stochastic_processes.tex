\documentclass[../../../main_math.tex]{subfiles}


\begin{document}
\changecolor{SP}
\begin{multicols}{2}[\section{Stochastic processes}]
  \subsection{Preliminaries}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be sequence of random variables such that: $$\sum_{n=1}^{\infty}\Exp(\abs{X_n})<\infty$$
    Then, $\sum_{n=1}^{\infty}X_n$ is a random variable and: $$
      \Exp\left(\sum_{n=1}^{\infty}X_n\right)=\sum_{n=1}^{\infty}\Exp(X_n)
    $$
  \end{proposition}
  \begin{proof}
    By the \mnameref{P:monotone} we have that
    $$\Exp\left(\sum_{n=1}^{\infty}\abs{X_n}\right)=\sum_{n=1}^{\infty}\Exp(\abs{X_n})<\infty$$
    so the random variable $Y:=\sum_{n=1}^{\infty}\abs{X_n}$ is integrable and $\forall N\in\NN$ satisfies: $$\abs{\sum_{n=1}^{N}X_n}\leq Y$$
    Now use the \mnameref{P:dominated}.
  \end{proof}
  \begin{proposition}[Law of total probability]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be such that $\bigsqcup_{n=1}^N A_n=\Omega'$ with $\Prob(\Omega')=1$. Then, $\forall A\in\mathcal{A}$: $$\Prob(A)=\sum_{n=1}^N\Prob(A_n)\Prob(A\mid A_n)$$
  \end{proposition}
  \begin{sproof}
    See the proof of \mnameref{P:totalprob}.
  \end{sproof}
  \begin{remark}
    Note that if there is some $A_n$ for which $\Prob(A_n)=0$, the conditional probability is not well-defined. But note that:
    $$0\leq \Prob(A_n)\Prob(A\mid A_n)=\Prob(A\cap A_n)\leq \Prob(A_n)=0$$
    So we can omit this term in the sum.
  \end{remark}
  \subsubsection{Conditional expectation}
  \begin{proposition}[Substitution principle]\label{SP:substitutionPrinciple}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}$ be a discrete random vector with support $S_{\vf{X}}$, $Y$ be a discrete random variable with support $S_Y$, $h:S_{\vf{X}}\times S_Y\rightarrow\RR$ be a function and $y\in S_Y$. Then:
    $$\Exp(h(\vf{X},Y)\mid Y=y)=\Exp((h(\vf{X}),Y=y)\mid Y=y)$$
  \end{proposition}
  \begin{proposition}[Law of total expectation]\label{SP:totalexp}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}$ is a discrete random vector with support $S_{\vf{X}}$, $\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be such that $\bigsqcup_{n=1}^N A_n=\Omega'$ with $\Prob(\Omega')=1$ and $h:S_{\vf{X}}\rightarrow\RR$ be a function. If $h(\vf{X})$ has finite expectation or $h\geq 0$, then: $$\Exp(h(\vf{X}))=\sum_{n=1}^N\Exp(h(\vf{X})\mid A_n)\Prob(A_n)$$
  \end{proposition}
  \begin{sproof}
    See the proof of \mnameref{P:totalexp}.
  \end{sproof}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}$ be a discrete random vector with support $S_{\vf{X}}$, $Y$ be a discrete random variable with support $S_Y$ and $h:S_{\vf{X}}\rightarrow\RR$ be a function. For all $\omega\in\Omega$ we define the random variable $\Exp(h(\vf{X})\mid Y)$ as
    $$\Exp(h(\vf{X})\mid Y)(\omega)=\Exp(h(\vf{X})\mid Y=y)$$
    provided that $Y(\omega)=y$.
    Note that it can also be written as:
    $$\Exp(h(\vf{X})\mid Y)=\sum_{y\in S_Y}\Exp(h(\vf{X})\mid Y=y)\indi{\{Y=y\}}$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}$ be a discrete random vector with support $S_{\vf{X}}$, $Y$ be a discrete random variable with support $S_Y$, $h,h_1,h_2:S_{\vf{X}}\rightarrow\RR$ be functions and $a,b\in\RR$. Then:
    \begin{enumerate}
      \item $\Exp(ah_1(\vf{X})+bh_2(\vf{X})\mid Y)=a\Exp(h_1(\vf{X})\mid Y)+b\Exp(h_1(\vf{X})\mid Y)$
      \item If $\vf{X}$ and $Y$ are independent, then $\Exp(h(\vf{X})\mid Y)=\Exp(h(\vf{X}))$.
      \item $\Exp(\Exp(h(\vf{X})\mid Y))=\Exp(h(\vf{X}))$
    \end{enumerate}
  \end{proposition}
  \begin{sproof}
    The first two properties are consequence of the fact that the conditional expectation is an expectation. For the last one note that:
    \begin{align*}
      \Exp(\Exp(h(\vf{X})\mid Y)) & =\sum_{y\in S_Y}\Exp(h(\vf{X})\mid Y=y)\Prob(Y=y) \\
                                  & =\Exp(h(\vf{X}))
    \end{align*}
    where the last equality is due to the \mnameref{SP:totalexp}.
  \end{sproof}
  \begin{theorem}[Wald theorem]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(Z_n)$ be a sequence of random variables, all of them with expectation $\mu\in\RR$, such that $\sup_{n\geq 1}\Exp(\abs{Z_n})=A<\infty$. If $N$ is an integrable random variable with support $\NN$ independent of $Z_n$ $\forall n\in\NN$, we have that:
    $$\Exp\left(\sum_{n=1}^NZ_n\right)=\mu\Exp(N)$$
  \end{theorem}
  \begin{proof}
    Note that $\Exp\left(\sum_{n=1}^NZ_n\right)=\Exp\left(\sum_{n=1}^\infty Z_n\indi{N\geq n}\right)$ and it is integrable because:
    \begin{multline*}
      \Exp\left(\sum_{n=1}^\infty \abs{Z_n\indi{N\geq n}}\right) =\sum_{n=1}^\infty \Exp(\abs{Z_n})\Exp(\indi{N\geq n}) \leq              \\
      \leq A\sum_{n=1}^\infty \Prob(N\geq n) =A\Exp(N)<\infty
    \end{multline*}
    where we have used the Independence of $(Z_n)$ and $N$ in the first equality.
    And so:
    $$\Exp\left(\sum_{n=1}^NZ_n\right)=\sum_{n=1}^\infty \Exp(Z_n)\Exp(\indi{N\geq n})=\mu\Exp(N)$$
  \end{proof}
  \begin{remark}
    Note that the equality remains true if $Z_n\geq 0$ $\forall n\in\NN$ even if $\Exp(N)=\infty$ because the equality $\sum_{n=1}^\infty \Prob(N\geq n)=\Exp(N)$ remains true.
  \end{remark}
  \subsubsection{Probability-generating function}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable with support $\NN\cup\{0\}$. The \emph{probability-generating function} (or \emph{pgf}) of $X$ is the function $g_X:\mathcal{D}_X\rightarrow \RR$ defined as:
    \begin{equation}\label{SP:probgenfunc}
      g_X(s)=\sum_{k=0}^\infty s^k\Prob(X=k)=\Prob(X=0)+\sum_{k=1}^\infty s^k\Prob(X=k)
    \end{equation}
    The set $\mathcal{D}_X$ is defined as all the points for which the series of \mcref{SP:probgenfunc} converges absolutely.
  \end{definition}
  \begin{lemma}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable with support $\NN\cup\{0\}$. Then, $[-1,1]\subseteq \mathcal{D}_X$ and $g_X(s)=\Exp(s^X)$ (with the convention that $0^0=1$).
  \end{lemma}
  \begin{proof}
    Clearly $g_X(s)=\Exp(s^X)$ and furthermore $\forall s\in[-1,1]$ we have $s\in \mathcal{D}_X$ because:
    $$\sum_{k=0}^\infty \abs{s}^k\Prob(X=k)\leq \sum_{k=0}^\infty \Prob(X=k)=1<\infty$$
  \end{proof}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be random variables with support $\NN\cup\{0\}$. Then: $$X\overset{\text{d}}{=}Y\iff g_X=g_Y$$
    Moreover: $$\Prob(X=k)=\frac{{g_Y}^{(k)}(0)}{k!}\quad \forall k\geq 0$$
  \end{theorem}
  \begin{sproof}
    Note that $g_X(s)$ is a power series defined in a neighbourhood of $s=0$ (recall \mcref{MA:derivk_powerseries}).
  \end{sproof}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be independent random variables with support $\NN\cup\{0\}$. Then, $\forall s\in\mathcal{D}_X\cap \mathcal{D}_Y$ we have: $$g_{X+Y}(s)=g_X(s)g_Y(s)$$
  \end{theorem}
  \begin{proof}
    Note that if $s\in\mathcal{D}_X\cap \mathcal{D}_Y$, then $s\in\mathcal{D}_{X+Y}$ because
    $$g_{X+Y}(\abs{s})=\Exp\left(\abs{s}^{X+Y}\right)=\Exp\left(\abs{s}^X\right)\Exp\left(\abs{s}^Y\right)<\infty$$
    due to the independence of $X$ and $Y$.
    To show the equality if $s=0$, we have:
    $$\Prob(X+Y=0)=\Prob(X=0,Y=0)=\Prob(X=0)\Prob(Y=0)$$
    where the first equality is due to $X,Y\in\NN\cup\{0\}$ and the second one is becuse of the independence.
    If $s\ne 0$, as before: $$g_{X+Y}({s})=\Exp({s}^{X+Y})=\Exp({s}^X)\Exp({s}^Y)=g_{X}({s})g_{Y}({s})$$
  \end{proof}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable with support $\NN\cup\{0\}$. Then, $\forall k\geq 1$ we have:
    $$\lim_{s\to 1^-}{g_X}^{(k)}(s)=\Exp(X(X-1)\cdots(X-k+1))$$
  \end{theorem}
  \begin{sproof}
    Take $(s_n)\in\RR$ such that $s_n\nearrow 1$. Differentiating term by term we have that:
    $$
      g_{X}^{(k)}(s_n)=\Exp(X(X-1)\cdots (X-k+1){(s_n)}^{X-k})
    $$
    for all $n\in\NN$. Moreover note that $X(X-1)\cdots (X-k+1){(s_n)}^{X-k}\nearrow X(X-1)\cdots (X-k+1)$. Now use the \mnameref{P:monotone}.
  \end{sproof}
  \begin{center}
    \def\arraystretch{1.3}
    \begin{tabular}{|c|c|c|}
      \hline
      $X$                     & $g_X(s)$                            & $\mathcal{D}_X$                                          \\
      \hline
      $k\in\NN$               & $\displaystyle s^{k}$               & $\RR$                                                    \\
      %\hline
      $U(\{k_1,\ldots,k_n\})$ & $\frac{1}{n}\sum_{i=1}^n s^{k_i}$   & $\RR$                                                    \\
      %\hline
      $\text{B}(n,p)$         & $\displaystyle {(ps+1-p)}^n$        & $\RR$                                                    \\
      %\hline
      $\text{Pois}(\lambda)$  & $\displaystyle \exp{\lambda(s-1)}$  & $\RR$                                                    \\
      %\hline
      $\text{Geo}(p)$         & $\displaystyle \frac{ps}{1-(1-p)s}$ & $\displaystyle\left(\frac{-1}{1-p},\frac{1}{1-p}\right)$ \\
      %\hline
      \hline
    \end{tabular}
    \captionof{table}{Probability-generating functions of common distributions.}
  \end{center}
  \subsection{Discrete-time Markov chains}
  \begin{definition}[Stochastic process]
    Let $T\subseteq \RR^n$ be a set, $(E,\mathcal{E})$ be a measurable space and $(\Omega,\mathcal{A},\Prob)$ be a probability space. A \emph{stochastic process} on $(\Omega,\mathcal{A},\Prob)$ with \emph{parameter set} $T$ and \emph{state space} $(E,\mathcal{E})$ is a family of random variables ${\{X_t\}}_{t\in T}$ from $(\Omega,\mathcal{A})$ to $(E,\mathcal{E})$. That is, $X_t:\Omega\to E$ satisfies ${X_t}^{-1}(B)\in\mathcal{A}$ for all $B\in\mathcal{E}$ and all $t\in T$.
  \end{definition}
  \begin{remark}
    In general we wil consider stochastic processes with parameter sets $T=\NN,\NN\cup\{0\},\ZZ,\RR,\RR_{\geq 0}$ and state spaces $(\NN\cup\{0\},\mathcal{P}(\NN \cup \{0\}))$ or $(\RR,\mathcal{B}(\RR))$.
  \end{remark}
  \subsubsection{Galton-Watson process}
  \begin{model}\label{SP:galtonwatsonModel}
    Let $(X_n)$, $n\in\NN\cup\{0\}$ be a sequence of discrete random vairables representing the number of new individuals of a certain population at the $n$-th generation. Suppose they are defined as $$X_{n+1}=\sum_{k=1}^{X_n}Z_{n+1}^{(k)}$$ and $X_0=1$. Here $Z_{n+1}^{(k)}$ has support $\NN\cup\{0\}$ $\forall n,k$ and represent the number of descendants (to the next generation) of the $k$-th individual of the $n$-th generation. Suppose that $Z_{n+1}^{(k)}\sim Z$ are \iid and independent from $(X_n)$. We would like to study the probability $\rho$ of extinction of this population: $$\rho=\Prob(\{X_n=0:\text{for some $n\in\NN$}\})=\Prob\left(\bigcup_{n=1}^\infty\{X_n=0\}\right)$$
  \end{model}
  \begin{lemma}\label{SP:lemmaGaltonWatson}
    Let $(Z_n)$ be a sequence of \iid random variables distributed as $Z$ with support $\NN\cup\{0\}$, and $N$ be a random variable also with support $\NN\cup\{0\}$ and independent to $(Z_n)$. Let $X=\sum_{k=1}^NZ_k$. Then, $\forall s\in[-1,1]$ we have: $$g_X(s)=g_N(g_Z(s))$$
  \end{lemma}
  \begin{proof}
    First suppose $N\leq M$ with $M\in\NN$ fixed. Then using the independence, the \mnameref{SP:totalexp} and the \mnameref{SP:substitutionPrinciple}:
    \begin{align*}
      g_X(s)=\Exp(s^X) & =\sum_{k=1}^M\Exp(s^X\mid N=k)\Prob(N=k)                                        \\
                       & = \sum_{k=1}^{M} \Exp\left(s^{\sum_{i=1}^{N}Z_i} \mid N = k\right) \Prob(N = k) \\
                       & = \sum_{k=1}^{M} \Exp\left(s^{\sum_{i=1}^{k}Z_i}\right) \Prob(N = k)            \\
                       & = \sum_{k=1}^{M} {g_Z(s)}^k \Prob(N = k)                                        \\
                       & = g_N(g_Z(s))
    \end{align*}
    Now if $N$ can take any value of $\NN\cup\{0\}$ we have that:
    \begin{align*}
      g_X(s) & =\Exp\left(s^{\sum_{i=1}^{N}Z_i}\right)                          \\
             & =\Exp\left(\lim_{M\to\infty}s^{\sum_{i=1}^{\min(N,M)}Z_i}\right) \\
             & =\lim_{M\to\infty}\Exp\left(s^{\sum_{i=1}^{\min(N,M)}Z_i}\right) \\
             & =\lim_{M\to\infty}g_{\min(N,M)}(g_Z(s))                          \\
             & =\lim_{M\to\infty}\Exp\left({(g_Z(s))}^{\min(N,M)}\right)        \\
             & =\Exp\left(\lim_{M\to\infty}{(g_Z(s))}^{\min(N,M)}\right)        \\
             & =g_N(g_Z(s))
    \end{align*}
    where both limit exchangings are due to the \mnameref{P:dominated} using the intagrable random variable 1.
  \end{proof}
  \begin{theorem}
    In the hypothesis of \mcref{SP:galtonwatsonModel}, we have that: $$\rho=g_Z(\rho)$$
  \end{theorem}
  \begin{proof}
    Note that $\{X_n=0\}\subseteq \{X_{n+1}=0\}$. Hence:
    $$\rho=\Prob\left(\bigcup_{n=1}^\infty\{X_n=0\}\right)\!=\lim_{n\to\infty}\Prob(X_n=0)=\lim_{n\to\infty}g_{X_n}(0)$$
    Now, using \mcref{SP:lemmaGaltonWatson} we have:
    $$g_{X_{n}}(s)=g_{X_{n-1}}(g_Z(s))=\cdots=g_{X_1}({g_Z}^n(s))$$
    But $X_1=1$ and so $g_{X_1}(s)=s$. So $g_{X_{n}}(s)={g_Z}^n(s)$ and therefore
    $g_{X_{n+1}}(0)=g_Z(g_{X_{n}}(0))$.
    Taking the limit as $n\to\infty$ and using the continuity of the pgf we get the result.
  \end{proof}
  \begin{theorem}
    In the hypothesis of \mcref{SP:galtonwatsonModel} and the additional assumption that $0<\Prob(Z=0)<1$ we have:
    \begin{enumerate}
      \item If $\Exp(Z)\leq 1$, $g_Z$ has only 1 fixed point (the trivial one, $s=1$). Hence, the population will extinct with probability 1.
      \item If $\Exp(Z)> 1$, $g_Z$ has a unique non-trivial fixed point on (0,1).
    \end{enumerate}
  \end{theorem}
  \begin{proof}
    First suppose $\Prob(Z=0)+\Prob(Z=1)=1$. Thus, $Z\almoste{\leq}1$ and so $\Exp(Z)\leq 1$. Moreover, $g_Z(s)=\Prob(Z=0)+s\Prob(Z=1)$, which is a line with slope $\Prob(Z=1)<1$. Hence, it has a unique fixed point, which is $s=1$.

    Now assume $\Prob(Z=0)+\Prob(Z=1)<1$. Then, $\exists k\geq 2$ with $\Prob(Z=k)>0$. Hence, ${g_Z}'(s)>0$ and ${g_Z}''(s)>0$ $\forall s\in (0,1)$. Now consider $f(s)=g(s)-s$. Note that $f$ is strictly convex in $(0,1)$ and $f(0)=g(0)=\Prob(Z=0)>0$. Finally note that $$\lim_{t\to 1^-} f'(s)=\lim_{t\to 1^-}g(s)-1=\Exp(Z)-1$$
    and so $\displaystyle\lim_{t\to 1^-} f'(s)$ is negative in the first case and positive in the second case. This imply that $f$ has no zeros on $(0,1)$ in the first case and exactly 1 zero in $(0,1)$ in the second case.

    It's missing to see that in the second case the probability of extinction $\rho$ is given by the fixed point in $(0,1)$, rather than 1. We have that:
    $$\rho=\lim_{n\to\infty}g_{X_n}(0)=\lim_{n\to\infty}{g_Z}^n(0)$$
    Since ${g_Z}'>0$, we have that ${g_Z}$ is increasing and so it is ${g_Z}^n$ $\forall n\in\NN$. Moreover if ${g_Z}(x_0)=x_0$, we have that ${g_Z}^n(x_0)=x_0$ $\forall n\in\NN$. Therefore $$0<{g_Z}(0)<{g_Z}^2(0)<\cdots<{g_Z}^n(0)<\cdots <x_0<\cdots <1$$
    And so the limit has to be $x_0$ (note that the limit does exist because $({g_Z}^n(0))$ is an increasing bounded sequence).
  \end{proof}
  \subsubsection{Gambler's ruin}
  \begin{definition}[Gambler's ruin problem]
    Consider a gambler with an initial capital $z\in\ZZ$ and suppose that he plays a game in which wins 1 unit of capital with probability $p$ and loses 1 unit of capital with probability $q:=1-p$. The game ends whenever the player is ruined or if he arrives to a capital of $a\in\ZZ$. All the plays are independent. We denote by $(X_k)$ the variables that measure the $k$-th play. That is: $$\Prob(X_k=1)=p\qquad\Prob(X_k=-1)=q$$
    We define $q_z$ as the probability of ruining himself starting with a capital of $z$, $p_z$ as the probability of winning the game starting with a capital of $z$ and $D_z$ as the duration of the game starting with a capital of $z$.
  \end{definition}
  \begin{proposition}
    Consider the Gambler's ruin problem. Then:
    $$q_z=\begin{cases}
        \frac{-{\left(\frac{q}{p}\right)}^a+{\left(\frac{q}{p}\right)}^z}{1-{\left(\frac{q}{p}\right)}^a} & \text{if $p\ne 1/2$} \\
        1-\frac{z}{a}                                                                                     & \text{if $p= 1/2$}
      \end{cases}$$
  \end{proposition}
  \begin{sproof}
    We have that $q_k$ solves the difference equation
    \begin{multline*}
      q_k =\Prob(\text{ruin}\mid X_k=1)\Prob(X_1=1)+\\
      +\Prob(\text{ruin}\mid X_k=-1)\Prob(X_1=-1) =q_{k+1}p+q_{k-1}q
    \end{multline*}
    with $q_0=1$ and $q_a=0$, whose solution is straightforward.
  \end{sproof}
  \begin{proposition}
    Consider the Gambler's ruin problem. Suppose that we play against another player (and so when we lose, he wins and viceversa). Let $p_z^*$, $q_z^*$ be the respective probabilities for the other player. Then:
    $$q_z+q_z^*=1$$
    Hence, $D_z\almoste{<}\infty$.
  \end{proposition}
  \begin{sproof}
    Note that
    $$q_z^*=\begin{cases}
        \frac{-{\left(\frac{p}{q}\right)}^a+{\left(\frac{p}{q}\right)}^{a-z}}{1-{\left(\frac{p}{q}\right)}^a} & \text{if $p\ne 1/2$} \\
        1-\frac{a-z}{a}                                                                                       & \text{if $p= 1/2$}
      \end{cases}$$
  \end{sproof}
  \begin{proposition}
    Let $d_z=\Exp(D_z)$ and suppose that this expectation is finite. Then:
    $$
      d_z=\begin{cases}
        \frac{z}{q-p}-\frac{a}{q-p}\frac{1-{\left(\frac{q}{p}\right)}^z}{1-{\left(\frac{q}{p}\right)}^a} & \text{if $p\ne 1/2$} \\
        z(a-z)                                                                                           & \text{if $p= 1/2$}
      \end{cases}
    $$
  \end{proposition}
  \begin{proof}
    We have that $q_k$ solves the difference equation:
    \begin{multline*}
      d_k =\Exp(D_k\mid X_k=1)\Prob(X_1=1)+\\
      +\Exp(D_k\mid X_k=-1)\Prob(X_1=-1) =\\=\Exp(D_{k+1}+1)p+\Exp(D_{k-1}+1)q= d_{k+1}p+d_{k-1}q+1
    \end{multline*}
    with $d_0=0$ and $d_a=0$, whose solution is straightforward ($d_k=\frac{k}{p-q}$ and $d_k=-k^2$ are particular solutions for the case $p\ne q$ and $p=q$, respectively).
  \end{proof}
  \subsubsection{Markov chains}
  \begin{definition}
    A \emph{Markov chain} is a sequence of discrete random variables $(X_n)$ with support $I$ such that:
    \begin{multline*}
      \Prob(X_{n+1}=j\mid X_0=i_0,\ldots,X_{n-1}=i_{n-1},X_n=i)=\\=\Prob(X_{n+1}=j\mid X_n=i)
    \end{multline*} for all $n\geq 0$ and all $i_0,\ldots,i_{n-1},i,j\in I$. This property is usually called \emph{Markov property}.
    If moreover $\Prob(X_{n+1}=j\mid X_n=i)$ does not depend on $n$, that is
    $$\Prob(X_{n+1}=j\mid X_n=i)=\Prob(X_{1}=j\mid X_0=i)$$
    then we say that the Markov chain is a \emph{time-homogeneous Markov chain}. The set $I$ is called \emph{state space} and its elements are called \emph{states} of the Markov chain.
  \end{definition}
  \begin{definition}[Stochastic matrix]
    Let $I$ be an index set. A matrix $\vf{P}=(p_{ij})_{i,j\in I}$ is called a \emph{stochastic matrix} if $p_{ij}\geq 0$ $\forall i,j\in I$ and: $$\sum_{j\in I}p_{ij}=1$$
  \end{definition}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We define the \emph{transition probabilities} $p_{ij}$ as the probability of going from state $i$ to state $j$. That is: $$p_{ij}=\Prob(X_{1}=j\mid X_0=i)$$
    The matrix $\vf{P}=(p_{ij})_{i,j\in I}$ is called the \emph{transition matrix} of the Markov chain. Finally, we define the probabilities $\pi_i$ as $\pi_i=\Prob(X_0=i)$. We define the vector $\vf{\pi}=(\pi_i)_{i\in I}$ as the \emph{initial distribution} of the Markov chain.
  \end{definition}
  \begin{proposition}
    Let $(X_n)$ be a time-homogeneous Markov chain. Then:
    \begin{enumerate}
      \item $\vf{P}$ is a stochastic matrix.
      \item $\sum_{i\in I}\pi_i=1$.
    \end{enumerate}
  \end{proposition}
  \begin{lemma}
    Let $I$, $F$ be finite or countable set, $(Z_n)$ be a sequence of random variables with support $F$, $X_0$ be a random variable with support $I$ and $f:I\times F\rightarrow I$ be a function. Consider the sequence $(X_n)$ defined by:
    $$
      X_{n+1}=f(X_n,Z_{n+1})
    $$
    If $\forall i_0,\ldots,i_{n-1},i\in I$ and $\forall k\in F$ we have:
    \begin{multline*}
      \Prob(Z_{n+1}=k\mid X_0=i_0,\ldots,X_{n-1}=i_{n-1},X_n=i)=\\=\Prob(Z_{n+1}=k\mid X_n=i)=\Prob(Z_{1}=k\mid X_0=i)
    \end{multline*}
    then $(X_n)$ is a time-homogeneous Markov chain with transition matrix $\vf{P}=(p_{ij})_{i,j\in I}$ given by:
    $$
      p_{ij}=\Prob(f(i,Z_1)=j\mid X_0=i)
    $$
  \end{lemma}
  \begin{proof}
    Let $C=\{X_0=i_0,\ldots,X_{n-1}=i_{n-1},X_n=i\}$ and let $A_{i,j}:=\{k\in F:f(i,k)=j\}$. We have:
    \begin{align*}
      \Prob(X_{n+1}=j\mid C) & =\Prob(f(i,Z_{n+1})=j\mid C)                 \\
                             & =\Prob(Z_{n+1}\in A_{i,j}\mid C)             \\
                             & =\sum_{k\in A_{i,j}}\Prob(Z_{n+1}=k\mid C)   \\
                             & =\sum_{k\in A_{i,j}}\Prob(Z_{1}=k\mid X_0=i) \\
                             & =\Prob(f(i,Z_{1})=j\mid X_0=i)
    \end{align*}
  \end{proof}
  \begin{definition}[Random walk]
    A \emph{random walk} is a sequence $(\vf{S}_n)$ with $\vf{S}_0=\vf{X}_0$ and $\vf{S}_n=\sum_{k=0}^{n}\vf{X}_k$, where $(\vf{X}_k)_{k\geq 1}$ is a sequence of \iid random vectors and $\vf{X}_0$ is a random vector independent from $(\vf{X}_k)$.
  \end{definition}
  \begin{definition}
    A \emph{simple random walk} is a random walk in which $X_k$ are random variables such that:
    $$
      X_k=\begin{cases}
        1  & \text{with probability $p$}   \\
        -1 & \text{with probability $1-p$}
      \end{cases}
    $$
  \end{definition}
  \begin{lemma}
    A sequence of \iid random variables, a random walk and a Galton-Watson process are all time-homogeneous Markov chains.
  \end{lemma}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We define the \emph{$n$-step transition probabilities} $p_{ij}^{(n)}$ as the probability of going from state $i$ to state $j$ in $n$ steps. That is: $$p_{ij}^{(n)}=\Prob(X_{n}=j\mid X_0=i)$$ The matrix $\vf{P}^{(n)}=(p_{ij}^{(n)})_{i,j\in I}$ is called \emph{$n$-step transition matrix} of the Markov chain.
  \end{definition}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We define the probabilities $\pi_i^{(n)}$ as the probability of being in state $i$ after $n$ steps. That is: $$\pi_i^{(n)}=\Prob(X_n=i)$$
    We define the vector $\vf{\pi}^{(n)}=(\pi_i^{(n)})_{i\in I}$ as \emph{$n$-step distribution} of the Markov chain.
  \end{definition}
  \begin{lemma}\label{SP:lema1Markov}
    Let $A$, $B$, $C$ be events in a probability space such that $\Prob(B\cap C)>0$. Then:
    $$\Prob(A\cap B\mid C)=\Prob(B\mid C)\Prob(A\mid B\cap C)$$
  \end{lemma}
  \begin{proof}
    \begin{align*}
      \Prob(A\cap B\mid C) & =\frac{\Prob(A\cap B\cap C)}{\Prob(C)}\frac{\Prob(B\cap C)}{\Prob(B\cap C)} \\
                           & =\Prob(B\mid C)\Prob(A\mid B\cap C)
    \end{align*}
  \end{proof}
  \begin{lemma}\label{SP:lema2Markov}
    Let $I$ be a finite or countable set and $A$ and $D_i$ for $i\in I$ be events in a probability space such that $\Prob(A\mid D_i)=p$ for all $i\in I$ and such that the $D_i$ are pairwise disjoint. Then:
    $$\Prob\left(A\mid \bigsqcup_{i\in I}D_i\right)=p$$
  \end{lemma}
  \begin{proof}
    \begin{align*}
      \Prob\left(A\mid \bigsqcup_{i\in I}D_i\right) & =\frac{\Prob(A\cap \bigsqcup_{i\in I}D_i)}{\Prob(\bigsqcup_{i\in I}D_i)} \\
                                                    & =\frac{\sum_{i\in I}\Prob(A\cap D_i)}{\sum_{i\in I}\Prob(D_i)}           \\
                                                    & =\frac{\sum_{i\in I}\Prob(A\mid D_i)\Prob(D_i)}{\sum_{i\in I}\Prob(D_i)} \\
                                                    & =\frac{\sum_{i\in I}p\Prob(D_i)}{\sum_{i\in I}\Prob(D_i)}                \\
                                                    & =p
    \end{align*}
  \end{proof}
  \begin{theorem}
    Let $(X_n)$ be a time-homogeneous Markov chain. Then, $\vf{P}^{(n)}=\vf{P}^n$.
  \end{theorem}
  \begin{proof}
    By induction on $n$. The case $n=1$ is clear. For $n\geq 2$ we have:
    \begin{align*}
      p_{ij}^{(n+1)} & =\Prob(X_{n+1}=j\mid X_0=i)                     \\
                     & =\sum_{k\in I}\Prob(X_{n+1}=j, X_n=k\mid X_0=i) \\
      \begin{split}
        &=\sum_{k\in I}\Prob(X_{n}=k\mid X_0=i)\cdot\\
        &\hspace{2.5cm}\cdot\Prob(X_{n+1}=j\mid X_n=k, X_0=i)
      \end{split}             \\
                     & =\sum_{k\in I}p_{ik}^{(n)}p_{kj}^{(1)}
    \end{align*}
    where the penultimate equality follows from \mcref{SP:lema1Markov} and the last equality follows from \mcref{SP:lema2Markov} and the Markov property because if $D=\{X_n=k, X_0=i\}$ we have that:
    \begin{multline*}
      D=\bigsqcup_{i_1,\ldots,i_{n-1}\in I}\{X_n=k, X_0=i,X_1=i_1,\ldots, X_{n-1}=\\=i_{n-1}\}
    \end{multline*}
    and so:
    \begin{multline*}
      \Prob(X_{n+1}=j\mid X_n=k, X_0=i)=\Prob(X_{n+1}=j\mid X_n=k,\\ X_0=i,X_1=i_1,\ldots,X_{n-1}=i_{n-1})=\\=\Prob(X_{n+1}=j\mid X_n=k)
    \end{multline*}
    Therefore, $\vf{P}^{(n+1)}=\vf{P}^n\vf{P}$, by induction hypothesis.
  \end{proof}
  \begin{theorem}[Chapman-Kolmogorov equation]\label{SP:ChapKolmo}
    Let $(X_n)$ be a time-homogeneous Markov chain and $i,j\in I$. Then:
    $$p_{ij}^{(m+n)}=\sum_{k\in I}p_{ik}^{(m)}p_{kj}^{(n)}$$
  \end{theorem}
  \begin{proposition}
    Let $(X_n)$ be a time-homogeneous Markov chain. Then:
    \begin{enumerate}
      \item $\vf{P}^{(0)}=\vf{I}_I$
      \item $\vf{\pi}^{(n)}=\vf{\pi}^{(0)}\vf{P}^n$
      \item $\Prob(X_0=i_0,\ldots,X_n=i_n)=\pi_{i_0}^{(0)}p_{i_0i_1}\cdots p_{i_{n-1}i_n}$
    \end{enumerate}
  \end{proposition}
  \begin{sproof}
    \begin{enumerate}
      \item $p_{ij}^{(0)}=\Prob(X_1=j\mid X_0=i)=\delta_{ij}$.
      \item
            \begin{multline*}
              \pi_i^{(n)}=\Prob(X_n=i)=\sum_{k\in I}\Prob(X_n=i\mid X_0=k)\cdot\\\cdot\Prob(X_0=k)=\sum_{k\in I}\pi_k^{(0)}p_{ki}^{(n)}
            \end{multline*}
      \item Use the \mnameref{P:totalprob} and the Markov property.
    \end{enumerate}
  \end{sproof}
  \subsubsection{Classification of states}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We say that a state $j\in I$ is \emph{reachable} from $i\in I$ if $\exists n\in \NN\cup\{0\}$ such that $p_{ij}^{(n)}>0$. In this case we will write $i\to j$.
  \end{definition}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We say that two states $i,j\in I$ \emph{communicate} if $i\to j$ and $j\to i$. In this case we will write $i\leftrightarrow j$.
  \end{definition}
  \begin{lemma}
    Let $(X_n)$ be a time-homogeneous Markov chain. Then, the relation $\leftrightarrow$ is an equivalence relation.
  \end{lemma}
  \begin{proof}
    The reflexivity and symmetry are clear. For the transitivity, suppose $i\leftrightarrow j$ and $j\leftrightarrow k$. Then, $\exists n,m\in I$ such that $p_{ij}^{(n)}>0$ and $p_{jk}^{(m)}>0$. Then by \mnameref{SP:ChapKolmo}:
    \begin{equation}\label{SP:corolariChapKolmo}
      p_{ik}^{(n+m)}=\sum_{\ell\in I} p_{i\ell}^{(n)}p_{\ell k}^{(m)}\geq p_{ij}^{(n)}p_{jk}^{(m)}>0
    \end{equation}
    Similarly, we have $p_{ki}^{(r+s)}>0$ for some $r,s\in I$. Therefore, $i\leftrightarrow k$.
  \end{proof}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. A subset $C\subseteq I$ is called \emph{irreducible class} if for any $i,j\in C$ we have $i\leftrightarrow j$. That is, if $C$ is an equivalence class of $\leftrightarrow$. If all the states are in the same equivalence class, then the Markov chain is called an \emph{irreducible chain}.
  \end{definition}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain and $i\in I$. We define the \emph{period} of $i$ as:
    $$d(i):=\gcd\{n\in\NN:p_{ii}^{(n)}>0\}$$
    with the convention that if $\{n\in\NN:p_{ii}^{(n)}>0\}=\varnothing$, then $d(i)=\infty$. If $d(i)=1$ we say that $i$ is \emph{aperiodic}.
  \end{definition}
  \begin{proposition}
    Let $(X_n)$ be a time-homogeneous Markov chain and $i,j\in I$. Then: $$i\leftrightarrow j\implies d(i)=d(j)$$
  \end{proposition}
  \begin{proof}
    Suppose $i\ne j$. We will see that if $p_{jj}^{(n)}>0$, then $d(i)\mid n$. Since $i\leftrightarrow j$, then $\exists r,s\in I$ such that $p_{ij}^{(r)}>0$ and $p_{ji}^{(s)}>0$. So as in \mcref{SP:corolariChapKolmo}, we have $p_{ii}^{(r+s)}>0$. Thus, $d(i)\mid r+s$. Moreover if $p_{jj}^{(n)}>0$, then:
    $$p_{ii}^{(r+n+s)}\geq p_{ij}^{(r)}p_{jj}^{(n)}p_{ji}^{(s)}>0$$
    So $d(i)\mid r+n+s$. Thus, $d(i)\mid n$ and so $d(j)\geq d(i)$ because $d(j)$ is the greatest common divisor of all such $n$. Repeating the argument exchanging $i$ and $j$ we get $d(j)= d(i)$.
  \end{proof}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. If the chain is irreducible, we will denote the common period by $d$. If $d=1$ we say that the chain is \emph{aperiodic}.
  \end{definition}
  \begin{proposition}
    Let $(X_n)$ be a time-homogeneous Markov chain. Suppose we have an irreducible chain of period $d>1$. Then, there exist subsets $C_0,\ldots,C_{d-1}\subseteq I$ such that $I=C_0\sqcup\cdots\sqcup C_{d-1}$ and such that if $j\in C_\alpha$, then:
    $$p_{jk}>0\implies k\in C_{{[\alpha+1]}_d}$$
    for all $k\in I$. Here ${[\alpha+1]}_d$ denotes $\alpha+1\mod{d}$.
  \end{proposition}
  \begin{proof}
    Let $i\in I$ and define
    $$C_\alpha:=\{j\in I:\exists n\in\NN\cup\{0\}\text{ with }p_{ij}^{(n d+\alpha)}>0\}$$
    Clearly $C_0\cup\cdots\cup C_{d-1}=I$. Let's see that $C_\alpha\cap C_\beta=\varnothing$ if $\alpha\ne\beta$. Suppose $k\in C_\alpha\cap C_\beta$. Note that since the chain is irreducible, $\exists m\in\NN\cup\{0\}$ such that $p_{ki}^{(m)}>0$. And so, as in \mcref{SP:corolariChapKolmo} we have $p_{kk}^{(n d+\alpha+m)}>0$ because $k\in C_\alpha$. Thus, $d\mid \alpha+m$. The same argument with $\beta$ implies $d\mid \beta+m$. So $d\mid \beta -\alpha$ and $\beta=\alpha$ because $\alpha,\beta\in\{0,\cdots,d-1\}$.

    Finally if $j\in C_\alpha$ is such that $p_{jk}>0$ for $k\in I$, then as in \mcref{SP:corolariChapKolmo} we have $p_{ik}^{(n d+\alpha +1)}>0$. So, if $\alpha+1\leq d-1$, then $k\in C_{\alpha+1}$. Otherwise $k\in C_0=C_{[\alpha+1]_d}$.
  \end{proof}
  \subsubsection{Stopping time and strong Markov property}
  \begin{proposition}\label{SP:MarkovImproved}
    Let $(X_n)$ be a time-homogeneous Markov chain, $k\in\NN$ and $A\subseteq I^k$ and $B\subseteq I^n$. Then:
    \begin{multline*}
      \Prob((X_{n+1},\ldots,X_{n+k})\in A\mid\!(X_0,\ldots,X_{n-1})\in B,X_n=i)=\\
      =\Prob((X_{n+1},\ldots,X_{n+k})\in A\mid X_n=i)=\\
      =\Prob((X_{1},\ldots,X_{k})\in A\mid X_0=i)
    \end{multline*}
    for all $n\geq 0$.
  \end{proposition}
  \begin{proof}
    By \mcref{SP:lema2Markov} it suffices to prove the statement for $B=\{i_0\}\times\cdots\times\{i_{n-1}\}$. Moreover since $A$ is countable we can suppose $A=\{j_1\}\times\cdots\times\{j_k\}$. We will prove it by induction on $k$ the homogeneous equality (the other one is even easier). The case $k=1$ is by definition. Now suppose $k\geq 2$. Then, denoting $C:=\{X_0=i_0,\ldots,X_{n-1}=i_{n-1},X_n=i\}$ we have:
    \begin{multline*}
      \Prob(X_{n+1}=j_1,\ldots,X_{n+k+1}=j_{k+1}\mid C) \\
      =\Prob(X_{n+k+1}=j_{k+1}\mid C,X_{n+1}=j_1,\ldots,X_{n+k}=j_k)\cdot\\
      \cdot\Prob(X_{n+1}=j_1,\ldots,X_{n+k}=j_k\mid C) \\
      =\Prob(X_{k+1}=j_{k+1}\mid X_0=i,X_{1}=j_1,\ldots,X_{k}=j_k)\cdot\\
      \cdot\Prob(X_{1}=j_1,\ldots,X_{k}=j_k\mid X_0=i) \\
      =\Prob(X_{1}=j_1,\ldots,X_{k+1}=j_{k+1}\mid X_0=i)
    \end{multline*}
    where in the second equality we have used the Markov property, the homogeneous property and induction hypothesis.
  \end{proof}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and let $I$ be a finite or countable set. For each $i\in I$, let $\mathcal{F}_i$ be a sub $\sigma$-algebra of $\mathcal{A}$, that is a subset of $\mathcal{A}$ which also $\sigma$-algebra. We say that $(\mathcal{F}_i)_{i\in I}$ is \emph{filtration} if for all $i\in I$ we have $\mathcal{F}_i\subseteq\mathcal{F}_{i+1}$. The tuple $(\Omega,\mathcal{A},(\mathcal{F}_i)_{i\in I},\Prob)$ is called a \emph{filtration space}.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}$ be a random vector. the \emph{$\sigma$-algebra generated by $\vf{X}$} is:
    $$\sigma(\vf{X}):=\{\vf{X}^{-1}(B):B\in\mathcal{B}(\RR^n)\}$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a time-homogeneous Markov chain. Then, if $\mathcal{F}_n:=\sigma(X_0,\ldots,X_n)$, the sequence $(\mathcal{F}_n)_{n\geq 0}$ is a filtration.
  \end{proposition}
  \begin{proof}
    Take $F\in\mathcal{F}_n$. Then:
    \begin{multline*}
      F=\{(X_0,\ldots,X_n)\in B\subseteq I^{n+1}\}=\\=\{(X_0,\ldots,X_n,X_{n+1})\in B\times I\subseteq I^{n+2}\}\in\mathcal{F}_{n+1}
    \end{multline*}
  \end{proof}
  \begin{definition}
    Let $(\Omega,\mathcal{F},(\mathcal{F}_n)_{n\geq 0},\Prob)$ be a filtration space and $\tau$ a random variable on it with support $\NN\cup\{0\}$. We say that $\tau$ is a \emph{stopping time} if $\forall n\geq 0$ we have:
    $$\{\tau= n\}\in\mathcal{F}_n$$
  \end{definition}
  \begin{remark}
    Intuitively, this condition means that the ``decision" of whether to stop at time $n$ must be based only on the information present at time $n$, not on any future information.
  \end{remark}
  \begin{lemma}
    Let $(\Omega,\mathcal{F},(\mathcal{F}_n)_{n\geq 0},\Prob)$ be a filtration space and $\tau$ be a random variable. Then:
    $$\tau\text{ is a stopping time}\iff\{\tau\leq n\}\in\mathcal{F}_n$$
  \end{lemma}
  \begin{proof}
    \begin{itemizeiff}
      $\displaystyle\{\tau\leq n\}=\bigsqcup_{m=1}^n\{\tau=m\}\in \mathcal{F}_n$
      because $\{\tau=m\}\in \mathcal{F}_{m}\subseteq \mathcal{F}_{n}$ $\forall m\leq n$.
      \item $\displaystyle\{\tau = n\}=\{\tau\leq n\}\setminus\{\tau\leq n-1\}\in \mathcal{F}_n$
      because $\{\tau\leq n-1\}\in \mathcal{F}_{n-1}\subseteq \mathcal{F}_{n}$.
    \end{itemizeiff}
  \end{proof}
  \begin{proposition}
    Let $(X_n)$ be a time-homogeneous Markov chain, $(\Omega,\mathcal{F},(\mathcal{F}_n)_{n\geq 0},\Prob)$ be a filtration space defined with $(X_n)$, $i\in I$ and $\tau_i$ be the random variable with support $\NN\cup\{0,\infty\}$ defined by:
    \begin{equation}\label{SP:tau_i}
      \tau_i(\omega)=\inf\{n\geq 1:X_n(\omega)=i\}
    \end{equation}
    with the convention that $\inf\varnothing=+\infty$.
    Then, $\tau_i$ is a stopping time.
  \end{proposition}
  \begin{proof}
    If $n=0$, then $\{\tau_i=0\}=\varnothing\in\mathcal{F}_0$. If $n=1$, then $\{\tau_i=1\}=\{X_0\in I,X_1=i\}\in\mathcal{F}_1$. If $n\geq 2$, then:
    $$\{\tau_i=n\}=\{X_0\in I,X_1,\ldots,X_{n-1}\in  \{i\}^c,X_n=i\}\in\mathcal{F}_n$$
  \end{proof}
  \begin{theorem}[Strong Markov property]\label{SP:MarkovStrong}
    Let $(X_n)$ be a time-homogeneous Markov chain, $(\Omega,\mathcal{F},(\mathcal{F}_n)_{n\geq 0},\Prob)$ be a filtration space defined with $(X_n)$ and $\tau$ be a stopping time. Suppose that $\Prob(\tau<\infty)>0$. Then:
    \begin{multline*}
      \Prob(X_{\tau+n+1}=j\mid X_{\tau}=i_0,\ldots,X_{\tau+n-1}=i_{n-1},X_{\tau +n}=i,\\\tau<\infty)=\Prob(X_{\tau+n+1}=j\mid X_{\tau+n}=i,\tau<\infty)=\\=\Prob(X_{1}=j\mid X_0=i)
    \end{multline*}
    on account that $\Prob(A)>0$, where $A:=\{X_{\tau}=i_0,\ldots,X_{\tau+n-1}=i_{n-1},X_{\tau +n}=i,\tau<\infty\}$.
  \end{theorem}
  \begin{proof}
    \begin{multline*}
      \Prob(X_{\tau+n+1}=j\mid A) =\sum_{m=0}^{\infty}\Prob(X_{\tau+n+1}=j,\tau=m\mid A)= \\
      =\sum_{\substack{m=0 \\\Prob(\tau=m,A)>0}}^{\infty}\Prob(X_{m+n+1}\mid A,\tau=m)\Prob(\tau=m\mid A)
    \end{multline*}
    Now note that since $\{\tau=m\}\in\mathcal{F}_m$, we can write:
    \begin{equation*}
      \{\tau=m\}=\bigsqcup_{j_0,\ldots,j_m}\{X_0=j_0,\ldots,X_m=j_m\}
    \end{equation*}
    for some $j_0,\ldots,j_m\in I$. But since $\Prob(\tau=m,A)>0$, we have that in this last expression $j_m=i_0$ and so using \mcref{SP:MarkovImproved} we get:
    \begin{multline*}
      \Prob(X_{\tau+n+1}=j\mid A) =\sum_{\substack{m=0 \\\Prob(\tau=m,A)>0}}^{\infty}\Prob(X_1=j\mid X_0=i)\cdot\\\cdot\Prob(\tau=m\mid A)=\Prob(X_1=j\mid X_0=i)
    \end{multline*}
  \end{proof}
  \begin{corollary}[Strong Markov property]\label{SP:MarkovStrong2}
    Let $(X_n)$ be a time-homogeneous Markov chain, $(\Omega,\mathcal{F},(\mathcal{F}_n)_{n\geq 0},\Prob)$ be a filtration space defined with $(X_n)$, $\tau$ be a stopping time, $k\in\NN$ and $A\subseteq I^k$ and $B\subseteq I^n$. Suppose that $\Prob(\tau<\infty)>0$. Then:
    \begin{multline*}
      \Prob((X_{\tau+n+1},\ldots,X_{\tau+n+k})\in A\mid (X_{\tau},,\ldots,X_{\tau+n-1})\in B, \\X_{\tau +n}=i,\tau<\infty)=\Prob((X_{1},\ldots,X_{k})\in A\mid X_0=i)
    \end{multline*}
    for all $n\geq 0$.
  \end{corollary}
  \subsubsection{Recurrence and transience}
  From now on we will omit saying that a stopping time $\tau$ is defined in a filtration space $(\Omega,\mathcal{F},(\mathcal{F}_n)_{n\geq 0},\Prob)$. Moreover given a Markov chain $(X_n)$, we will denote by $\Prob_i(A):=\Prob(A\mid X_0=i)$, for any event $A$.
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain, $i,j\in I$ and consider the stopping time $\tau_j$ of \mcref{SP:tau_i}. We define $f_{ij}:=\Prob_i(\tau_j<\infty)$. We say that $i$ is \emph{transient} if $f_{ii}<1$ and \emph{recurrent} if $f_{ii}=1$. Finally we define $N_i$ as:
    $$
      N_i:=\abs{\{n\in\NN:X_n=i\}}=\sum_{n=1}^{\infty}\indi{\{X_n=i\}}
    $$
  \end{definition}
  \begin{remark}
    Roughly speaking, if $i$ is recurrent it means that the chain will return at least once to $i$. On the other hand, if $i$ is transient, it means that the chain may never return to $i$.
  \end{remark}
  \begin{proposition}\label{SP:recurrence}
    Let $(X_n)$ be a time-homogeneous Markov chain and $i\in I$. Then:
    $$
      \Prob_i(N_i\geq k)={(f_{ii})}^k
    $$
  \end{proposition}
  \begin{proof}
    First suppose $f_{ii}=0$. Then:
    $$
      \Prob_i(N_i\geq k)\leq\Prob_i(N_i\geq 1)=\Prob_i(\tau_i<\infty)=f_{ii}=0
    $$
    Now assume $f_{ii}=\Prob_i(\tau_i<\infty)>0$. We will prove the statement by induction on $k$. The case $k=1$ is clear. For $k\geq 2$, we define:
    $$
      \tau_i^k:= \inf\{n>\tau_i^{k-1}:X_n=i\}
    $$
    with the convention that $\tau_i^1=\tau_i$. An easy check shows that $\tau_i^k$ is a stopping time $\forall k\in\NN$ and $\{\tau_i^k<\infty\}\subseteq \{\tau_i^{k-1}<\infty\}$. Thus:
    \begin{multline*}
      \Prob_i(N_i\geq k) =\Prob_i(\tau_i^k<\infty)=\Prob_i(\tau_i^k<\infty,\tau_i^{k-1}<\infty)\\=\Prob_i(\tau_i^{k-1}<\infty)\Prob_i(\tau_i^k<\infty\mid\tau_i^{k-1}<\infty)=\\={(f_{ii})}^{k-1}\Prob_i(\tau_i^k<\infty\mid\tau_i^{k-1}<\infty)
    \end{multline*}
    So it's missing to prove that $\Prob_i(\tau_i^k<\infty\mid\tau_i^{k-1}<\infty)={f_{ii}}$. But:
    \begin{multline*}
      \Prob_i(\tau_i^k<\infty\mid\tau_i^{k-1}<\infty)=\\=\sum_{m=1}^{\infty}\Prob_i(\tau_i^k=m+\tau_i^{k-1}\mid\tau_i^{k-1}<\infty)\\=\sum_{m=1}^{\infty}\Prob_i(\tau_i^k=m+\tau_i^{k-1}\mid X_{\tau_i^{k-1}}=i,\tau_i^{k-1}<\infty)
      \\=\sum_{m=1}^{\infty}\Prob_i( X_{\tau_i^{k-1}+1}\ne i,\ldots,X_{\tau_i^{k-1}+m-1}\ne i,\\X_{\tau_i^{k-1}+m}=i\mid X_{\tau_i^{k-1}}=i,\tau_i^{k-1}<\infty)
      \\=\sum_{m=1}^{\infty}\Prob_i( X_{1}\ne i,\ldots,X_{m-1}\ne i,X_{m}=i\mid X_{0}=i)=\\=\sum_{m=1}^{\infty}\Prob_i(\tau_i=m)=\Prob_i(\tau_i<\infty)=f_{ii}
    \end{multline*}
    where we have used the \mnameref{SP:MarkovStrong}.
  \end{proof}
  \begin{theorem}
    Let $(X_n)$ be a time-homogeneous Markov chain and $i\in I$. Then:
    \begin{itemize}
      \item $i$ is recurrent $\implies\Prob_i(N_i=\infty)=1$
      \item $i$ is transient $\implies\Prob_i(N_i<\infty)=1$
    \end{itemize}
  \end{theorem}
  \begin{proof}
    Note that $\{N_i\geq k\}\searrow\{N_i=\infty\}$, so by \mcref{SP:recurrence} we get:
    \begin{multline*}
      \Prob_i(N_i=\infty)=\lim_{k\to\infty}\Prob_i(N_i\geq k)={(f_{ii})}^k=\\=\begin{cases}
        1 & \text{if }i\text{ is recurrent} \\
        0 & \text{if }i\text{ is transient}
      \end{cases}
    \end{multline*}
  \end{proof}
\end{multicols}
\end{document}