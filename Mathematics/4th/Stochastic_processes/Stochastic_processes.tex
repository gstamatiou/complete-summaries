\documentclass[../../../main_math.tex]{subfiles}


\begin{document}
\changecolor{SP}
\begin{multicols}{2}[\section{Stochastic processes}]
  \subsection{Preliminaries}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be sequence of random variables such that: $$\sum_{n=1}^{\infty}\Exp(\abs{X_n})<\infty$$
    Then, $\sum_{n=1}^{\infty}X_n$ is a random variable and: $$
      \Exp\left(\sum_{n=1}^{\infty}X_n\right)=\sum_{n=1}^{\infty}\Exp(X_n)
    $$
  \end{proposition}
  \begin{proof}
    By the \mnameref{P:monotone} we have that
    $$\Exp\left(\sum_{n=1}^{\infty}\abs{X_n}\right)=\sum_{n=1}^{\infty}\Exp(\abs{X_n})<\infty$$
    so the random variable $Y:=\sum_{n=1}^{\infty}\abs{X_n}$ is integrable and $\forall N\in\NN$ satisfies: $$\abs{\sum_{n=1}^{N}X_n}\leq Y$$
    Now use the \mnameref{P:dominated}.
  \end{proof}
  \begin{proposition}[Law of total probability]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be such that $\bigsqcup_{n=1}^N A_n=\Omega'$ with $\Prob(\Omega')=1$. Then, $\forall A\in\mathcal{A}$: $$\Prob(A)=\sum_{n=1}^N\Prob(A_n)\Prob(A\mid A_n)$$
  \end{proposition}
  \begin{sproof}
    See the proof of \mnameref{P:totalprob}.
  \end{sproof}
  \begin{remark}
    Note that if there is some $A_n$ for which $\Prob(A_n)=0$, the conditional probability is not well-defined. But note that:
    $$0\leq \Prob(A_n)\Prob(A\mid A_n)=\Prob(A\cap A_n)\leq \Prob(A_n)=0$$
    So we can omit this term in the sum.
  \end{remark}
  \subsubsection{Conditional expectation}
  \begin{proposition}[Substitution principle]\label{SP:substitutionPrinciple}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}$ be a discrete random vector with outcomes in $S_{\vf{X}}$, $Y$ be a discrete random variable with outcomes in $S_Y$, $h:S_{\vf{X}}\times S_Y\rightarrow\RR$ be a function and $y\in S_Y$. Then:
    $$\Exp(h(\vf{X},Y=y)\mid Y=y)=\Exp((h(\vf{X}),Y)\mid Y=y)$$
  \end{proposition}
  \begin{proposition}[Law of total expectation]\label{SP:totalexp}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}$ is a discrete random vector with outcomes in $S_{\vf{X}}$, $\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be such that $\bigsqcup_{n=1}^N A_n=\Omega'$ with $\Prob(\Omega')=1$ and $h:S_{\vf{X}}\rightarrow\RR$ be a function. If $h(\vf{X})$ has finite expectation or $h\geq 0$, then: $$\Exp(h(\vf{X}))=\sum_{n=1}^N\Exp(h(\vf{X})\mid A_n)\Prob(A_n)$$
  \end{proposition}
  \begin{sproof}
    See the proof of \mnameref{P:totalexp}.
  \end{sproof}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}$ be a discrete random vector with outcomes in $S_{\vf{X}}$, $Y$ be a discrete random variable with outcomes in $S_Y$ and $h:S_{\vf{X}}\rightarrow\RR$ be a function. For all $\omega\in\Omega$ we define the random variable $\Exp(h(\vf{X})\mid Y)$ as
    $$\Exp(h(\vf{X})\mid Y)(\omega)=\Exp(h(\vf{X})\mid Y=y)$$
    provided that $Y(\omega)=y$.
    Note that it can also be written as:
    $$\Exp(h(\vf{X})\mid Y)=\sum_{y\in S_Y}\Exp(h(\vf{X})\mid Y=y)\indi{\{Y=y\}}$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}$ be a discrete random vector with outcomes in $S_{\vf{X}}$, $Y$ be a discrete random variable with outcomes in $S_Y$, $h,h_1,h_2:S_{\vf{X}}\rightarrow\RR$ be functions and $a,b\in\RR$. Then:
    \begin{enumerate}
      \item $\Exp(ah_1(\vf{X})+bh_2(\vf{X})\mid Y)=a\Exp(h_1(\vf{X})\mid Y)+b\Exp(h_1(\vf{X})\mid Y)$
      \item If $\vf{X}$ and $Y$ are independent, then $\Exp(h(\vf{X})\mid Y)=\Exp(h(\vf{X}))$.
      \item $\Exp(\Exp(h(\vf{X})\mid Y))=\Exp(h(\vf{X}))$.
    \end{enumerate}
  \end{proposition}
  \begin{sproof}
    The first two properties are consequence of the fact that the conditional expectation is an expectation. For the last one note that:
    \begin{align*}
      \Exp(\Exp(h(\vf{X})\mid Y)) & =\sum_{y\in S_y}\Exp(h(\vf{X})\mid Y=y)\Prob(Y=y) \\
                                  & =\Exp(h(\vf{X}))
    \end{align*}
    where the last equality is due to the \mnameref{SP:totalexp}.
  \end{sproof}
  \begin{theorem}[Wald theorem]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(Z_n)$ be a sequence of random variables, all of them with expectation $\mu\in\RR$, such that $\sup_{n\geq 1}\Exp(\abs{Z_n})=A<\infty$. If $N$ is an integrable random variable with outcomes in $\NN$ independent of $Z_n$ $\forall n\in\NN$, we have that:
    $$\Exp\left(\sum_{n=1}^NZ_n\right)=\mu\Exp(N)$$
  \end{theorem}
  \begin{proof}
    Note that $\Exp\left(\sum_{n=1}^NZ_n\right)=\Exp\left(\sum_{n=1}^\infty Z_n\indi{N\geq n}\right)$ and it is integrable because:
    \begin{align*}
      \Exp\left(\sum_{n=1}^\infty \abs{Z_n\indi{N\geq n}}\right) & =\sum_{n=1}^\infty \Exp(\abs{Z_n})\Exp(\indi{N\geq n}) \\
                                                                 & \leq A\sum_{n=1}^\infty \Prob(N\geq n)                 \\
                                                                 & =A\Exp(N)                                              \\
                                                                 & <\infty
    \end{align*}
    where we have used the Independence of $(Z_n)$ and $N$ in the first equality.
    And so:
    $$\Exp\left(\sum_{n=1}^NZ_n\right)=\sum_{n=1}^\infty \Exp(Z_n)\Exp(\indi{N\geq n})=\mu\Exp(N)$$
  \end{proof}
  \begin{remark}
    Note that the equality remains true if $Z_n\geq 0$ $\forall n\in\NN$ even if $\Exp(N)=\infty$ because the equality $\sum_{n=1}^\infty \Prob(N\geq n)=\Exp(N)$ remains true.
  \end{remark}
  \subsubsection{Probability-generating function}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable with outcomes in $\NN\cup\{0\}$. The \emph{probability-generating function} (or \emph{pgf}) of $X$ is the function $g_X:\mathcal{D}_X\rightarrow \RR$ defined as:
    \begin{equation}\label{SP:probgenfunc}
      g_X(s)=\sum_{k=0}^\infty s^k\Prob(X=k)=\Prob(X=0)+\sum_{k=1}^\infty s^k\Prob(X=k)
    \end{equation}
    The set $\mathcal{D}_X$ is defined as a all the points for which the series of \mcref{SP:probgenfun} converges absolutely.
  \end{definition}
  \begin{lemma}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable with outcomes in $\NN\cup\{0\}$. Then, $[-1,1]\subseteq \mathcal{D}_X$ and if $s\ne 0$, $g_X(s)=\Exp(s^X)$.
  \end{lemma}
  \begin{proof}
    Clearly $g_X(s)=\Exp(s^X)$ and furthermore $\forall s\in[-1,1]$ we have $s\in \mathcal{D}_X$ because:
    $$\sum_{k=0}^\infty \abs{s}^k\Prob(X=k)\leq \sum_{k=0}^\infty \Prob(X=k)=1<\infty$$
  \end{proof}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be random variables with outcomes in $\NN\cup\{0\}$. Then: $$X\overset{\text{d}}{=}Y\iff g_X=g_Y$$
    Moreover: $$\Prob(X=k)=\frac{{g_X}^{(k)}(0)}{k!}\quad \forall k\geq 0$$
  \end{theorem}
  \begin{sproof}
    Note that $g_X(s)$ is a power series defined in a neighbourhood of $s=0$.
  \end{sproof}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be independent random variables with outcomes in $\NN\cup\{0\}$. Then, $\forall s\in\mathcal{D}_X\cap \mathcal{D}_Y$ we have: $$g_{X+Y}(s)=g_X(s)g_Y(s)$$
  \end{theorem}
  \begin{proof}
    Note that if $s\in \in\mathcal{D}_X\cap \mathcal{D}_Y$, then $s\in\mathcal{D}_{X+Y}$ because
    $$g_{X+Y}(\abs{s})=\Exp\left(\abs{s}^{X+Y}\right)=\Exp\left(\abs{s}^X\right)\Exp\left(\abs{s}^Y\right)<\infty$$
    due to the independence of $X$ and $Y$.
    To show the equality if $s=0$, we have:
    $$\Prob(X+Y=0)=\Prob(X=0,Y=0)=\Prob(X=0)\Prob(Y=0)$$
    where the first equality is due to $X,Y\in\NN\cup\{0\}$ and the second one is becuse of the independence.
    If $s\ne 0$, as before: $$g_{X+Y}({s})=\Exp({s}^{X+Y})=\Exp({s}^X)\Exp({s}^Y)=g_{X}({s})g_{Y}({s})$$
  \end{proof}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable with outcomes in $\NN\cup\{0\}$. Then, $\forall k\geq 1$ we have:
    $$\lim_{s\to 1^-}{g_X}^{(k)}(s)=\Exp(X(X-1)\cdots(X-k+1))$$
  \end{theorem}
  \begin{sproof}
    Differentiate term by term and use \mnameref{MA:abelthm}.
  \end{sproof}
  \begin{center}
    \def\arraystretch{1.3}
    \begin{tabular}{|c|c|c|}
      \hline
      $X$                     & Probability-generating function     \\
      \hline
      $c\in\RR$               & $\displaystyle s^{c}$               \\
      %\hline
      $U(\{x_1,\ldots,x_n\})$ & $\frac{1}{n}\sum_{i=1}^n s^{x_i}$   \\
      %\hline
      $\text{B}(n,p)$         & $\displaystyle {(ps+1-p)}^n$        \\
      %\hline
      $\text{Pois}(\lambda)$  & $\displaystyle \exp{\lambda(s-1)}$  \\
      %\hline
      $\text{Geo}(p)$         & $\displaystyle \frac{ps}{1-(1-p)s}$ \\
      %\hline
      \hline
    \end{tabular}
    \captionof{table}{Probability-generating functions of common distributions.}
  \end{center}
  \subsection{Discrete-time Markov chains}
  \subsubsection{Galton-Watson process}
  \begin{model}\label{SP:galtonwatsonModel}
    Let $(X_n)$, $n\in\NN\cup\{0\}$ be a sequence of discrete random vairables representing the number of new individuals of a certain population at the $n$-th generation. Suppose they are defined as $$X_{n+1}=\sum_{k=1}^{X_n}Y_{n+1}^{(k)}$$ and $X_0=1$. Here $Y_{n+1}^{(k)}$ has outcomes in $\NN\cup\{0\}$ $\forall n,k$ and represent the number of descendants (to the next generation) of the $k$-th individual of the $n$-th generation. Suppose that $Y_{n+1}^{(k)}\sim Y$ are \iid We would like to study the probability $\rho$ of extinction of this population: $$\rho=\Prob(\{X_n=0:\text{for some $n\in\NN$}\})=\Prob\left(\bigcup_{n=1}^\infty\{X_n=0\}\right)$$
  \end{model}
  \begin{lemma}\label{SP:lemmaGaltonWatson}
    Let $(Y_n)$ be a sequence of \iid random variables distributed as $Y$, whose outcomes are in $\NN\cup\{0\}$, and $N$ be a random variable also with outcomes in $\NN\cup\{0\}$ and independent to $(Y_n)$. Let $X=\sum_{k=1}^NY_k$. Then, $\forall s\in[-1,1]$ we have: $$g_X(s)=g_N(g_Y(s))$$
  \end{lemma}
  \begin{proof}
    First suppose $N\leq M$ with $M\in\NN$ fixed. Then using the independence, the \mnameref{P:totalexp} and the \mnameref{SP:substitutionPrinciple}:
    \begin{align*}
      g_X(s)=\Exp(s^X) & =\sum_{k=1}^M\Exp(s^X\mid N=k)\Prob(N=k)                                        \\
                       & = \sum_{k=1}^{M} \Exp\left(s^{\sum_{i=1}^{N}Y_i} \mid N = k\right) \Prob(N = k) \\
                       & = \sum_{k=1}^{M} \Exp\left(s^{\sum_{i=1}^{k}Y_i}\right) \Prob(N = k)            \\
                       & = \sum_{k=1}^{M} {g_Y(s)}^k \Prob(N = k)                                        \\
                       & = g_N(g_Y(s))
    \end{align*}
    Now if $N$ can take any value of $\NN\cup\{0\}$ we have that:
    \begin{align*}
      g_X(s) & =\Exp\left(s^{\sum_{i=1}^{N}Y_i}\right)                          \\
             & =\Exp\left(\lim_{M\to\infty}s^{\sum_{i=1}^{\min(N,M)}Y_i}\right) \\
             & =\lim_{M\to\infty}\Exp\left(s^{\sum_{i=1}^{\min(N,M)}Y_i}\right) \\
             & =\lim_{M\to\infty}g_{\min(N,M)}(g_Y(s))                          \\
             & =\lim_{M\to\infty}\Exp\left({(g_Y(s))}^{\min(N,M)}\right)        \\
             & =\Exp\left(\lim_{M\to\infty}{(g_Y(s))}^{\min(N,M)}\right)        \\
             & =g_N(g_Z(s))
    \end{align*}
    where both limit exchangings are due to the \mnameref{P:dominated} using the intagrable random variable 1.
  \end{proof}
  \begin{theorem}
    In the hypothesis of \mcref{SP:galtonwatsonModel}, we have that: $$\rho=g_Y(\rho)$$
  \end{theorem}
  \begin{proof}
    Note that $\{X_n=0\}\subseteq \{x_{n+1}=0\}$. Hence:
    $$\rho=\Prob\left(\bigcup_{n=1}^\infty\{X_n=0\}\right)\!=\lim_{n\to\infty}\Prob(X_n=0)=\lim_{n\to\infty}g_{X_n}(0)$$
    Now, using \mcref{SP:lemmaGaltonWatson} we have:
    $$g_{X_{n}}(s)=g_{X_{n-1}}(g_Y(s))=\cdots=g_{X_1}({g_Y}^n(s))$$
    But $X_1=1$ and so $g_{X_1}(s)=s$. So $g_{X_{n}}(s)={g_Y}^n(s)$ and therefore
    $g_{X_{n+1}}(0)=g_Y(g_{X_{n}}(0))$.
    Taking the limit as $n\to\infty$ and using the continuity of the pgf we get the result.
  \end{proof}
  \begin{theorem}
    In the hypothesis of \mcref{SP:galtonwatsonModel} and the additional assumption that $0<\Prob(Y=0)<1$ we have:
    \begin{enumerate}
      \item If $\Exp(Y)\leq 1$, $g_Y$ has only 1 fixed point (the trivial one, 1). Hence, the population will extinct with probability 1.
      \item If $\Exp(Y)> 1$, $g_Y$ has a unique non-trivial fixed point on (0,1).
    \end{enumerate}
  \end{theorem}
  \begin{proof}
    First suppose $\Prob(Y=0)+\Prob(Y=1)=1$. Thus, $Y\almoste{\leq}1$ and so $\Exp(Y)\leq 1$. Moreover, $g_Y(s)=\Prob(Y=0)+s\Prob(Y=1)$, which is a line with slope $\Prob(Y=1)<1$. Hence, it has a unique fixed point, which is $s=1$.

    Now assume $\Prob(Y=0)+\Prob(Y=1)<1$. Then, $\exists k\geq 2$ with $\Prob(Y=k)>0$. Hence, ${g_Y}'(s)>0$ and ${g_Y}''(s)>0$ $\forall s\in (0,1)$. Now consider $f(s)=g(s)-s$. Note that $f$ is strictly convex in $(0,1)$ and $f(0)=g(0)=\Prob(Y=0)>0$. Finally note that $$\lim_{t\to 1^-} f'(s)=\lim_{t\to 1^-}g(s)-1=\Exp(Y)-1$$
    and so $\displaystyle\lim_{t\to 1^-} f'(s)$ is negative in the first case and positive in the second case. This imply that $f$ has no zeros on $(0,1)$ in the first case and exactly 1 zero in $(0,1)$ in the second case.

    It's missing to see that in the second case the probability of extinction $\rho$ is given by the fixed point in $(0,1)$, rather than 1. We have that:
    $$\rho=\lim_{n\to\infty}g_{X_n}(0)=\lim_{n\to\infty}{g_Y}^n(0)$$
    Since $g'>0$, we have that $g$ is increasing and so it is $g^n$ $\forall n\in\NN$. Moreover if $g(x_0)=x_0$, we have that $g^n(x_0)=x_0$ $\forall n\in\NN$. Therefore $$0<g(0)<g^2(0)<\cdots<g^n(0)<\cdots <x_0<\cdots <1$$
    And so the limit has to be $x_0$ (note that the limit does exist because $({g_Y}^n(0))$ is an increasing bounded sequence).
  \end{proof}
  \subsubsection{Gambler's ruin}
  \begin{definition}[Gambler's ruin problem]
    Consider a gambler with an initial capital $z\in\ZZ$ and suppose that he plays a game in which wins 1 unit of capital with probability $p$ and loses 1 unit of capital with probability $q:=1-p$. The game ends whenever the player is ruined or if he arrives to a capital of $a\in\ZZ$. All the plays are independent. We denote by $(X_k)$ the variables that measure the $k$-th play. That is: $$\Prob(X_k=1)=p\qquad\Prob(X_k=-1)=q$$
    We define $q_z$ as the probability of ruining himself starting with a capital of $z$, $p_z$ as the probability of winning the game starting with a capital of $z$ and $D_z$ as the duration of the game starting with a capital of $z$.
  \end{definition}
  \begin{proposition}
    Consider the Gambler's ruin problem. Then:
    $$q_z=\begin{cases}
        \frac{-{\left(\frac{q}{p}\right)}^a+{\left(\frac{q}{p}\right)}^z}{1-{\left(\frac{q}{p}\right)}^a} & \text{if $p\ne 1/2$} \\
        1-\frac{z}{a}                                                                                     & \text{if $p= 1/2$}
      \end{cases}$$
  \end{proposition}
  \begin{sproof}
    We have that $q_k$ solves the differenc equation:
    \begin{multline*}
      q_k =\Prob(\text{ruin}\mid X_k=1)\Prob(X_1=1)+\\
      +\Prob(\text{ruin}\mid X_k=-1)\Prob(X_1=-1) =q_{k+1}p+q_{k-1}q
    \end{multline*}
    with $q_0=1$ and $q_a=0$, whose solution is straightforward.
  \end{sproof}
  \begin{proposition}
    Consider the Gambler's ruin problem. Suppose that we play against another player (and so when we lose, he wins and viceversa). Let $p_z^*$, $q_z^*$ be the respective probabilities for the other player. Then:
    $$q_z+q_z^*=1$$
    Hence, $D\almoste{<}\infty$.
  \end{proposition}
  \begin{sproof}
    Note that
    $$q_z^*=\begin{cases}
        \frac{-{\left(\frac{p}{q}\right)}^a+{\left(\frac{p}{a}\right)}^{a-z}}{1-{\left(\frac{p}{q}\right)}^a} & \text{if $p\ne 1/2$} \\
        1-\frac{a-z}{a}                                                                                       & \text{if $p= 1/2$}
      \end{cases}$$
  \end{sproof}
  \begin{proposition}
    Let $d_z=\Exp(D_z)$ and suppose that this expectation is finite. Then:
    $$
      d_z=\begin{cases}
        \frac{z}{q-p}-\frac{a}{q-p}\frac{1-{\left(\frac{q}{p}\right)}^z}{1-{\left(\frac{q}{p}\right)}^a} & \text{if $p\ne 1/2$} \\
        z(a-z)                                                                                           & \text{if $p= 1/2$}
      \end{cases}
    $$
  \end{proposition}
  \begin{proof}
    We have that $q_k$ solves the differenc equation:
    \begin{multline*}
      d_k =\Exp(D_k\mid X_k=1)\Prob(X_1=1)+\\
      +\Exp(D_k\mid X_k=-1)\Prob(X_1=-1) =\\=\Exp(D_{k+1}+1)p+\Exp(D_{k-1}+1)q= d_{k+1}p+d_{k-1}q+1
    \end{multline*}
    with $d_0=0$ and $d_a=0$, whose solution is straightforward. The second equality is due to the independence and the \mnameref{SP:substitutionPrinciple}.
  \end{proof}
  \subsubsection{Markov chains}
  \begin{definition}
    A \emph{Markov chain} is a sequence of random variables discrete random variables $(X_n)$ with support $I$ such that:
    \begin{multline*}
      \Prob(X_{n+1}=j\mid X_0=i_0,\ldots,X_{n-1}=i_{n-1},X_n=i)=\\=\Prob(X_{n+1}=j\mid X_n=i)
    \end{multline*} for all $i_0,\ldots,i_{n-1},i,j\in I$. This property is usually called \emph{Markov property}.
    If moreover $\Prob(X_{n+1}=j\mid X_n=i)$ does not depend on $n$, then we say that the Markov chain is a \emph{time-homogeneous Markov chain}.
  \end{definition}
  \begin{definition}[Stochastic matrix]
    A matrix $\vf{P}=(p_{ij})_{i,j\in I}$ is a \emph{stochastic matrix} if $p_{ij}\geq 0$ $\forall i,j\in I$ and: $$\sum_{j\in I}p_{ij}=1$$
  \end{definition}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We define $p_{ij}$ as the probability of going from state $i$ to state $j$. That is: $$p_{ij}=\Prob(X_{1}=j\mid X_0=i)$$
    The matrix $\vf{P}=(p_{ij})_{i,j\in I}$ is called the \emph{transition matrix} of the Markov chain. Finally, we define the probabilities $\pi_i$ as $\pi_i=\Prob(X_0=i)$. We define the vector $\vf{\pi}=(\pi_i)_{i\in I}$ as the \emph{initial distribution} of the Markov chain.
  \end{definition}
  \begin{proposition}
    Let $(X_n)$ be a time-homogeneous Markov chain. Then:
    \begin{enumerate}
      \item $\vf{P}$ is a stochastic matrix.
      \item $\sum_{i\in I}\pi_i=1$.
    \end{enumerate}
  \end{proposition}
  \begin{definition}[Random walk]
    A \emph{random walk} is a sequence $(\vf{S}_n)$ with $\vf{S}_0=\vf{X}_0$ and $\vf{S}_n=\sum_{k=0}^{n}\vf{X}_n$, where $(\vf{X}_n)_{n\geq 1}$ is a sequence of \iid random vectors and $\vf{X}_0$ is a random vector independent from $(\vf{X}_n)$.
  \end{definition}
  \begin{definition}
    A \emph{simple random walk} is a random walk in which $X_k$ are random variables such that:
    $$
      X_k=\begin{cases}
        1  & \text{with probability $p$}   \\
        -1 & \text{with probability $1-p$}
      \end{cases}
    $$
  \end{definition}
  \begin{lemma}
    A sequence of \iid random variables, a random walk and a Galton-Watson process are all time-homogeneous Markov chains.
  \end{lemma}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We define the probabilities $p_{ij}^{(n)}$ as the probability of going from state $i$ to state $j$ in $n$ steps. That is: $$p_{ij}^{(n)}=\Prob(X_{n}=j\mid X_0=i)$$ The matrix $\vf{P}^{(n)}=(p_{ij}^{(n)})_{i,j\in I}$ is called the \emph{$n$-step transition matrix} of the Markov chain.
  \end{definition}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We define the probabilities $\pi_i^{(n)}$ as the probability of being in state $i$ after $n$ steps. That is: $$\pi_i^{(n)}=\Prob(X_n=i)$$
    We define the vector $\vf{\pi}^{(n)}=(\pi_i^{(n)})_{i\in I}$ as the $n$-step stationary distribution of the Markov chain.
  \end{definition}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We define the \emph{stationary distribution} of the Markov chain as the distribution of the random variable $X_0$.
  \end{definition}
  \begin{lemma}\label{SP:lema1Markov}
    Let $A$, $B$, $C$ be events in a probability space such that $\Prob(B\cap C)>0$. Then:
    $$\Prob(A\cap B\mid C)=\Prob(B\mid C)\Prob(A\mid B\cap C)$$
  \end{lemma}
  \begin{proof}
    \begin{align*}
      \Prob(A\cap B\mid C) & =\frac{\Prob(A\cap B\cap C)}{\Prob(C)}\frac{\Prob(B\cap C)}{\Prob(B\cap C)} \\
                           & =\Prob(B\mid C)\Prob(A\mid B\cap C)
    \end{align*}
  \end{proof}
  \begin{lemma}\label{SP:lema2Markov}
    Let $I$ be a finite or countable set and $A$ and $D_i$ for $i\in I$ be events in a probability space such that $\Prob(A\mid D_i)=p$ for all $i\in I$ and such that the $D_i$ are pairwise disjoint. Then:
    $$\Prob\left(A\mid \bigsqcup_{i\in I}D_i\right)=p$$
  \end{lemma}
  \begin{proof}
    \begin{align*}
      \Prob\left(A\mid \bigsqcup_{i\in I}D_i\right) & =\frac{\Prob(A\cap \bigsqcup_{i\in I}D_i)}{\Prob(\bigsqcup_{i\in I}D_i)} \\
                                                    & =\frac{\sum_{i\in I}\Prob(A\cap D_i)}{\sum_{i\in I}\Prob(D_i)}           \\
                                                    & =\frac{\sum_{i\in I}\Prob(A\mid D_i)\Prob(D_i)}{\sum_{i\in I}\Prob(D_i)} \\
                                                    & =\frac{\sum_{i\in I}p\Prob(D_i)}{\sum_{i\in I}\Prob(D_i)}                \\
                                                    & =p
    \end{align*}
  \end{proof}
  \begin{theorem}
    Let $(X_n)$ be a time-homogeneous Markov chain. Then, $\vf{P}^{(n)}=\vf{P}^n$.
  \end{theorem}
  \begin{proof}
    By induction on $n$. The case $n=1$ is clear. For $n\geq 2$ we have:
    \begin{align*}
      p_{ij}^{(n+1)} & =\Prob(X_{n+1}=j\mid X_0=i)                     \\
                     & =\sum_{k\in I}\Prob(X_{n+1}=j, X_n=k\mid X_0=i) \\
      \begin{split}
        &=\sum_{k\in I}\Prob(X_{n}=k\mid X_0=i)\cdot\\
        &\hspace{2.5cm}\cdot\Prob(X_{n+1}=j\mid X_n=k, X_0=i)
      \end{split}             \\
                     & =\sum_{k\in I}p_{ij}^{(n)}p_{kj}^{(1)}
    \end{align*}
    where the penultimate equality follows from \mcref{SP:lema1Markov} and the last equality follows from \mcref{SP:lema2Markov} and the Markov property because if $D=\{X_n=k, X_0=i\}$ we have that:
    \begin{multline*}
      D=\\\!=\bigsqcup_{i_1,\ldots,i_{n-1}\in I}\{X_n=k, X_0=i,X_1=i_1,\ldots,X_{n-1}=i_{n-1}\}
    \end{multline*}
    and so:
    \begin{multline*}
      \Prob(X_{n+1}=j\mid X_n=k, X_0=i)=\Prob(X_{n+1}=j\mid X_n=k,\\ X_0=i,X_1=i_1,\ldots,X_{n-1}=i_{n-1})=\\=\Prob(X_{n+1}=j\mid X_n=k)
    \end{multline*}
    Therefore, $\vf{P}^{(n+1)}=\vf{P}^n\vf{P}$, by induction hypothesis.
  \end{proof}
  \begin{proposition}
    Let $(X_n)$ be a time-homogeneous Markov chain. Then:
    \begin{enumerate}
      \item $\vf{P}^{(0)}=\vf{I}_I$
      \item $\vf{\pi}^{(n)}=\vf{\pi}^{(0)}\vf{P}^n$
      \item $\Prob(X_0=i_0,\ldots,X_n=i_n)=\pi_{i_0}^{(0)}p_{i_0i_1}\cdots p_{i_{n-1}i_n}$
    \end{enumerate}
  \end{proposition}
  \begin{sproof}
    \begin{enumerate}
      \item $p_{ij}^{(0)}=\Prob(X_1=j\mid X_0=i)=\delta_{ij}$.
      \item $\pi_i^{(n)}=\Prob(X_n=i)=\sum_{k\in I}\Prob(X_n=i\mid X_0=k)=\sum_{k\in I}\pi_k^{(0)}p_{ki}^{(n)}$
      \item Use the \mnameref{P:totalprob} and the Markov property.
    \end{enumerate}
  \end{sproof}
  \subsubsection{Classification of states}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We say that a state $j\in I$ is \emph{reachable} form $i\in I$ if $\exists n\in I$ such that $p_{ij}^{(n)}>0$. In this case we will write $i\to j$.
  \end{definition}
  \begin{definition}
    Let $(X_n)$ be a time-homogeneous Markov chain. We say that two states $i,j\in I$ \emph{communicate} if $i\to j$ and $j\to i$. In this case we will write $i\leftrightarrow j$.
  \end{definition}
  \begin{lemma}
    Let $(X_n)$ be a time-homogeneous Markov chain. Then, the relation $\leftrightarrow$ is an equivalence relation.
  \end{lemma}
  \begin{proof}
    The reflexivity and symmetry are clear. For the transitivity, suppose $i\leftrightarrow j$ and $j\leftrightarrow k$. Then, $\exists n,m\in I$ such that $p_{ij}^{(n)}>0$ and $p_{jk}^{(m)}>0$. Then:
    $$p_{ik}^{(n+m)}=\sum_{\ell\in I} p_{i\ell}^{(n)}p_{\ell k}^{(m)}\geq p_{ij}^{(n)}p_{jk}^{(m)}>0>0$$
  \end{proof}
\end{multicols}
\end{document}