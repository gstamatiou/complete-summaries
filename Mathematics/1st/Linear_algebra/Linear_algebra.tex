\documentclass[../../../main.tex]{subfiles}

\begin{document}
\begin{multicols}{2}[\section{Linear algebra}]
\subsection{Matrices}
\begin{definition}
    A \textit{linear equation} is an equation of the form $$a_1x_1+\cdots+a_nx_n=b,$$ where $x_1,\ldots,x_n$ are the \textit{variables} or \textit{unknowns} and $a_i,b\in\RR$ are the coefficients of the equation. The term $b$ is usually called \textit{constant term}.
\end{definition}
\begin{definition}
    A \textit{system of linear equations} is a collection of one or more linear equations involving the same set of variables.
\end{definition}
\begin{definition}
    Let
    \begin{equation*}
        \arraycolsep=1pt
        \left\{
        \begin{array}{ccccc}
            a_{11}x_1 & + \cdots + & a_{1n}x_n & = & b_1\\
            \vdots & \vdots & \vdots & & \vdots\\
            a_{m1}x_1 & + \cdots + & a_{mn}x_n & = & b_m
        \end{array}
        \right.
    \end{equation*}
    be a system of linear equations. A \textit{solution of a system of equations} is a set of numbers $c_1,\ldots,c_n$ such that $a_{i1}c_1+\cdots+a_{in}c_n=b_i$ for $i=1,\ldots,m$. A linear system may behave in any one of three possible ways:
    \begin{enumerate}
        \item The system has a single unique solution. 
        \item The system has infinitely many solutions.
        \item The system has no solution.
    \end{enumerate}
\end{definition}
\begin{definition}
    Two systems of equations are \textit{equivalent} if they have the same solutions.
\end{definition}
\begin{definition}[Matrix]
    A \textit{matrix $\mathblack{A}$ with coefficients in $\RR$} is a table of real numbers arranged in rows and columns. That is, $\mathblack{A}$ is of the form: 
    \begin{equation*}
        \mathblack{A}=(a_{ij})=
        \begin{pmatrix}
            a_{11} & \cdots & a_{1n} \\
            \vdots & \ddots & \vdots \\
            a_{m1} & \cdots & a_{mn} 
        \end{pmatrix},
    \end{equation*}
    for some values $a_{ij}\in\RR$, $i=1,\ldots,m$ and $j=1,\ldots,n$. The set of $m\times n$ matrices with real coefficients is denoted by $\mathcal{M}_{m\times n}(\RR)$\footnote{In the case when $m=n$ we will denote $\mathcal{M}_{n\times n}(\RR)$ by $\mathcal{M}_n(\RR)$.}.
\end{definition}
\begin{definition}
    Let $\mathblack{A},\mathblack{B}\in\mathcal{M}_{m\times n}(\RR)$ and $\alpha\in\RR$. If $\mathblack{A}=(a_{ij})$ and $\mathblack{B}=(b_{ij})$, we define the \textit{sum $\mathblack{A}+\mathblack{B}$} as: $$\mathblack{A}+\mathblack{B}=(a_{ij}+b_{ij}).$$
    We define the \textit{product $\alpha\mathblack{A}$} as: $$\alpha\mathblack{A}=(\alpha a_{ij}).$$
\end{definition}
\begin{prop}[Properties of addition and scalar multiplication of matrices]
    The following properties are satisfied:
    \begin{enumerate}
        \item Commutativity: $$\mathblack{A}+\mathblack{B}=\mathblack{B}+\mathblack{A},$$ for all $\mathblack{A},\mathblack{B}\in\mathcal{M}_{m\times n}(\RR)$.
        \item Associativity: $$(\mathblack{A}+\mathblack{B})+\mathblack{C}=\mathblack{A}+(\mathblack{B}+\mathblack{C}),$$ for all $\mathblack{A},\mathblack{B},\mathblack{C}\in\mathcal{M}_{m\times n}(\RR)$.
        \item Additive identity element: $\exists\mathblack{0}\in\mathcal{M}_{m\times n}(\RR)$ such that $$\mathblack{A}+\mathblack{0}=\mathblack{A},$$ for all $\mathblack{A}\in\mathcal{M}_{m\times n}(\RR)$.
        \item Additive inverse element: $\forall\mathblack{A}\in\mathcal{M}_{m\times n}(\RR)$ $\exists(-\mathblack{A})\in\mathcal{M}_{m\times n}(\RR)$ such that $$\mathblack{A}+(-\mathblack{A})=\mathblack{0}.$$
        \item Distributivity: $$(\alpha+\beta)\mathblack{A}=\alpha\mathblack{A}+\beta\mathblack{A},$$ for all $\mathblack{A}\in\mathcal{M}_{m\times n}(\RR)$ and all $\alpha,\beta\in\RR$.
    \end{enumerate}
\end{prop}
\begin{definition}
    Let $\mathblack{A}\in\mathcal{M}_{m\times n}(\RR)$ and $\mathblack{B}\in\mathcal{M}_{n\times p}(\RR)$. We define the \textit{product $\mathblack{A}\mathblack{B}$} as $$\mathblack{A}\mathblack{B}=(c_{ij}),\quad\text{where }c_{ij}=\sum_{k=1}^na_{ik}b_{kj}.$$
\end{definition}
\begin{prop}[Properties of matrix product]
    The following properties are satisfied:
    \begin{enumerate}
        \item Associativity: $$(\mathblack{A}\mathblack{B})\mathblack{C}=\mathblack{A}(\mathblack{B}\mathblack{C}),$$ for all $\mathblack{A}\in\mathcal{M}_{m\times n}(\RR)$, $\mathblack{B}\in\mathcal{M}_{n\times p}(\RR)$ and $\mathblack{C}\in\mathcal{M}_{p\times q}(\RR)$.
        \item Multiplicative identity element: $\exists\mathblack{I}_n\in\mathcal{M}_n(\RR)$ such that 
        \begin{align*}
            &\mathblack{A}\mathblack{I}_n=\mathblack{A}\quad\forall\mathblack{A}\in\mathcal{M}_{m\times n}(\RR)\text{ and }\\
            &\mathblack{I}_n\mathblack{A}=\mathblack{A}\quad\forall\mathblack{A}\in\mathcal{M}_{n\times p}(\RR).
        \end{align*}
        \item Distributivity: $$(\mathblack{A}+\mathblack{B})\mathblack{C}=\mathblack{A}\mathblack{C}+\mathblack{B}\mathblack{C},$$ for all $\mathblack{A},\mathblack{B}\in\mathcal{M}_{m\times n}(\RR)$ and $\mathblack{C}\in\mathcal{M}_{n\times p}(\RR)$.
    \end{enumerate}
\end{prop}
\begin{definition}
    We say a matrix $\mathblack{A}\in\mathcal{M}_n(\mathbb{R})$ is \textit{invertible} if there is a matrix $\mathblack{B}\in\mathcal{M}_n(\mathbb{R})$ satisfying $$\mathblack{A}\mathblack{B}=\mathblack{B}\mathblack{A}=\mathblack{I}_n.$$
\end{definition}
\begin{lemma}
    The product of invertible matrices is invertible.
\end{lemma}
\begin{definition}
    Let $\mathblack{A}\in\mathcal{M}_{m\times n}(\mathbb{R})$ be a matrix. The \textit{$i$-th pivot of $\mathblack{A}$} is the first nonzero element in the $i$-th row of $\mathblack{A}$.
\end{definition}
\begin{definition}[Row echelon form]
    A matrix $\mathblack{A}\in\mathcal{M}_{m\times n}(\mathbb{R})$ is in \textit{row echelon form} if:
    \begin{itemize}
        \item All rows consisting of only zeroes are at the bottom.
        \item The pivot of a nonzero row is always strictly to the right of the pivot of the row above it.
    \end{itemize}
\end{definition}
\begin{definition}[Reduced row echelon form]
    A matrix $\mathblack{A}\in\mathcal{M}_{m\times n}(\mathbb{R})$ is in \textit{reduced row echelon form} if:
    \begin{itemize}
        \item It is in row echelon form.
        \item Pivots are equal to 1.
        \item Each column containing a pivot has zeros in all its other entries.
    \end{itemize}
\end{definition}
\begin{theorem}[Gau\ss' theorem]
    Let $\mathblack{A}\in\mathcal{M}_{m\times n}(\mathbb{R})$ be a matrix. Then, there is an invertible matrix $\mathblack{P}\in\mathcal{M}_m(\mathbb{R})$ such that $\mathblack{P}\mathblack{A}=\mathblack{A'}$ is in reduced row echelon form. Moreover, $\mathblack{A'}$ is uniquely determined by $\mathblack{A}$.
\end{theorem}
\begin{theorem}[PAQ reduction theorem]
    Let $\mathblack{A}\in\mathcal{M}_{m\times n}(\mathbb{R})$ be a matrix. Then, there exist invertible matrices $\mathblack{P}\in\mathcal{M}_m(\mathbb{R})$ and $\mathblack{Q}\in\mathcal{M}_n(\mathbb{R})$ such that 
    $$\mathblack{P}\mathblack{A}\mathblack{Q}=\left(
    \begin{array}{@{\,} c|c @{\,}}
        \mathblack{I}_r & \mathblack{0}\\
        \hline
        \mathblack{0} & \mathblack{0}
    \end{array}
    \right).$$
    The number $r$ is uniquely determined by $\mathblack{A}$.
\end{theorem}
\begin{definition}[Rank]
    Let $\mathblack{A}\in\mathcal{M}_{m\times n}(\mathbb{R})$ be a matrix and suppose 
    $$\mathblack{P}\mathblack{A}\mathblack{Q}=\left(
    \begin{array}{@{\,} c|c @{\,}}
        \mathblack{I}_r & \mathblack{0}\\
        \hline
        \mathblack{0} & \mathblack{0}
    \end{array}
    \right)$$ for some matrices $\mathblack{P}\in\mathcal{M}_m(\mathbb{R})$ and $\mathblack{Q}\in\mathcal{M}_n(\mathbb{R})$. We define the \textit{rank of $\mathblack{A}$}, denoted by $\rank \mathblack{A}$, as the number ones in the matrix $\mathblack{P}\mathblack{A}\mathblack{Q}$, that is, $\rank\mathblack{A}:=r$.
\end{definition}
\begin{prop}
    Let $\mathblack{A},\mathblack{A}'\in\mathcal{M}_{m\times n}(\mathbb{R})$ and $\mathblack{B},\mathblack{B}'\in\mathcal{M}_{1\times n}(\mathbb{R})$ be matrices and $\mathblack{P}\in\mathcal{M}_m(\mathbb{R})$ be an invertible matrix. Suppose we have a system of linear equations $\mathblack{A}x=\mathblack{B}$. If $\mathblack{P}(\mathblack{A}\mid\mathblack{B})=(\mathblack{A}'\mid\mathblack{B}')$\footnote{Here $(\mathblack{A}\mid\mathblack{B})$ denotes the augmented matrix obtained by appending the columns of $\mathblack{B}$ to the columns of $\mathblack{A}$.}, then the systems $\mathblack{A}x=\mathblack{B}$ and $\mathblack{A}'x=\mathblack{B}'$ are equivalent.
\end{prop}
\begin{corollary}
    Reduced row echelon form of an invertible matrix is the identity matrix.
\end{corollary}
\begin{definition}[Transposition]
    Let $\mathblack{A}\in\mathcal{M}_{m\times n}(\mathbb{R})$ be a matrix. If $\mathblack{A}=(a_{ij})$, we define the \textit{transpose $\transpose{A}$ of $\mathblack{A}$} as the matrix $\transpose{A}=(b_{ij})$, where $b_{ij}=a_{ji}$ for $i=1,\ldots,m$ and $j=1,\ldots,n$.
\end{definition}
\begin{prop}
    Let $\mathblack{A}\in\mathcal{M}_{m\times n}(\mathbb{R})$ be a matrix. Then, $\rank \mathblack{A}=\rank\transpose{A}$.
\end{prop}
\begin{theorem}[Rouché–Frobenius theorem]
    Let $\mathblack{A}x=\mathblack{B}$ be a system of equations with $n$ variables. The system is:
    \begin{itemize}
        \item \textit{determined and consistent} if and only if $\rank \mathblack{A}=\rank (\mathblack{A}\mid \mathblack{B})=n$.
        \item \textit{indeterminate with $s$ free variables} if and only if $\rank \mathblack{A}=\rank (\mathblack{A}\mid \mathblack{B})=n-s$.
        \item \textit{inconsistent} if and only if $\rank \mathblack{A}\ne\rank (\mathblack{A}\mid \mathblack{B})$.
    \end{itemize}
\end{theorem}
\begin{definition}[Determinant]
    A determinant is a function $\det:\mathcal{M}_n(\mathbb{R})\rightarrow\mathbb{R}$ satisfying the following properties:
    \begin{enumerate}
        \item If $\mathblack{A}=(\mathblack{a}_1\mid\cdots\mid \mathblack{a}_n)$, where $\mathblack{a}_i$ are column vectors in $\RR^n$ for $i=1,\ldots,n$ and $\mathblack{a}_j=\lambda\mathblack{u}+\mu\mathblack{v}$ for some other column vectors $\mathblack{u}$ and $\mathblack{v}$, then:
        \begin{multline*}
            \det \mathblack{A}=\det(\mathblack{a}_1\mid\cdots\mid\mathblack{a}_j\mid\cdots\mid \mathblack{a}_n)=\\=\det(\mathblack{a}_1\mid\cdots\mid \mathblack{a}_{j-1}\mid\lambda \mathblack{u}+\mu \mathblack{v}\mid \mathblack{a}_{j+1}\mid\cdots\mid \mathblack{a}_n)=\\=\lambda\det(\mathblack{a}_1\mid\cdots\mid \mathblack{a}_{j-1}\mid \mathblack{u}\mid \mathblack{a}_{j+1}\mid\cdots\mid \mathblack{a}_n)+\\+\mu\det(\mathblack{a}_1\mid\cdots\mid \mathblack{a}_{j-1}\mid \mathblack{v}\mid \mathblack{a}_{j+1}\mid\cdots\mid \mathblack{a}_n).
        \end{multline*}
        \item The determinant changes its sign whenever two columns are swapped.
        \item $\det \mathblack{I}_n=1$ for all $n\in\NN$.
    \end{enumerate}
\end{definition}
\begin{lemma}
    Whenever two columns of a matrix are identical, the determinant is 0.
\end{lemma}
\begin{prop}
    Let $\mathblack{A}\in\mathcal{M}_n(\mathbb{R})$ be a matrix in its row echelon form. If $\mathblack{A}=(a_{ij})$, then $$\det\mathblack{A}=\prod_{i=1}^na_{ii}.$$ 
\end{prop}
\begin{prop}
    Let $\mathblack{A}\in\mathcal{M}_n(\mathbb{R})$ be a matrix. The following are equivalent:
    \begin{enumerate}
        \item $\mathblack{A}$ is not invertible.
        \item $\rank\mathblack{A}<n$.
        \item $\det\mathblack{A}=0$.
    \end{enumerate}
\end{prop}
\begin{theorem}
    Let $\det:\mathcal{M}_n(\mathbb{R})\rightarrow\mathbb{R}$ be a determinant. Then, for all matrices $\mathblack{A},\mathblack{B}\in\mathcal{M}_n(\mathbb{R})$, $$\det (\mathblack{A}\mathblack{B})=\det\mathblack{A}\det\mathblack{B}.$$
\end{theorem}
\begin{corollary}
    Let $\det,\det':\mathcal{M}_n(\mathbb{R})\rightarrow\mathbb{R}$ be two determinants. Then, for all matrix $\mathblack{A}\in\mathcal{M}_n(\mathbb{R})$, $\det\mathblack{A}=\det' \mathblack{A}$.
\end{corollary}
\begin{prop}
    Let $\mathblack{A}\in\mathcal{M}_n(\mathbb{R})$. Then, $$\det \mathblack{A}=\sum_{\sigma\in S_n}\varepsilon(\sigma)\prod_{i=1}^na_{i\sigma(i)}.$$
\end{prop}
\begin{prop}
    For all matrix $\mathblack{A}\in\mathcal{M}_n(\mathbb{R})$, $\det\mathblack{A}=\det\transpose{A}$.
\end{prop}
\begin{prop}
    Let $\mathblack{A}=(a_{ij})\in\mathcal{M}_n(\mathbb{R})$. We denote by $\mathblack{A}_{ij}$ the square matrix obtained from $\mathblack{A}$ by removing the $i$-th row and $j$-th column. Then, for every $i\in\{1,\ldots,n\}$, $$\det\mathblack{A}=\sum_{j=1}^n(-1)^{i+j}a_{ij}\det\mathblack{A}_{ij}.$$
\end{prop}
\begin{definition}
    Let $\mathblack{A}=(a_{ij})\in\mathcal{M}_n(\mathbb{R})$. We define the \textit{cofactor matrix $\mathblack{C}$ of $\mathblack{A}$} as: $$\mathblack{C}=(b_{ij}),\quad\text{where }b_{ij}=(-1)^{i+j}\det\mathblack{A}_{ij}\footnote{$\mathblack{C}$ is usually denoted as $\cofactor\mathblack{C}$.}.$$ We define the \textit{adjugate matrix $\adjugate\mathblack{A}$ of $\mathblack{A}$} as: $$\adjugate\mathblack{A}=\transpose{\mathblack{C}}.$$
\end{definition}
\begin{theorem}
    Let $\mathblack{A}\in\mathcal{M}_n(\mathbb{R})$. Then, $\adjugate\mathblack{A}=(\det \mathblack{A})I_n$. Moreover if $\det \mathblack{A}\ne 0$, then: $$\mathblack{A}^{-1}=\frac{1}{\det \mathblack{A}}\adjugate\mathblack{A}$$
\end{theorem}
\subsection{Espais vectorials}
\begin{definition}
Sigui $E$ a $K$-espai vectorial i $F$ a subconjunt de $E$, llavors $(F,+_E,\cdot_E)$ is a $K$-espai vectorial si es verifica $\lambda v_1+\mu v_2\in F$ $\forall v_1,v_2\in F$ i $\forall\lambda,\mu\in K$.
\end{definition}
\begin{lemma}
La intersecció de subespais vectorials is a subespai vectorial.
\end{lemma}
\begin{definition}
Donat a $K$-espai vectorial $E$, a base de $E$ is a conjunt ordenat $B$ de vectors de $E$ que és: 
\begin{enumerate}
    \item Sistema de generadors de $E$.
    \item Linealment independent.
\end{enumerate}
\end{definition}
\begin{theorem}[\bfseries Teorema de Steinitz]
Donat a $K$-espai vectorial $E$, $B$ a base de $E$ i $(v_1,\ldots,v_k)$ vectors linealment independents de $E$, aleshores podem substituir $k$ vectors apropiats de $B$ per $(v_1,\ldots,v_k)$ i definir a nova base.
\end{theorem}
\begin{definition}
La suma de dos subespais $F,G$ dins d'un $K$-espai vectorial $E$ és: $F+G=\langle F\cup G\rangle=\{u+v\mid u\in F,v\in G\}$.
\end{definition}
\begin{prop}[\bfseries Fórmula de Gra\ss mann]
$\dim (F+G)+\dim(F\cap G)=\dim F+\dim G$.
\end{prop}
\begin{definition}
Sigui $E$ a $K$-espai vectorial i siguin $F,G\subset E$ dos subespais vectorials. Llavors la suma $F+G$ is directe ($F\oplus G$) $\iff F\cap G=\{0\}$.
\end{definition}
\begin{definition}
Donada a matrix $A\in\mathcal{M}_n(\mathbb{R})$, a menor d'ordre $r$ de $A$ is a submatrix $A'\in\mathcal{M}_r(\mathbb{R})$ obtinguda seleccionant $r$ files i $r$ columnes de $A$.
\end{definition}
\begin{definition}
Sigui $E$ a $K$-espai vectorial i $F\subset E$ a subespai vectorial. Anomenarem subespai complementari de $F$ a tot subespai $G\subset E$ que compleixi $F\oplus G=E$.
\end{definition}
\begin{definition}
Direm que dos vectors $u,v\in E$ són equivalents mòdul $F$ (on $F\subset E$ a subespai vectorial) si $u-v\in F$ i escriurem $u\sim_Fv$. $\sim_F$ is a relació d'equivalència.
\end{definition}
\begin{definition}
L'espai quocient $E/F$ is el conjunt de les classes d'equivalència $u+F=[u]$ amb les operacions:
$$[u]+[v]=[u+v]\qquad a[u]=[au]$$ $E/F$ is a $K$-espai vectorial.
\end{definition}
\begin{prop}
Sigui $E$ a $K$-espai vectorial de dimensió $n<\infty$ i $F\subset E$ a subespai vectorial, llavors $\dim (E/F)=\dim E-\dim F$.
\end{prop}
\subsection{Aplicacions lineals}
\begin{definition}
Sigui $E,F$ dos $K$-espais vectorials. Una function $f:E\rightarrow F$ is lineal si es compleix $f(\lambda v_1+\mu v_2)=\lambda f(v_1)+\mu f(v_2)$ $\forall v_1,v_2\in E$ i $\forall\lambda,\mu\in K$.
\end{definition}
\begin{prop}
Si $f:E\rightarrow F$ i $g:F\rightarrow G$ són aplicacions lineals, llavors la composició $g\circ f:E\rightarrow G$ is lineal.
\end{prop}
\begin{prop}
Si $f:E\rightarrow F$ is a function lineal, llavors $f^{-1}:F\rightarrow E$ is lineal.
\end{prop}
\begin{prop}
Sigui $f:E\rightarrow F$ a function lineal entre $K$-espais vectorials i siguin $G\subset E$ i $H\subset F$ subespais vectorials. Aleshores:
\begin{enumerate}
    \item $f(G)=\{f(u)\mid u\in G\}\subset F$ is a subespai vectorial.
    \item $f^{-1}(H)=\{u\in E\mid f(u)\in H\}\subset E$ is a subespai vectorial.
\end{enumerate}
Si $G=E$ i $H=\{0\}$, aleshores:
\begin{enumerate}
    \item $f(E)$ is la imatge de $f$ i es denota $\text{Im }f$.
    \item $f^{-1}(0)$ is el nucli de $f$ i es denota $\text{Ker }f$.
\end{enumerate}
\end{prop}
\begin{prop}
Si $E,F$ són $K$-espais vectorials de dimensió finita i $f:E\rightarrow F$, aleshores $f$ is injectiva $\iff \dim(\text{Ker }f)=0$ i $f$ is exhaustiva $\iff \dim(\text{Im }f)=\dim F$.
\end{prop}
\begin{definition}
\hfill
\begin{enumerate}
    \item A monomorfisme is a function lineal injectiva.
    \item A epimorfisme is a function lineal exhaustiva.
    \item A isomorfisme is a function lineal bijectiva.
    \item A endomorfisme is a function lineal d'un espai vectorial en ell mateix.
    \item A automorfisme is a endomorfisme bijectiu.
\end{enumerate}
\end{definition}
\begin{lemma}
Donada $f:E\rightarrow F$ a function lineal, on $E,F$ són $K$-espais vectorials, i $u_1,\ldots,u_k\in E$ es compleix $\langle f(u_1),\ldots,f(u_k)\rangle=f(\langle u_1,\ldots,u_k\rangle)$.
\end{lemma}
\begin{theorem}[\bfseries Isomorfisme de coordenació]
Sigui $E$ a $K$-espai vectorial  de dimensió $n$ i $B=(u_1,\ldots,u_n)$ a base de $E$. Llavors l'function $f:K^n\rightarrow E$, $f(a_1,\ldots,a_n)=a_1u_1+\cdots a_nu_n$ is a isomorfisme.
\end{theorem}
\begin{theorem}[\bfseries Teorema de l'isomorfisme]
Sigui $f:E\rightarrow F$ a function lineal. Existeix a isomorfisme $\Tilde{f}:E/\text{Ker }f\rightarrow \text{Im }f$ complint $f=i\circ\Tilde{f}\circ\pi$ on $\pi:E\rightarrow E/\text{Ker }f$ i $i:\text{Im }f\rightarrow F$.
\end{theorem}
\begin{corollary}
Sigui $f:E\rightarrow F$ a function lineal i suposem que $\dim E=n<\infty$. Llavors $n=\dim(\text{Ker }f)+\dim(\text{Im }f)$.
\end{corollary}
\begin{corollary}
Siguin $E,F$ dos $K$-espais vectorials de dimensió $n<\infty$ i $f:E\rightarrow F$ a function lineal. Llavors $f$ is injectiva $\iff f$ is exhaustiva $\iff f$ is bijectiva.
\end{corollary}
\begin{theorem}[\bfseries Teoremes d'Emmy Noether]
Sigui $E$ a $K$-espai vectorial, $F,G\subset E$ dos subespais:
\begin{enumerate}
    \item Existeix a isomorfisme $F/(F\cap G)\cong(F+G)/G$.
    \item Si $G\subset F\subset E$, existeix a isomorfisme $(E/G)/(F/G)\cong E/F$.
\end{enumerate}
\end{theorem}
\begin{theorem}
Siguin $E,F$ dos $K$-espais vectorials i siguin $B=(u_1,\ldots,u_n)$ a base de $E$ i $v_1,\ldots,v_n\in F$ vectors qualssevol. Llavors existeix a única function lineal $f:E\rightarrow F$ such that $f(u_i)=v_i$, $i=1,\ldots,n$. 
\end{theorem}
\begin{prop}
Siguin $f:E\rightarrow F$ i $g:F\rightarrow G$ dues aplicacions lineals entre $K$-espais vectorials i siguin $B,B',B''$ bases de $E,F$ i $G$ respectivament. Llavors $g\circ f:E\rightarrow G$ té matrix $[g\circ f]_{B,B''}=[g]_{B',B''}[f]_{B,B'}$ en les bases $B, B''$.
\end{prop}
\begin{corollary}
Les matrices de canvi de base són invertibles i $[id]_{B,B'}^{-1}=[id]_{B',B}$.
\end{corollary}
\begin{prop}
Donada $f:E\rightarrow F$ a function lineal i bases $B_1,B_2$ de $E$ i $B_1',B_2'$ de $F$ es compleix: $[f]_{B_2,B_2'}=[id]_{B_1',B_2'}[f]_{B_1,B_1'}[id]_{B_2,B_1}$.
\end{prop}
\begin{theorem}
Donada qualsevol function lineal $f:E\rightarrow F$ on $\dim E=n, \dim F=m$. Existeixen bases $B_0$ de $E$ i $B_0'$ de $F$ en les quals $[f]_{B_0,B_0'}=\left(\begin{array}{@{\,} c|c @{\,}}
    I_r & 0\\
    \hline
    0 & 0
    \end{array}\right)$ on $r=\dim\text{Im }f$ i $[f]_{B_0,B_0'}=[id]_{B',B_0'}[f]_{B,B'}[id]_{B_0,B}$ $\forall B,B'$ base de $E$ i $F$ respectivament.
\end{theorem}
\begin{lemma}
Donats dos $K$ espais vectorials $E,F$ el conjunt $\mathcal{L}(E,F)=\{f\mid f\text{ is a function}\\ \text{lineal de $E$ a $F$}\}$ is a $K$-espai vectorial i es compleix que $(\lambda f+\mu g)(v)=\lambda f(v)+\mu g(v)$ $\forall f,g\in\mathcal{L}(E,F)$ i $\lambda,\mu\in K$.
\end{lemma}
\begin{prop}
Siguin $E,F$ dos $K$-espais vectorials amb $\dim E=n<\infty$, $\dim F=m<\infty$. Per a tota base $B$ de $E$ i $B'$ de $F$, l'function $\varphi:\mathcal{L}(E,F)\rightarrow\mathcal{M}_{m\times n}(K)$ definida per $\varphi(f)=[f]_{B,B'}$ is a isomorfisme.
\end{prop}
\begin{corollary}
Si $\dim E=n$ i $\dim F=m$ aleshores $\dim \mathcal{L}(E,F)=mn$.
\end{corollary}
\begin{definition}
L'espai dual d'un $K$-espai vectorial is $\mathcal{L}(E,K)=E^*$. En el cas $\dim E=n<\infty$, escollir a base $B$ determina a isomorfisme $\varphi:E^*\rightarrow\mathcal{M}_{1\times n}(K)$, definit per $\varphi(\omega)=[\omega]_{B,B'}$. Deduïm, doncs, que $\dim E^*=\dim E$.
\end{definition}
\begin{definition}
Donats $E$ a $K$-espai vectorial de dimensió finita i $B=(u_1,\ldots,u_n)$ a base de $E$. La base dual de $B$ is la base de $E^*$ formada per $(\eta_1,\ldots,\eta_n)$ on $\eta_i(u_j)=\delta_{ij}$.
\end{definition}
\begin{lemma}
Sigui $E$ a $K$-espai vectorial de dimensió $n<\infty$ i sigui $B=(u_1,\ldots,u_n)$ a base de $E$. $\forall u\in E$, $(u_1^*(u),\ldots,u_n^*(u))=[u]_B\in K^n$ on $(u_1^*,\ldots,u_n^*)$ is la base dual de $B$.
\end{lemma}
\begin{lemma}
Sigui $E$ a $K$-espai vectorial de dimensió $n<\infty$ i sigui $B=(u_1,\ldots,u_n)$ a base de $E$. $\forall \omega\in E^*$, $(\omega(u_1),\ldots,\omega(u))=[\omega]_{B^*}$.
\end{lemma}
\begin{definition}
Si $f\in \mathcal{L}(E,F)$ l'function $f^*:F^*\rightarrow E^*$ definida per $f^*(\omega)=\omega\circ f$ s'anomena function dual de $f$. Aquesta function is lineal.
\end{definition}
\begin{theorem}
Siguin $E,F$ dos $K$-espais vectorials de dimensió finita i siguin $B,B'$ bases de $E,F$ respectivament. Llavors $[f^*]_{B'^*,B^*}=([f]_{B,B'})^t$.
\end{theorem}
\begin{definition}
Donat a $K$-espai vectorial $E$, el bidual is el $K$-espai vectorial definit per $(E^*)^*=\mathcal{L}(E^*,K)$. A més, si $\dim E=n<\infty$, l'function $f:E\rightarrow (E^*)^*$ definida per $f(v)=\phi_v$ $(\phi_v:E^*\rightarrow K$, $\phi_v(\omega)=\omega(v))$ is a isomorfisme natural. 
\end{definition}
\begin{definition}
Sigui $E$ a $K$-espai vectorial i $F$ a subespai vectorial de $E^*$. El subespai de $E$ incident de $F$ is $F^{inc}=\{v\in E\mid \omega(v)=0\;\forall\omega\in F\}$.
\end{definition}
\begin{theorem}
Sigui $E$ a $K$-espai vectorial de dimensió $n<\infty$ i $F\subset E^*$ a subespai amb $\dim F=m$. Llavors $\dim F^{inc}=n-m$.
\end{theorem}
\begin{definition}
Donat a subespai vectorial i $F\subset E$, el seu subespai incident is $F^{inc}=\{\omega\in E^*\mid \omega(v)=0\;\forall v\in F\}$.
\end{definition}
\begin{prop}
$(F^{inc})^{inc}=F$ tant si $F\subset E$ com si $F\subset E^*$.
\end{prop}
\subsection{Classificació d'endomorfismes}
\begin{definition}
Dues matrices $M,N\in\mathcal{M}_n(K)$ s'anomenen similars si existeix $P\in\mathcal{M}_n(K)$ invertible such that $M=P^{-1}NP$.
\end{definition}
\begin{prop}
Donats $f,g\in\mathcal{L}(E)$ on $E$ is a $K$ espai vectorial de dimensió $n<\infty$:
\begin{enumerate}
    \item $f$ i $g$ són similars $\iff\forall B$ base de $E$ les matrices $[f]_B$ i $[g]_B$ són similars.
    \item $f$ i $g$ són similars $\iff\exists h$ automorfisme such that $g=h^{-1}fh$.
\end{enumerate}
\end{prop}
\begin{definition}
Una matrix $A\in\mathcal{M}_n(K)$ is diagonalitzable si is similar a a matrix diagonal. A endomorfisme is diagonalitzable si la seva matrix en alguna base is diagonalitzable.
\end{definition}
\begin{definition}
Donat $f\in\mathcal{L}(E)$ diem que a vector $u\in E$, $u\ne 0$ is vector propi de $f$ de valor propi $\lambda$ si $f(u)=\lambda u$.
\end{definition}
\begin{lemma}
Donats $f\in\mathcal{L}(E)$ i $\lambda\in K$, els vectors propis de $f$ de valor propi $\lambda$ són els vectors no nuls del subespai $Ker(f-\lambda id)$ (subespai propi de valor propi $\lambda$).
\end{lemma}
\begin{definition}
Donada a matrix $A\in\mathcal{M}_n(K)$, el polinomi $p_A(\lambda)=\det(A-\lambda I_n)$ s'anomena polinomi característic de $A$.
\end{definition}
\begin{prop}
Donat $f\in\mathcal{L}(E)$, vectors propis de $f$ de valors propis diferents són linealment independents.
\end{prop}
\begin{prop}
Sigui $E$ a $K$-espai vectorial de dimensió $n<\infty$ i sigui $\lambda$ a arrel del polinomi característic $p_f(x)$ de multiplicitat $m$. Llavors $1\leq \dim(\text{Ker}(f-\lambda id))\leq m$.
\end{prop}
\begin{theorem}[\bfseries Teorema de diagonalització]
Sigui $f\in\mathcal{L}(E)$, $f$ is diagonalitzable si i només si:
\begin{enumerate}
    \item $p_f(x)=(-1)^n(x-\lambda_1)^{m_1}\cdots(x-\lambda_k)^{m_k}$ amb $\lambda_1,\ldots,\lambda_k\in K$ diferents.
    \item $\dim(\text{Ker}(f-\lambda_i id))=m_i$.
\end{enumerate}
\end{theorem}
\begin{corollary}
Si $n=\dim E$ i $f$ té $n$ valors propis diferents, $f$ is diagonalitzable.
\end{corollary}
\begin{definition}
El polinomi mínim de $f$ is a polinomi $P\in K[x]$ such that:
\begin{itemize}
    \item $P(f)=0$.
    \item $P$ is mònic.
    \item $P$ is de grau mínim.
\end{itemize}
\end{definition}
\begin{theorem}[\bfseries Teorema de Cayley-Hamilton]
Sigui $K$ a cos, $n\geq 1$ i $A\in\mathcal{M}_n(K)$. Llavors $m_A(x)\mid p_A(x)\mid m_A(x)^n$. Sigui $K$ a cos i $f\in\mathcal{L}(E)$, $\dim_K E=n$. Llavors $m_f(x)\mid p_f(x)\mid m_f(x)^n$. 
\end{theorem}
\begin{definition}
A cos satisfent que tot polinomi de grau $\geq 1$ factoritzi completament en factors lineals s'anomena algebraicament tancat.
\end{definition}
\begin{definition}
Donat $f\in\mathcal{L}(E)$ diem que $W\subseteq E$ is a subespai invariant de $E$ per $f$ si $f(W)\subseteq W$.
\end{definition}
\begin{lemma}
Donat $f\in\mathcal{L}(E)$ si $E=W_1\oplus W_2$ amb $W_1,W_2$ subespais invariants, aleshores $p_f(x)=p_{f|W_1}(x)p_{f|W_2}(x)$ i $m_f(x)=\text{mcm}(m_{f|W_1}(x),m_{f|W_2}(x))$.
\end{lemma}
\begin{theorem}
Sigui $f\in\mathcal{L}(E)$ , $\dim E=n<\infty$. Si $p_f(x)=q_1(x)^{n_1}\cdots q_r(x)^{n_r}$ i $m_f(x)=q_1(x)^{m_1}\cdots q_r(x)^{m_r}$ amb $q_i(x)$ factors irreductibles diferents. Aleshores $E=\text{Ker}(q_1(f)^{m_1})\oplus\cdots\oplus\text{Ker}(q_r(f)^{m_r})$. A més $\text{Ker}(q_i(f)^{m_i})=n_i\text{deg}(q_i(x))$.
\end{theorem}
\begin{theorem}
Si $p_f(x)=(x-\lambda_1)^{n_1}\cdots(x-\lambda_k)^{n_k}$ i $g$ compleix:
\begin{enumerate}
    \item $p_f(x)=p_g(x)$
    \item $m_f(x)=m_g(x)$
    \item $\dim(\text{Ker}((f-\lambda id)^r))=\dim(\text{Ker}((g-\lambda id)^r))$ $\forall\lambda\in K$ $\forall r\geq 1$,
\end{enumerate}
llavors $f\sim g$.
\end{theorem}
\begin{prop}
Donats $E$ a $K$-espai vectorial de dimensió $n<\infty$, $A\in\mathcal{M}_n(K)$. Si $p_A(x)=\pm(x-\lambda_1)^{n_1}\cdots(x-\lambda_k)^{n_k}$, existeix a matrix invertible $P$ complint: $$P^{-1}AP=\begin{pmatrix}
J_1 & & & \\
& J_2 & & \\
& & \ddots & \\
& & & J_e \\
\end{pmatrix}$$
on $J_1,\ldots,J_e$ són blocs de Jordan de valors propis $\lambda_1,\ldots,\lambda_k$ complint:
\begin{enumerate}
    \item Per a cada $\lambda_i$ la suma de les mides dels blocs de Jordan de valor propi $\lambda_i$ is $n_i$.
    \item Les mides dels blocs de Jordan estan determinades per $\dim(\text{Ker}((f-\lambda id)^r))$.
\end{enumerate}
\end{prop}
\subsection{Formes bilineals simètriques}
\begin{definition}
Siguin $K$ a cos i $E,F,G$ tres $K$-espais vectorials. Diem que a function $\varphi:E\times F\rightarrow G$ is bilineal si:
\begin{enumerate}
    \item $\varphi(\lambda u_1+\mu u_2,v)=\lambda \varphi(u_1,v)+\mu \varphi(u_2,v)$ $\forall u_1,u_2\in E$, $\forall v\in F$ i $\forall\lambda,\mu\in K$.
    \item $\varphi(u,\lambda v_1+\mu v_2)=\lambda \varphi(u,v_1)+\mu \varphi(u,v_2)$ $\forall v_1,v_2\in F$, $\forall u\in E$ i $\forall\lambda,\mu\in K$.
\end{enumerate}
\end{definition}
\begin{definition}
Una forma bilineal sobre el $K$-espai vectorial $E$ is a function lineal $\varphi:E\times E\rightarrow K$.
\end{definition}
\begin{definition}
Una forma bilineal $\varphi:E\times E\rightarrow K$ is simètrica si $\varphi(u,v)=\varphi(v,u)$ $\forall u,v\in E$.
\end{definition}
\begin{definition}
Sigui $\varphi:E\times E\rightarrow K$ a forma bilineal i sigui $B=(v_1,\ldots,v_n)$ a base de $E$. La matrix de la forma bilineal $\varphi$ respecte de la base $B$ is la matrix: $$[\varphi]_B=\begin{pmatrix}
\varphi(v_1,v_1) & \varphi(v_1,v_2) & \cdots & \varphi(v_1,v_n) \\
\varphi(v_2,v_1) & \varphi(v_2,v_2) & \cdots & \varphi(v_2,v_n) \\
\vdots & \vdots & \ddots & \vdots \\
\varphi(v_n,v_1) & \varphi(v_n,v_2) & \cdots & \varphi(v_n,v_n) \\
\end{pmatrix}.$$ 
\end{definition}
\begin{prop}
Sigui $B=(u_1,\ldots,u_n)$ a base de $E$. Una forma bilineal $\varphi$ sobre $E$ is simètrica si, i només si, $[\varphi]_B$ is a matrix simètrica.
\end{prop}
\begin{prop}
Sigui $\varphi:E\times E\rightarrow K$ a forma bilineal. Siguin $B,B'$ bases de $E$. Aleshores es compleix $[\varphi]_{B'}=([id]_{B',B})^t[\varphi]_B[id]_{B',B}$.
\end{prop}
\begin{definition}
Sigui $\varphi:E\times E\rightarrow \mathbb{R}$ a forma bilineal. Es diu que $u,v\in E$ són ortogonals si $\varphi(u,v)=0$. A vector no nul $v\in E$ is isòtrop si is ortogonal a ell mateix, is a dir, si $\varphi(v,v)=0$. Sigui $B=(v_1,\ldots,v_n)$ a base de $E$. Es diu que $B$ is a base ortogonal respecte de $\varphi$ si $\varphi(v_i,v_j)=0$ $\forall i\ne j$.
\label{ALG-isotrop}
\end{definition}
\begin{theorem}
Sigui $\varphi:E\times E\rightarrow \mathbb{R}$ a forma bilineal simètrica. Llavors $E$ té a base ortogonal respecte de $\varphi$.
\end{theorem}
\begin{definition}
Sigui $\varphi$ a forma bilineal simètrica sobre $E$. Sigui $W$ a subespai vectorial de $E$. Definim l'ortogonal de $W$ respecte de $\varphi$ per $$W^\perp=\{u\in E:\varphi(u,\omega)=0\;\forall\omega\in W\}$$ Definim el radical de $\varphi$ per $$\text{rad }\varphi=E^\perp$$ Direm que $\varphi$ is no singular si $\text{rad}(\varphi)=\{0\}$.
\label{ALG-singular}
\end{definition}
\begin{definition}
Sigui $\varphi$ a forma bilineal simètrica no singular sobre $E$. Donat a $u\in E$ definim $\varphi_u:E\rightarrow\mathbb{R}$, $\varphi_u(v)=\varphi(u,v)$. Llavors l'function 
\begin{align*}
    \Phi: E&\rightarrow E^*\\
    u&\mapsto\varphi_u
\end{align*} is a isomorfisme. 
\end{definition}
\begin{definition}
Sigui $\varphi$ a forma bilineal simètrica no singular sobre $E$. Sigui $W$ a subespai vectorial de $E$. Llavors:
\begin{enumerate}
    \item $\dim E=\dim W+\dim W^\perp$.
    \item $(W^\perp)^\perp=W$.
    \item Si la restricció de $\varphi$ sobre $W$ is no singular, llavors $E=W\oplus W^\perp$.
\end{enumerate}
\end{definition}
\begin{definition}
Sigui $\varphi$ a forma bilineal simètrica sobre $E$. Direm que la suma $W_1+W_2$ de dos subespais vectorials $W_1$ i $W_2$ de $E$ is a suma ortogonal si is directa i $\varphi(u,v)=0$ $\forall u\in W_1$ i $v\in W_2$. Escriurem $W_1\perp W_2$ per denotar que la suma $W_1+W_2$ is ortogonal.
\end{definition}
\begin{definition}
Sigui $\varphi$ a forma bilineal simètrica sobre $E$. Siguin $W_1$ i $W_2$ dos subespais vectorials de $E$ such that $E=W_1\perp W_2$. Llavors per a cada vector $v\in E$ existeixen $v_1\in W_1$ i $v_2\in W_2$ únics such that $v=v_1+v_2$. L'function $\pi:E\rightarrow W_i$ definida per $\pi(v)=v_i$ amb $v_i\in W_i$ es diu que is la projecció ortogonal de $E$ sobre $W_i$ segons la descomposició $E=W_1\perp W_2$.
\label{perpendicular}
\end{definition}
\begin{definition}
Una geometria ortogonal sobre $\mathbb{R}$ is a parell $(E,\varphi)$, on $E$ is a $\mathbb{R}$-espai vectorial i $\varphi$ is a forma bilineal simètrica sobre $E$.
\end{definition}
\begin{definition}
Siguin $(E_1,\varphi_1)$ i $(E_2,\varphi_2)$ dues geometries ortogonals sobre $\mathbb{R}$. Una isometria de $(E_1,\varphi_1)$ a $(E_2,\varphi_2)$ is a isomorfisme $f:E_1\rightarrow E_2$ such that $$\varphi_2(f(u),f(v))=\varphi_1(u,v)$$ per a tot $u,v\in E_1$. Direm que $(E_1,\varphi_1)$ i $(E_2,\varphi_2)$ són isomètriques si existeix a isometria de $(E_1,\varphi_1)$ a $(E_2,\varphi_2)$.
\label{isometry}
\end{definition}
\begin{definition}
Sigui $E$ a $\mathbb{R}$-espai vectorial. Direm que dues formes bilineals simètriques $\varphi_1,\varphi_2$ sobre $E$ són equivalents si, i només si, $(E,\varphi_1)$ i $(E,\varphi_2)$ són isomètriques.
\end{definition}
\begin{definition}
Siguin $A,B\in\mathcal{M}_n(\mathbb{R})$. Direm que $A$ i $B$ són congruents si existeix a matrix $P\in\mathcal{M}_n(\mathbb{R})$ invertible such that $A=P^tBP$.
\end{definition}
\begin{prop}
Sigui $E$ a $\mathbb{R}$-espai vectorial de dimensió $n<\infty$ i $\varphi_1,\varphi_2$ formes bilineals simètriques sobre $E$. Sigui $B_1$ a base de $V$. Llavors les condicions següents són equivalents:
\begin{enumerate}
    \item Les geometries ortogonals $(E,\varphi_1)$ i $(E,\varphi_2)$ $\varphi$ i $\psi$ són isomètriques.
    \item Existeix a bases $B_2$ de $E$ such that $[\varphi_1]_{B_1}=[\varphi_2]_{B_2}$.
    \item Les matrices $[\varphi_1]_{B_1}$ i $[\varphi_2]_{B_2}$ són congruents.
\end{enumerate}
\end{prop}
\begin{theorem}[\bfseries Teorema de Sylvester o Llei d'inèrcia]
Sigui $E$ a $\mathbb{R}$-espai vectorial de dimensió $n<\infty$. Sigui $\varphi$ a forma bilineal simètrica sobre $E$. Llavors existeix a base $B$ de $E$ such that $$[\varphi]_B=\begin{pmatrix}
0 &&&&&&&&&\\
& \ddots&&&&&&&\\
&& 0&&&&\bigzero&&\\
&&& 1 &&&&&\\
&&&& \ddots &&&&\\
&&&&& 1 &&&\\
&&\bigzero&&&& -1 &&\\
&&&&&&&\ddots &\\
&&&&&&&& -1
\end{pmatrix}$$
on a la diagonal hi ha $r_0\;0's$, $r_+\;1's$ i $r_-\;-1's$ i $r,r_+,r_-$ no depenen de la base $B$.
\end{theorem}
\begin{definition}
Sigui $\varphi$ a forma bilineal simètrica sobre a $\mathbb{R}$-espai vectorial $E$ de dimensió $n<\infty$. Sigui $B$ a base ortogonal de $E$ respecte de $\varphi$. Definim el rang de $\varphi$ com $\rank (\varphi)=\rank ([\varphi]_B)$. Definim la signatura de $\varphi$ com $\text{sig}(\varphi)=(r_+,r_-)$, on $r_+$ is el nombre de reals positius que hi ha a la diagonal de $[\varphi]_B$ i $r_-$ is el nombre de reals negatius que hi ha a la diagonal de $[\varphi]_B$.
\end{definition}
\begin{theorem}
Siguin $(E_1,\varphi_1)$, $(E_2,\varphi_2)$ dues geometries ortogonals sobre $\mathbb{R}$ de dimensió finita. Llavors $(E_1,\varphi_1)$ i $(E_2,\varphi_2)$ són isomètriques si, i només si, $\dim E_1=\dim E_2$ i $\text{sig}(\varphi_1)=\text{sig}(\varphi_2)$.
\end{theorem}
\begin{definition}
Sigui $\varphi$ a forma bilineal simètrica sobre a $\mathbb{R}$-espai vectorial $E$. Es diu que $\varphi$ is definida positiva si $\forall v\in E$, $v\ne 0$, tenim $\varphi(v,v)>0$. Es diu que $\varphi$ is definida negativa si $\forall v\in E$, $v\ne 0$, tenim $\varphi(v,v)<0$. 
\end{definition}
\begin{definition}
A producte escalar sobre a $\mathbb{R}$-espai vectorial $E$ is a forma bilineal simètrica definida positiva sobre $E$. A espai vectorial euclidià is a parell $(E,\varphi)$, on $E$ is a $\mathbb{R}$-espai vectorial i $\varphi$ is a producte escalar sobre $E$.\label{espai_euclidia}
\end{definition}
\begin{theorem}[Desigualtat de Cauchy-Schwartz]
Sigui $\varphi$ a producte escalar sobre a $\mathbb{R}$-espai vectorial $E$, llavors: $$\varphi(u,v)^2\leq \varphi(u,u)\varphi(v,v)\quad\forall u,v\in E$$
\end{theorem}
\begin{definition}
Sigui $E$ a $\mathbb{R}$-espai vectorial. Una norma sobre $E$ is a function \begin{align*}
    \|\;\|:E&\rightarrow\mathbb{R}\\
    u&\mapsto\|u\|
\end{align*}
such that \begin{enumerate}
    \item $\|u\|=0\iff u=0$.
    \item $\|\lambda u\|=|\lambda|\|u\|$, $\forall u\in E$, $\lambda\in\mathbb{R}$.
    \item $\|u+v\|\leq\|u\|+\|v\|$, $\forall u,v\in E$.
\end{enumerate}
\end{definition}
\begin{prop}
Sigui $(E,\varphi)$ a espai euclidià. Llavors l'function
\begin{align*}
    \|\;\|_\varphi:E&\rightarrow\mathbb{R}\\
    u&\mapsto\|u\|_\varphi=\sqrt{\varphi(u,u)}
\end{align*}
és a norma, que es diu norma associada al producte escalar $\varphi$.
\end{prop}
\begin{definition}
Sigui $(E,\varphi)$ a espai vectorial euclidià de dimensió finita. Diem que a base $B=(v_1,\ldots,v_n)$ is ortonormal respecte de $\varphi$ si is ortogonal respecte de $\varphi$ i $\|v_i\|_\varphi=1$ per a $i=1,\ldots,n$.
\end{definition}
\begin{corollary}
Sigui $(E,\varphi)$ a espai vectorial euclidià de dimensió finita. Llavors $E$ té a base ortonormal respecte $\varphi$.
\end{corollary}
\begin{definition}
Sigui $(E,\varphi)$ a espai vectorial euclidià. Siguin $u,v\in E\setminus\{0\}$. Definim l'angle respecte $\varphi$ entre $u$ i $v$ com l'únic $\alpha\in[0,\pi]$ such that: $$\cos{\alpha}=\frac{\varphi(u,v)}{\|u\|_\varphi\|v\|_\varphi}$$
\end{definition}
\begin{definition}
Sigui $(E,\varphi)$ a espai vectorial euclidià. Sigui $f\in\mathcal{L}(E)$. Definim l'adjunt de $f$ respecte de $\varphi$ com l'únic endomorfisme $f'$ de $E$ such that $\varphi(f(u),v)=\varphi(u,f'(v))$ $\forall u,v\in E$. Si $f=f'$ diem que $f$ is autoadjunt. 
\end{definition}
\begin{lemma}
Sigui $(E,\varphi)$ a espai vectorial euclidià de dimensió $n<\infty$. Sigui $f$ a endomorfisme autoadut de $E$. Llavors existeixen $\lambda_1,\ldots,\lambda_n\in\mathbb{R}$ such that $$p_f(x)=(x-\lambda_1 )\cdots(x-\lambda_n)$$
\end{lemma}
\begin{definition}
Sigui $A\in\mathcal{M}_n(K)$. Aleshores $A$ is ortogonal si, i només si, $PP^t=P^tP=I_n$.
\end{definition}
\begin{theorem}[Teorema espectral]
Sigui $(E,\varphi)$ a espai vectorial euclidià de dimensió $n<\infty$. Sigui $f\in\mathcal{L}(E)$ autoadjunt de $E$. Llavors $E$ té a base ortonormal de vectors propis de $f$. En particular, l’endomorfisme $f$ diagonalitza.
\end{theorem}
\begin{corollary}
Tota matrix simètrica $A\in\mathcal{M}_n(\mathbb{R})$ is diagonalitzable.
\end{corollary}
\begin{definition}
Donada a matrix $A\in\mathcal{M}_{m\times n}(\mathbb{C})$, $A=(a_{ij})$ denotem per $\overline{A}=(\overline{a_{ij}})$ la conjugada de $A$. $\forall A_1,A_2,A,B$ matrices de mides adequades i $\forall\lambda\in\mathbb{C}$ es satisfan les següents propietats:
\begin{enumerate}
    \item $\overline{A_1+A_2}=\overline{
    A_1}+\overline{A_2}$
    \item $\overline{AB}=\overline{
    A}\overline{B}$
    \item $\overline{\lambda A}=\overline{\lambda}\overline{A}$
\end{enumerate}
\end{definition}
\begin{theorem}[Regla dels signes de Descartes]
Donat a polinomi $P(x)=a_dx^d+\cdots+a_0$:
\begin{enumerate}
    \item El nombre d'arrels positives de $P(x)$ is com a molt igual al nombre de canvis de signe en $[a_{\!_d},a_{\!_{d-1}},\ldots,a_{\!_1},a_{\!_0}]$.
    \item SI $P(x)=a_d(x-\alpha_1)^{n_1}\cdots(x-\alpha_r)^{n_r}$, aleshores el nombre d'arrels positives is igual al nombre de canvis de signe (comptant les arrels amb multiplicitat).
\end{enumerate}
\end{theorem}
\end{multicols}
\end{document}