\documentclass[../../../main.tex]{subfiles}


\begin{document}
\renewcommand{\col}{\alg}
\begin{multicols}{2}[\section{Linear algebra}]
  \subsection{Matrices}
  \subsubsection{Linear systems}
  \begin{definition}
    A \emph{linear equation} is an equation of the form $$a_1x_1+\cdots+a_nx_n=b$$ where $x_1,\ldots,x_n$ are the \emph{variables} or \emph{unknowns} and $a_i,b\in\RR$, $i=1,\ldots,n$, are the coefficients of the equation. The term $b$ is usually called \emph{constant term}.
  \end{definition}
  \begin{definition}
    A \emph{system of linear equations} is a collection of one or more linear equations involving the same set of variables.
  \end{definition}
  \begin{definition}
    Let
    \begin{equation*}
      \arraycolsep=1pt
      \left\{
      \begin{aligned}
        a_{11}x_1  + \cdots +  a_{1n}x_n & =  b_1     \\
                                         & \;\;\vdots \\
        a_{m1}x_1  + \cdots +  a_{mn}x_n & =  b_m
      \end{aligned}
      \right.
    \end{equation*}
    be a system of linear equations. A \emph{solution of a system of equations} is a set of numbers $c_1,\ldots,c_n$ such that $$a_{i1}c_1+\cdots+a_{in}c_n=b_i$$ for $i=1,\ldots,m$. A linear system may behave in three possible ways:
    \begin{enumerate}
      \item The system has a unique solution.
      \item The system has infinitely many solutions.
      \item The system has no solution.
    \end{enumerate}
  \end{definition}
  \begin{definition}
    Two systems of equations are \emph{equivalent} if they have the same solutions.
  \end{definition}
  \subsubsection{Matrices}
  \begin{definition}[Matrix]
    A \emph{matrix} $\vf{A}$ with coefficients in $\RR$ is a table of real numbers arranged in rows and columns. That is, $\vf{A}$ is of the form:
    \begin{equation*}
      \vf{A}=(a_{ij})=
      \begin{pmatrix}
        a_{11} & \cdots & a_{1n} \\
        \vdots & \ddots & \vdots \\
        a_{m1} & \cdots & a_{mn}
      \end{pmatrix}
    \end{equation*}
    for some values $a_{ij}\in\RR$, $i=1,\ldots,m$ and $j=1,\ldots,n$. The set of $m\times n$ matrices with real coefficients is denoted by $\mathcal{M}_{m\times n}(\RR)$\footnote{In the case when $m=n$ we will denote $\mathcal{M}_{n\times n}(\RR)$ by $\mathcal{M}_n(\RR)$.}.
  \end{definition}
  \begin{definition}
    Let $\vf{A},\vf{B}\in\mathcal{M}_{m\times n}(\RR)$ and $\alpha\in\RR$. If $\vf{A}=(a_{ij})$ and $\vf{B}=(b_{ij})$, we define the \emph{sum} $\vf{A}+\vf{B}$ as: $$\vf{A}+\vf{B}=(a_{ij}+b_{ij})$$
    We define the \emph{product} $\alpha\vf{A}$ as: $$\alpha\vf{A}=(\alpha a_{ij})$$
  \end{definition}
  \begin{proposition}[Properties of addition and scalar multiplication of matrices]
    The following properties are satisfied:
    \begin{enumerate}
      \item Commutativity: $$\vf{A}+\vf{B}=\vf{B}+\vf{A}$$ for all $\vf{A},\vf{B}\in\mathcal{M}_{m\times n}(\RR)$.
      \item Associativity: $$(\vf{A}+\vf{B})+\vf{C}=\vf{A}+(\vf{B}+\vf{C})$$ for all $\vf{A},\vf{B},\vf{C}\in\mathcal{M}_{m\times n}(\RR)$.
      \item Additive identity element: $\exists\vf{0}\in\mathcal{M}_{m\times n}(\RR)$ such that $$\vf{A}+\vf{0}=\vf{A}$$ for all $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$.
      \item Additive inverse element: $\forall\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ $\exists(-\vf{A})\in\mathcal{M}_{m\times n}(\RR)$ such that $$\vf{A}+(-\vf{A})=\vf{0}$$
      \item Distributivity: $$(\alpha+\beta)\vf{A}=\alpha\vf{A}+\beta\vf{A}$$ for all $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ and all $\alpha,\beta\in\RR$.
    \end{enumerate}
  \end{proposition}
  \begin{definition}
    Let $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ and $\vf{B}\in\mathcal{M}_{n\times p}(\RR)$. We define the \emph{product} $\vf{A}\vf{B}$ as $$\vf{A}\vf{B}=(c_{ij})\quad\text{where }c_{ij}=\sum_{k=1}^na_{ik}b_{kj}$$
  \end{definition}
  \begin{proposition}[Properties of matrix product]
    The following properties are satisfied:
    \begin{enumerate}
      \item Associativity: $$(\vf{A}\vf{B})\vf{C}=\vf{A}(\vf{B}\vf{C})$$ for all $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$, $\vf{B}\in\mathcal{M}_{n\times p}(\RR)$ and $\vf{C}\in\mathcal{M}_{p\times q}(\RR)$.
      \item Multiplicative identity element: $\exists\vf{I}_n\in\mathcal{M}_n(\RR)$ such that
            \begin{align*}
               & \vf{A}\vf{I}_n=\vf{A}\quad\forall\vf{A}\in\mathcal{M}_{m\times n}(\RR)\text{ and } \\
               & \vf{I}_n\vf{A}=\vf{A}\quad\forall\vf{A}\in\mathcal{M}_{n\times p}(\RR)
            \end{align*}
      \item Distributivity: $$(\vf{A}+\vf{B})\vf{C}=\vf{A}\vf{C}+\vf{B}\vf{C}$$ for all $\vf{A},\vf{B}\in\mathcal{M}_{m\times n}(\RR)$ and $\vf{C}\in\mathcal{M}_{n\times p}(\RR)$.
    \end{enumerate}
  \end{proposition}
  \begin{definition}
    We say that a matrix $\vf{A}\in\mathcal{M}_n(\RR)$ is \emph{invertible} if there is a matrix $\vf{B}\in\mathcal{M}_n(\RR)$ satisfying $$\vf{A}\vf{B}=\vf{B}\vf{A}=\vf{I}_n$$
    The set of invertible matrices of size $n$ over $\RR$ is denoted by $\GL_n(\RR)$\footnote{Or more generally, the set of invertible matrices of size $n$ over a field (see \cref{AS_field}) $K$ is denoted by $\GL_n(K)$.}.
  \end{definition}
  \begin{lemma}
    The product of invertible matrices is invertible.
  \end{lemma}
  \subsubsection{Echelon form of a matrix}
  \begin{definition}
    Let $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ be a matrix. The \emph{$i$-th pivot} of $\vf{A}$ is the first nonzero element in the $i$-th row of $\vf{A}$.
  \end{definition}
  \begin{definition}[Row echelon form]
    A matrix $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ is in \emph{row echelon form} if:
    \begin{itemize}
      \item All rows consisting of only zeros are at the bottom.
      \item The pivot of a nonzero row is always strictly to the right of the pivot of the row above it.
    \end{itemize}
  \end{definition}
  \begin{definition}[Reduced row echelon form]
    A matrix $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ is in \emph{reduced row echelon form} if:
    \begin{itemize}
      \item It is in row echelon form.
      \item Pivots are equal to 1.
      \item Each column containing a pivot has zeros in all its other entries.
    \end{itemize}
  \end{definition}
  \begin{theorem}[Gau\ss' theorem]
    Let $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ be a matrix. Then, there is a matrix $\vf{P}\in\GL_m(\RR)$ such that $\vf{P}\vf{A}=\vf{A'}$ is in reduced row echelon form. Moreover, $\vf{A'}$ is uniquely determined by $\vf{A}$.
  \end{theorem}
  \begin{theorem}[PAQ reduction theorem]
    Let $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ be a matrix. Then, there exist matrices $\vf{P}\in\GL_m(\RR)$ and $\vf{Q}\in\GL_n(\RR)$ such that:
    $$\vf{P}\vf{A}\vf{Q}=\left(
      \begin{array}{@{\,} c|c @{\,}}
          \vf{I}_r & \vf{0} \\
          \hline
          \vf{0}   & \vf{0}
        \end{array}
      \right)$$
    The number $r$ is uniquely determined by $\vf{A}$.
  \end{theorem}
  \subsubsection{Rank of a matrix}
  \begin{definition}[Rank]
    Let $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ be a matrix and suppose
    $$\vf{P}\vf{A}\vf{Q}=\left(
      \begin{array}{@{\,} c|c @{\,}}
          \vf{I}_r & \vf{0} \\
          \hline
          \vf{0}   & \vf{0}
        \end{array}
      \right)$$ for some matrices $\vf{P}\in\mathcal{M}_m(\RR)$ and $\vf{Q}\in\mathcal{M}_n(\RR)$. We define the \emph{rank} of $\vf{A}$, denoted by $\rank \vf{A}$, as the number ones in the matrix $\vf{P}\vf{A}\vf{Q}$, that is, $\rank\vf{A}:=r$.
  \end{definition}
  \begin{proposition}
    Let $\vf{A},\vf{A}'\in\mathcal{M}_{m\times n}(\RR)$, $\vf{B},\vf{B}'\in\mathcal{M}_{1\times n}(\RR)$ and $\vf{P}\in\GL_m(\RR)$ be matrices. Suppose we have a system of linear equations $\vf{A}\vf{x}=\vf{B}$. If $\vf{P}(\vf{A}\mid\vf{B})=(\vf{A}'\mid\vf{B}')$\footnote{Here $(\vf{A}\mid\vf{B})$ denotes the augmented matrix obtained by appending the columns of $\vf{B}$ to the columns of $\vf{A}$.}, then the systems $\vf{A}\vf{x}=\vf{B}$ and $\vf{A}'\vf{x}=\vf{B}'$ are equivalent.
  \end{proposition}
  \begin{corollary}
    The reduced row echelon form of an invertible matrix is the identity matrix.
  \end{corollary}
  \begin{definition}[Transposition]
    Let $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ be a matrix. If $\vf{A}=(a_{ij})$, we define the \emph{transpose} $\transpose{\vf{A}}$ of $\vf{A}$ as the matrix $\transpose{\vf{A}}=(b_{ij})$, where $b_{ij}=a_{ji}$ for $i=1,\ldots,m$ and $j=1,\ldots,n$.
  \end{definition}
  \begin{proposition}
    Let $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ be a matrix. Then, $\rank \vf{A}=\rank\transpose{\vf{A}}$.
  \end{proposition}
  \begin{theorem}[Rouch√©-Frobenius theorem]
    Let $\vf{A}\vf{x}=\vf{B}$ be a system of equations with $n$ variables. The system is:
    \begin{itemize}
      \item \emph{determined and consistent} if and only if $$\rank \vf{A}=\rank (\vf{A}\mid \vf{B})=n$$
      \item \emph{indeterminate} with $s$ \emph{free variables} if and only if $$\rank \vf{A}=\rank (\vf{A}\mid \vf{B})=n-s$$
      \item \emph{inconsistent} if and only if $$\rank \vf{A}\ne\rank (\vf{A}\mid \vf{B})$$
    \end{itemize}
  \end{theorem}
  \subsubsection{Determinant of a matrix}
  \begin{definition}[Determinant]
    A determinant is a function $\det:\mathcal{M}_n(\RR)\rightarrow\RR$ satisfying the following properties:
    \begin{enumerate}
      \item If $\vf{A}=(\vf{a}_1\mid\cdots\mid \vf{a}_n)$, where $\vf{a}_i$ are column vectors in $\RR^n$ for $i=1,\ldots,n$ and $\vf{a}_j=\lambda\vf{u}+\mu\vf{v}$ for some other column vectors $\vf{u}$ and $\vf{v}$, then:
            \begin{multline*}
              \det \vf{A}=\det(\vf{a}_1\mid\cdots\mid\vf{a}_j\mid\cdots\mid \vf{a}_n)=\\=\det(\vf{a}_1\mid\cdots\mid \vf{a}_{j-1}\mid\lambda \vf{u}+\mu \vf{v}\mid \vf{a}_{j+1}\mid\cdots\mid \vf{a}_n)=\\=\lambda\det(\vf{a}_1\mid\cdots\mid \vf{a}_{j-1}\mid \vf{u}\mid \vf{a}_{j+1}\mid\cdots\mid \vf{a}_n)+\\+\mu\det(\vf{a}_1\mid\cdots\mid \vf{a}_{j-1}\mid \vf{v}\mid \vf{a}_{j+1}\mid\cdots\mid \vf{a}_n)
            \end{multline*}
      \item The determinant changes its sign whenever two columns are swapped.
      \item $\det \vf{I}_n=1$ for all $n\in\NN$.
    \end{enumerate}
  \end{definition}
  \begin{lemma}
    Whenever two columns of a matrix are identical, the determinant is 0.
  \end{lemma}
  \begin{proposition}
    Let $\vf{A}\in\mathcal{M}_n(\RR)$ be a matrix in its row echelon form. If $\vf{A}=(a_{ij})$, then: $$\det\vf{A}=\prod_{i=1}^na_{ii}$$
  \end{proposition}
  \begin{proposition}
    Let $\vf{A}\in\mathcal{M}_n(\RR)$ be a matrix. The following are equivalent:
    \begin{enumerate}
      \item $\vf{A}$ is not invertible.
      \item $\rank\vf{A}<n$.
      \item $\det\vf{A}=0$.
    \end{enumerate}
  \end{proposition}
  \begin{theorem}
    Let $\det:\mathcal{M}_n(\RR)\rightarrow\RR$ be a determinant. Then, for all matrices $\vf{A},\vf{B}\in\mathcal{M}_n(\RR)$: $$\det (\vf{A}\vf{B})=\det\vf{A}\det\vf{B}$$
  \end{theorem}
  \begin{corollary}
    Let $\det,\det':\mathcal{M}_n(\RR)\rightarrow\RR$ be two determinants. Then, for all matrix $\vf{A}\in\mathcal{M}_n(\RR)$: $$\det\vf{A}={\det}'\vf{A}$$
  \end{corollary}
  \begin{proposition}
    Let $\vf{A}\in\mathcal{M}_n(\RR)$. Then: $$\det \vf{A}=\sum_{\sigma\in \text{S}_n}\varepsilon(\sigma)\prod_{i=1}^na_{i\sigma(i)}$$
  \end{proposition}
  \begin{proposition}
    For all matrix $\vf{A}\in\mathcal{M}_n(\RR)$: $$\det\vf{A}=\det\transpose{\vf{A}}$$
  \end{proposition}
  \begin{proposition}
    Let $\vf{A}=(a_{ij})\in\mathcal{M}_n(\RR)$. We denote by $\vf{A}_{ij}$ the square matrix obtained from $\vf{A}$ by removing the $i$-th row and $j$-th column. Then, for every $i\in\{1,\ldots,n\}$: $$\det\vf{A}=\sum_{j=1}^n(-1)^{i+j}a_{ij}\det\vf{A}_{ij}$$
  \end{proposition}
  \begin{definition}
    Let $\vf{A}=(a_{ij})\in\mathcal{M}_n(\RR)$. We define the \emph{cofactor matrix} $\vf{C}$ of $\vf{A}$ as: $$\vf{C}=(b_{ij}),\quad\text{where }b_{ij}=(-1)^{i+j}\det\vf{A}_{ij}\footnote{$\vf{C}$ is usually denoted as $\cofactor\vf{A}$.}$$ We define the \emph{adjugate matrix} $\adjugate\vf{A}$ of $\vf{A}$ as: $$\adjugate\vf{A}=\transpose{\vf{C}}$$
  \end{definition}
  \begin{theorem}
    Let $\vf{A}\in\mathcal{M}_n(\RR)$. Then: $$\vf{A}\adjugate\vf{A}=(\det \vf{A})\vf{I}_n$$ Moreover if $\det \vf{A}\ne 0$, then: $$\vf{A}^{-1}=\frac{1}{\det \vf{A}}\adjugate\vf{A}$$
  \end{theorem}
  \subsubsection{Block matrices}
  \begin{definition}
    Let $\vf{A}\in\mathcal{M}_{m\times n}(\RR)$ and $r,s\in\NN$ such that $r\leq m$ and $s\leq n$. We define a \emph{block matrix} as a matrix of the form
    $$
      \begin{pmatrix}
        \vf{A}_{11} & \cdots & \vf{A}_{1s} \\
        \vdots      & \ddots & \vdots      \\
        \vf{A}_{r1} & \cdots & \vf{A}_{rs}
      \end{pmatrix}
    $$
    where $\vf{A}_{ij}$, $i=1,\ldots,r$ and $j=1,\ldots,s$ are submatrices of $\vf{A}$ created from partitioning the $\vf{A}$ with $s-1$ vertical lines and $r-1$ horizontal lines.
  \end{definition}
  \begin{proposition}
    Let $\vf{A},\vf{B}\in\mathcal{M}_{m\cdot n}(\RR)$ be a block matrix of the form:
    $$\vf{A}=
      \begin{pmatrix}
        \vf{X} & \vf{0} \\
        \vf{Z} & \vf{Y}
      \end{pmatrix}\quad
      \vf{B}=
      \begin{pmatrix}
        \vf{X} & \vf{W} \\
        \vf{0} & \vf{Y}
      \end{pmatrix}
    $$
    where $\vf{X}\in\mathcal{M}_m(\RR)$, $\vf{Y}\in\mathcal{M}_n(\RR)$, $\vf{Z}\in\mathcal{M}_{n\times m}(\RR)$ and $\vf{W}\in\mathcal{M}_{m\times n}(\RR)$. Then, $$\det\vf{A}=\det(\vf{X})\det(\vf{Y})=\det\vf{B}$$
  \end{proposition}
  \subsection{Vector spaces}
  \subsubsection{Introduction and basic definitions}
  \begin{definition}
    A \emph{vector space} over a field\footnote{See \cref{AS_field}.} $K$ is a set $V$ together with two operations
    \begin{align*}
      \function{+}{V\times V}{V}{(\vf{v}_1,\vf{v}_2)}{\vf{v}_1+ \vf{v}_2} & \function{\cdot}{K\times V}{V}{(\lambda,\vf{v}_2)}{\lambda\cdot\vf{v}_2}
    \end{align*}
    that satisfy the following properties:
    \begin{enumerate}
      \item $\vf{v}_1+(\vf{v}_2+\vf{v}_3)=(\vf{v}_1+\vf{v}_2)+\vf{v}_3\quad\forall\vf{v}_1,\vf{v}_2,\vf{v}_3\in V$.
      \item $\vf{v}_1+\vf{v}_2=\vf{v}_2+\vf{v}_1\quad\forall\vf{v}_1,\vf{v}_2\in V$.
      \item $\exists\vf{0}\in V$ such that $\vf{v}+\vf{0}=\vf{v}\quad\forall\vf{v}\in V$.
      \item $\forall\vf{v}\in V$ there exists $-\vf{v}\in V$ such that $\vf{v}+(-\vf{v})=\vf{0}$.
      \item $\lambda\cdot(\mu\cdot\vf{v})=(\lambda\mu)\cdot\vf{v}\quad\forall\vf{v}\in V$ and $\forall\lambda,\mu\in K$.
      \item $1\cdot\vf{v}=\vf{v}\quad\forall\vf{v}\in V$, where 1 denotes the multiplicative identity element in $K$.
      \item $\lambda\cdot(\vf{v}_1+\vf{v}_2)=\lambda\cdot\vf{v}_1+\lambda\cdot\vf{v}_2\quad\forall\vf{v}_1,\vf{v}_2\in V$ and $\forall\lambda\in K$.
      \item $(\lambda+\mu)\cdot\vf{v}=\lambda\cdot\vf{v}+\mu\cdot\vf{v}\quad\forall\vf{v}\in V$ and $\forall\lambda,\mu\in K$.
    \end{enumerate}
    In these conditions, we say that $(V,+,\cdot)$ is a vector space\footnote{For simplicity we will denote the vector space only by $V$ and if the context is clear we won't refer to its associated field. Moreover sometimes we will also omit the product $\cdot$ between a scalar and a vector.}. The elements of $V$ are called \emph{vectors} and the elements of $K$, \emph{scalars}.
  \end{definition}
  \begin{definition}
    Let $V$ be a vector space over a field $K$ and $U\subseteq V$ be a subset of $V$. Then, $U$ is a vector space over $K$ if the following property is satisfied:
    $$\lambda \vf{u}_1+\mu \vf{u}_2\in U\quad\forall \vf{u}_1,\vf{u}_2\in U\text{ and }\forall\lambda,\mu\in K$$
  \end{definition}
  \begin{definition}
    Let $V$ be a vector space and $U\subseteq V$. $U$ is a \emph{vector subspace} of $V$ if it's itself a vector space with the operations defined in $V$.
  \end{definition}
  \begin{definition}
    Let $V$ be a vector space over a field $K$. A \emph{linear combination} of the vectors $\vf{v}_1,\ldots,\vf{v}_n\in V$ is a vector of the form $$a_1\vf{v}_1+\cdots+a_n\vf{v}_n$$ where $a_i\in K$, $i=1,\ldots,n$.
  \end{definition}
  \begin{definition}
    Let $V$ be a vector space over a field $K$ and $U\subseteq V$. The set $$\langle U\rangle=\{a_1\vf{u}_1+\cdots+a_n\vf{u}_n:a_i\in K,\vf{u}_i\in U,i=1,\ldots,n\}$$ is called \emph{subspace generated} by $U$.
  \end{definition}
  \begin{lemma}
    Let $V$ be a vector space and $U\subseteq V$. Then, $\langle U\rangle$ is a vector subspace of $V$. Moreover, $\langle U\rangle$ is the smallest subspace containing $U$.
  \end{lemma}
  \begin{definition}
    Let $V$ be a vector space and $U\subseteq V$. We say that $U$ is a \emph{generating set} of $V$ if $\langle U\rangle=V$.
  \end{definition}
  \subsubsection{Linear independence}
  \begin{definition}
    Let $V$ be a vector space over a field $K$. The vectors $\vf{v}_1,\ldots,\vf{v}_n\in V$ are \emph{linearly independent} if the unique solution of the equation $$a_1\vf{v}_1+\cdots+a_n\vf{v}_n=0$$ for $a_i\in K$, $i=1,\ldots,n$, is $a_1=\cdots=a_n=0$. Otherwise we say that the vectors $\vf{v}_1,\ldots,\vf{v}_n$ are \emph{linearly dependent}.
  \end{definition}
  \begin{lemma}
    Let $V$ be a vector space. The vectors $\vf{v}_1,\ldots,\vf{v}_n$ are linearly dependent if and only if one of them is a linear combination of the others.
  \end{lemma}
  \begin{definition}
    Let $V$ be a vector space. A \emph{basis} of $V$ is an ordered set $\mathcal{B}=(\vf{v}_1,\ldots,\vf{v}_n)$ of vectors of $V$ such that:
    \begin{enumerate}
      \item $\langle \vf{v}_1,\ldots,\vf{v}_n\rangle = V$.
      \item $\vf{v}_1,\ldots,\vf{v}_n$ are linearly independent.
    \end{enumerate}
  \end{definition}
  \begin{lemma}[Steinitz exchange lemma]
    Let $V$ be a vector space, $\mathcal{B}$ be bases of $V$ be and $\vf{v}_1,\ldots,\vf{v}_k\in V$ be linearly independent vectors of $V$. Then, we can exchange $k$ appropriate vectors of $\mathcal{B}$ by $\vf{v}_1,\ldots,\vf{v}_k$ to define a new basis.
  \end{lemma}
  \begin{corollary}
    Let $V$ be a vector space that has a finite basis $\mathcal{B}=(\vf{v}_1,\ldots,\vf{v}_n)$. Then, all basis of $V$ be are finite and they have the same number ($n$) of vectors.
  \end{corollary}
  \begin{lemma}
    Let $V$ be a vector space. Suppose we have a generating set $S=\{\vf{v}_1,\ldots,\vf{v}_n\}$ of $V$. Then, $V$ be admits a basis formed with a subset of $S$.
  \end{lemma}
  \begin{definition}
    Let $V$ be a finite vector space over a field $K$. The \emph{dimension} of $V$, denoted by $\dim_K V$ (or $\dim V$ if $K$ can be inferred from context), is the number of vectors in any basis of $V$.
  \end{definition}
  \begin{definition}
    Let $V$ be a finite vector space over a field $K$, $\mathcal{B}=(\vf{v}_1,\ldots,\vf{v}_n)$ be a basis of $V$ be and $\vf{v}\in V$. Suppose $$\vf{v}=a_1\vf{v}_1+\cdots+a_n\vf{v}_n$$ for some $a_i\in K$, $i=1,\ldots,n$. We call $(a_1,\ldots,a_n)\in K^n$ \emph{coordinates} of $\vf{v}$ on the basis $\mathcal{B}$ and we denote it by $[\vf{v}]_{\mathcal{B}}$.
  \end{definition}
  \begin{proposition}
    Let $V$ be a vector space. If $\dim V<\infty$, the maximum number of linearly independent vectors is equal to $\dim V$. If $\dim V=\infty$, there is no such maximum.
  \end{proposition}
  \begin{proposition}
    Let $V$ be a vector space of dimension $n$. Then, $n$ is the minimum size of a generating set of $V$.
  \end{proposition}
  \begin{proposition}
    Let $V$ be a finite vector space and $U$ be a vector subspace of $V$. Then, $\dim U\leq\dim V$ and $$\dim U=\dim V\iff U=V$$
  \end{proposition}
  \subsubsection{Sum of subspaces}
  \begin{lemma}
    Let $V$ be a vector space and $U,W\subseteq V$ be two vector subspaces of $V$. Then, the intersection $U\cap W$ is a vector subspace of $V$.
  \end{lemma}
  \begin{definition}
    Let $V$ be a vector space and $U,W\subseteq V$ be two vector subspaces of $V$. The \emph{sum} of $U$ and $W$ is: $$U+W=\langle U\cup W\rangle=\{\vf{u}+\vf{w}: \vf{u}\in U,\vf{w}\in W\}$$
  \end{definition}
  \begin{proposition}[Gra\ss mann formula]
    Let $V$ be a finite vector space and $U,W\subseteq V$ be two vector subspace of $V$. Then: $$\dim (U+W)+\dim(U\cap W)=\dim U+\dim W$$
  \end{proposition}
  \begin{lemma}
    Let $V$ be a vector space and $U,W\subseteq V$ be two vector subspaces of $V$. Then, $U\cap W=\{0\}$ if and only if all vectors $\vf{v}\in U+W$ can be written uniquely as $\vf{v}=\vf{u}+\vf{w}$, with $\vf{u}\in U$ and $\vf{w}\in W$.
  \end{lemma}
  \begin{definition}[Direct sum]
    Let $V$ be a vector space and $U,W\subseteq V$ be two vector subspaces of $V$. Then, the sum $U+W$ is \emph{direct} if $U\cap W=\{0\}$. In this case we denote the sum as $U\oplus W$. More generally, if $U_1,\ldots,U_n\subseteq V$ are vector subspaces of $V$, the sum $U=U_1+\cdots+U_n$ is direct if all vector $\vf{u}\in U$ can be written uniquely as $\vf{u}=\vf{u}_1+\cdots+\vf{u}_n$, where $\vf{u}_i\in U_i$ for $i=1,\ldots,n$. In this case we denote the sum by $U_1\oplus\cdots\oplus U_n$.
  \end{definition}
  \subsubsection{Rank of a matrix}
  \begin{definition}
    Let $\vf{A}\in\mathcal{M}_{n\times m}(\RR)$. The \emph{row rank} of $\vf{A}$ is the dimension of the subspace generated by the rows of $\vf{A}$ in $\RR^m$. Analogously, the \emph{column rank} of $\vf{A}$ is the dimension of the subspace generated by the columns of $\vf{A}$ in $\RR^n$.
  \end{definition}
  \begin{proposition}
    Let $\vf{A}\in\mathcal{M}_{n\times m}(\RR)$. Then, the row rank of $\vf{A}$ is equal to the column rank of $\vf{A}$. Therefore, we refer to it simply as \emph{rank} of $\vf{A}$ or $\rank\vf{A}$.
  \end{proposition}
  \begin{definition}
    Let $\vf{A}\in\mathcal{M}_n(\RR)$. A \emph{minor} of order $k$ of $\vf{A}$ is a submatrix $\vf{A}'\in\mathcal{M}_k(\RR)$ obtained from $\vf{A}$ selecting $k$ rows and $k$ columns of $\vf{A}$.
  \end{definition}
  \begin{proposition}
    Let $\vf{A}\in\mathcal{M}_{n\times m}(\RR)$. Then:
    $$\rank\vf{A}=\max\{k:\text{$\vf{A}$\ has an invertible minor of order $k$}\}$$
  \end{proposition}
  \subsubsection{Quotient vector space}
  \begin{definition}
    Let $V$ be a vector space and $U\subseteq V$ be a vector subspace. We say that $W\subseteq V$ is a \emph{complementary subspace} of $U$ if $U\oplus W=V$.
  \end{definition}
  \begin{definition}
    Let $V$ be a finite vector space of dimension $n$ and $U\subseteq V$ be a vector subspace of dimension $m$. Then, there exists a complementary subspace of $U$ and its dimension is $n-m$.
  \end{definition}
  \begin{definition}
    Let $V$ be a vector space and $U\subseteq V$ be a vector subspace. We say the vectors $\vf{v}_1,\vf{v}_2\in V$ are \emph{equivalent} modulo $U$, $\vf{v}_1\sim_U\vf{v}_2$, if $\vf{v}_1-\vf{v}_2\in U$.
  \end{definition}
  \begin{lemma}
    Let $V$ be a vector space and $U\subseteq V$ be a vector subspace. Then, $\sim_U$ is an equivalence relation and, moreover, if $\vf{v}\in V$ the \emph{equivalence class} $[\vf{v}]$ of $\vf{v}$ is: $$[\vf{v}]=\vf{v}+U:=\{\vf{v}+\vf{u}:\vf{u}\in U\}$$
  \end{lemma}
  \begin{definition}
    Let $V$ be a vector space over a field $K$ and $U\subseteq V$ be a vector subspace. We define the \emph{quotient space} $\quot{V}{U}$ under $\sim_U$ as the set of equivalence classes with the operations defined as:
    $$[\vf{v}_1]+[\vf{v}_2]=[\vf{v}_1+\vf{v}_2]\qquad \lambda[\vf{v}_1]=[\lambda\vf{v}_1]$$
    for all $\vf{v}_1,\vf{v}_2\in V$ and all $\lambda\in K$.
  \end{definition}
  \begin{proposition}
    Let $V$ be a vector space over a field $K$ and $U\subseteq V$ be a vector subspace. The set $\quot{V}{U}$ together with the two operations defined above is a vector space over $K$.
  \end{proposition}
  \begin{proposition}
    Let $V$ be a finite vector space of dimension $n$ and $U\subseteq V$ be a vector subspace. Then: $$\dim\left(\quot{V}{U}\right)=\dim V-\dim U$$
  \end{proposition}
  \subsection{Linear maps}
  \begin{definition}
    Let $U$, $V$ be two vector spaces over a field $K$. A function $f:U\rightarrow V$ is a \emph{linear map} if $\forall\vf{u}_1,\vf{u}_2\in U$ and $\forall\lambda\in K$ the following two conditions are satisfied:
    \begin{enumerate}
      \item $f(\vf{u}_1+\vf{u}_2)=f(\vf{u}_1)+f(\vf{u}_2)$.
      \item $f(\lambda \vf{u}_1)=\lambda f(\vf{u}_1)$.
    \end{enumerate}
  \end{definition}
  \begin{proposition}
    Let $U$, $V$ be two vector spaces over a field $K$. Then, if $f:U\rightarrow V$ is a linear map, $\forall\vf{u}_1,\vf{u}_2\in U$ and $\forall\lambda,\mu\in K$ we have:
    \begin{enumerate}
      \item $f(\vf{0})=\vf{0}$.
      \item $f(-\vf{u}_1)=-f(\vf{u}_1)$.
      \item $f(\lambda\vf{u}_1+\mu\vf{u}_2)=\lambda f(\vf{u}_1)+\mu f(\vf{u}_2)$.
    \end{enumerate}
  \end{proposition}
  \begin{proposition}
    Let $U$, $V$, $W$ be three vector spaces. If $f:U\rightarrow V$ and $g:V\rightarrow W$ are linear maps, then $g\circ f:U\rightarrow W$ is a linear map.
  \end{proposition}
  \begin{proposition}
    Let $U$, $V$ be two vector spaces. If $f:U\rightarrow V$ is a bijective linear map, then $f^{-1}:U\rightarrow V$ is a linear map.
  \end{proposition}
  \begin{proposition}
    Let $U$, $V$ be two vector spaces, $f:U\rightarrow V$ be a linear map and $W\subseteq U$ and $Z\subseteq V$ be vector subspaces. Then:
    \begin{enumerate}
      \item $f(W)=\{f(\vf{w}): \vf{w}\in W\}\subseteq V$ is a vector subspace.
      \item $f^{-1}(Z)=\{\vf{u}\in U: f(\vf{u})\in Z\}\subseteq U$ is a vector subspace.
    \end{enumerate}
    In particular, $f(V)$ is denoted by $\im f$ and $f^{-1}(\{0\})$ is denoted by $\ker f$ and these subspaces are called \emph{image} of $f$ and \emph{kernel} of $f$, respectively. More precisely, their definitions are:
    $$\im f=\{f(\vf{u}): \vf{u}\in U\}\qquad\ker f=\{\vf{u}\in U: f(\vf{u})=0\}$$
  \end{proposition}
  \begin{proposition}
    Let $U$, $V$ be two vector spaces and $f:U\rightarrow V$ be a linear map. Then:
    \begin{enumerate}
      \item $f$ is injective if and only if $\ker f=\{0\}$
      \item $f$ is surjective if and only if $\im f=V$.
    \end{enumerate}
  \end{proposition}
  \begin{corollary}
    Let $U$, $V$ be two finite vector spaces and $f:U\rightarrow V$ be a linear map. Then:
    \begin{enumerate}
      \item $f$ is injective if and only if $\dim(\ker f)=0$
      \item $f$ is surjective if and only if $\dim(\im f)=\dim V$.
    \end{enumerate}
  \end{corollary}
  \begin{definition}
    \hfill
    \begin{itemize}
      \item A monomorphism is an injective linear map.
      \item An epimorphism is a surjective linear map.
      \item An isomorphism is a bijective linear map.
      \item An endomorphism is a linear map from a vector space to itself.
      \item An automorphism is a bijective endomorphism.
    \end{itemize}
  \end{definition}
  \begin{definition}
    We say that two vector spaces $U$ and $V$ are \emph{isomorphic}, $V\cong U$, if there exists an isomorphism between them.
  \end{definition}
  \begin{proposition}
    Let $U$, $V$ be two vector spaces and $f:U\rightarrow V$ be a monomorphism. If $\vf{u}_1,\ldots,\vf{u}_n\in U$ are linearly independent vectors, then $f(\vf{u}_1),\ldots,f(\vf{u}_n)$  are linearly independent.
  \end{proposition}
  \begin{lemma}
    Let $U$, $V$ be two vector spaces and $f:U\rightarrow V$ be a linear map. If $\vf{u}_1,\ldots,\vf{u}_n\in U$, then: $$\langle f(\vf{u}_1),\ldots,f(\vf{u}_n)\rangle=f(\langle\vf{u}_1,\ldots,\vf{u}_n\rangle)$$
  \end{lemma}
  \begin{corollary}
    Let $U$, $V$ be two vector spaces and $f:U\rightarrow V$ be an epimorphism. If $\langle\vf{u}_1,\ldots,\vf{u}_n\rangle= U$, then $\langle f(\vf{u}_1),\ldots,f(\vf{u}_n)\rangle=V$.
  \end{corollary}
  \begin{corollary}
    Let $U$, $V$ be two vector spaces and $f:U\rightarrow V$ be an isomorphism. If $(\vf{u}_1,\ldots,\vf{u}_n)$ is a basis of $U$, then $(f(\vf{u}_1),\ldots,f(\vf{u}_n))$ is a basis of $V$.
  \end{corollary}
  \begin{theorem}[Coordination theorem]
    Let $V$ be a finite vector space over a field $K$ of dimension $n$ and $\mathcal{B}=(\vf{v}_1,\ldots,\vf{v}_n)$ be a basis of $V$. Then, the function $f:K^n\rightarrow V$ defined by $$f(a_1,\ldots,a_n)=a_1\vf{v}_1+\cdots a_n\vf{v}_n$$ is an isomorphism.
  \end{theorem}
  \begin{corollary}
    Two finite vector spaces are isomorphic if and only if they have the same dimension.
  \end{corollary}
  \subsubsection{Isomorphism theorems}
  \begin{theorem}[First isomorphism theorem]
    Let $U$, $V$ be two vector spaces and $f:U\rightarrow V$ be a linear map. Then, there exists an isomorphism $\Tilde{f}:\quot{U}{\ker f}\rightarrow \im f$ satisfying $f=\Tilde{f}\circ\pi$, where $\pi:U\rightarrow \quot{U}{\ker f}$, $\pi(\vf{u})=[\vf{u}]$.
    \begin{center}
      \begin{minipage}{\linewidth}
        \centering
        \includestandalone[mode=image|tex,width=0.35\linewidth]{Images/first_isomorphism}
        \captionof{figure}{}
      \end{minipage}
    \end{center}
  \end{theorem}
  \begin{corollary}
    Let $U$, $V$ be two vector spaces such that $\dim U=n$ and let $f:U\rightarrow V$ be a linear map. Then: $$\dim(\ker f)+\dim(\im f)=n$$
  \end{corollary}
  \begin{corollary}
    Let $U$, $V$ be two finite vector spaces of dimensions $n$ and $f:U\rightarrow V$ be a linear map. Then: $$f\text{ is injective}\iff f\text{ is surjective}\iff f\text{ is bijective}$$
  \end{corollary}
  \begin{theorem}[Second isomorphism theorem]
    Let $V$ be a vector space and $U,W\subseteq V$ be two vector subspaces. Then, there exists an isomorphism $$\quot{U}{U\cap W}\cong\quot{U+W}{W}$$
  \end{theorem}
  \begin{theorem}[Third isomorphism theorem]
    Let $U$, $V$, $W$ be three vector spaces such that $W\subseteq U\subseteq V$. Then, there exists an isomorphism $$\quot{{(\textstyle\quot{V}{W})}}{{(\textstyle\quot{U}{W})}}\cong\quot{V}{U}$$
  \end{theorem}
  \begin{theorem}
    Let $U$, $V$ be two vector spaces over a field $K$, $\mathcal{B}=(\vf{u}_1,\ldots,\vf{u}_n)$ be a basis of $U$ and $\vf{v}_1,\ldots,\vf{v}_n\in V$ be any vectors of $V$. Then, there exists a unique linear map $f:U\rightarrow V$ such that $f(\vf{u}_i)=\vf{v}_i$, $i=1,\ldots,n$.
  \end{theorem}
  \subsubsection{Matrix of a linear map}
  \begin{proposition}
    Let $U$, $V$ be two finite vector spaces over a field $K$ with $\dim U=n$ and $\dim V=m$, $\mathcal{B}$ and $\mathcal{B}'$ be bases of $U$ be and $V$ respectively and $f:U\rightarrow V$ be a linear map. Then, there exists a matrix $\vf{A}\in\mathcal{M}_{m\times n}(K)$ such that $\forall\vf{u}\in U$: $$[f(\vf{u})]_{\mathcal{B}'}=\vf{A}[\vf{u}]_\mathcal{B}$$
    The matrix $\vf{A}$ is called \emph{matrix} of $f$ in the basis $\mathcal{B}$ and $\mathcal{B}'$ and it is denoted by $[f]_{\mathcal{B},\mathcal{B}'}$\footnote{If $U=V$ and $\mathcal{B}=\mathcal{B}'$, we denote $[f]_{\mathcal{B},\mathcal{B}}$ simply by $[f]_{\mathcal{B}}$.}.
  \end{proposition}
  \begin{corollary}
    Let $V$ be a finite vector space, $\mathcal{B}$ and $\mathcal{B}'$ be two basis of $V$ respectively and $\id:V\rightarrow V$ be the identity linear map. Then, $\forall\vf{u}\in V$ we have: $$[\vf{u}]_{\mathcal{B}'}=[\id]_{\mathcal{B},\mathcal{B}'}[\vf{u}]_\mathcal{B}$$ The matrix $[\id]_{\mathcal{B},\mathcal{B}'}$ is called \emph{change-of-basis matrix}.
  \end{corollary}
  \begin{proposition}
    Let $U$, $V$, $W$ be three vector spaces, $\mathcal{B}$, $\mathcal{B}'$, $\mathcal{B}''$ be bases of $U$, $V$ and $W$ respectively and $f:U\rightarrow V$ and $g:V\rightarrow W$ be linear maps. Then, $g\circ f:U\rightarrow W$ has the following matrix in the basis $\mathcal{B}$ and $\mathcal{B}''$: $$[g\circ f]_{\mathcal{B},\mathcal{B}''}=[g]_{\mathcal{B}',\mathcal{B}''}[f]_{\mathcal{B},\mathcal{B}'}$$
  \end{proposition}
  \begin{corollary}
    Let $V$ be a finite vector space, $\mathcal{B}$ and $\mathcal{B}'$ be two basis of $V$. Then, the matrix $[id]_{\mathcal{B},\mathcal{B}'}$ is invertible and $${\left([\id]_{\mathcal{B},\mathcal{B}'}\right)}^{-1}=[\id]_{\mathcal{B}',\mathcal{B}}$$
  \end{corollary}
  \begin{corollary}
    Let $U$, $V$ be two finite vector spaces, $\mathcal{B}$ and $\mathcal{B}'$ be bases of $U$ and $V$ respectively and $f:U\rightarrow V$ be a linear map. Then:
    \begin{enumerate}
      \item $f$ is injective $\iff\rank[f]_{\mathcal{B},\mathcal{B}'}=\dim U$.
      \item $f$ is surjective $\iff\rank[f]_{\mathcal{B},\mathcal{B}'}=\dim V$.
    \end{enumerate}
  \end{corollary}
  \begin{corollary}
    Let $U$, $V$ be two finite vector spaces. A linear map $f:U\rightarrow V$ is an isomorphism if and only if there exist basis $\mathcal{B}$ and $\mathcal{B}'$ of $U$ and $V$ respectively such that $[f]_{\mathcal{B},\mathcal{B}'}$ is invertible.
  \end{corollary}
  \begin{proposition}[Change of basis formula]
    Let $U$, $V$ be two finite vector spaces, $\mathcal{B}_1$ and $\mathcal{B}_2$ be bases of $U$, $\mathcal{B}_1'$ and $\mathcal{B}_2'$ be bases of $V$ and $f:U\rightarrow V$ be a linear map. Then: $$[f]_{\mathcal{B}_2,\mathcal{B}_2'}=[\id]_{\mathcal{B}_1',\mathcal{B}_2'}[f]_{\mathcal{B}_1,\mathcal{B}_1'}[\id]_{\mathcal{B}_2,\mathcal{B}_1}$$
  \end{proposition}
  \begin{lemma}
    Let $U$, $V$ be two finite vector spaces over a field $K$ with $\dim U=n$ and $\dim V=m$ and $\mathcal{B}$ and $\mathcal{B}'$ be bases of $U$ be and $V$ respectively. Then, any matrix $\vf{A}\in\mathcal{M}_{m\times n}(K)$ determines a linear map $f:U\rightarrow V$ with $[f]_{\mathcal{B},\mathcal{B}'}=\vf{A}$.
  \end{lemma}
  \begin{theorem}
    Let $U$, $V$ be two finite vector spaces and $f:U\rightarrow V$ be a linear map. Then, there exist basis $\mathcal{B}$ of $U$ and $\mathcal{B}'$ of $V$ such that:
    $$[f]_{\mathcal{B},\mathcal{B}'}=\left(
      \begin{array}{c|c}
          \vf{I}_r & \vf{0} \\
          \hline
          \vf{0}   & \vf{0}
        \end{array}\right)$$
    where $r=\dim\left(\im f\right)$.
  \end{theorem}
  \subsubsection{Dual space}
  \begin{lemma}
    Let $U$, $V$ be two finite vector spaces over a field $K$. Then, the set $$\mathcal{L}(U,V):=\{f: f\text{ is a linear map from $U$ to $V$}\}\footnote{If $U=V$, we denote $\mathcal{L}(V,V)$ simply as $\mathcal{L}(V)$.}$$ is a vector space over $K$ with the operations defined as:
    \begin{enumerate}
      \item $(f+g)(\vf{u})=f(\vf{u})+f(\vf{u})\quad\forall f,g\in\mathcal{L}(U,V)$ and $\forall \vf{u}\in U$.
      \item $(f\lambda)(\vf{u})=\lambda f(\vf{u})\quad\forall f,g\in\mathcal{L}(U,V)$, $\forall \vf{u}\in U$ and $\forall \lambda\in K$.
    \end{enumerate}
  \end{lemma}
  \begin{proposition}
    Let $U$, $V$ be two finite vector spaces over a field $K$ with $\dim U=n$ and $\dim V=m$. Then, for all basis $\mathcal{B}$ of $U$ be and $\mathcal{B}'$ of $V$, the function
    $$\function{}{\mathcal{L}(U,V)}{\mathcal{M}_{m\times n}(K)}{f}{[f]_{\mathcal{B},\mathcal{B}'}}$$
    is an isomorphism.
  \end{proposition}
  \begin{corollary}
    Let $U$, $V$ be two finite vector spaces with $\dim U=n$, $\dim V=m$. Then, $\dim \mathcal{L}(U,V)=nm$.
  \end{corollary}
  \begin{definition}\label{LA_dual}
    Let $V$ be a vector space over a field $K$. We define the \emph{dual space} $V^*$ of $V$ as: $$V^*:=\mathcal{L}(V,K)$$
  \end{definition}
  \begin{proposition}
    Let $V$ be a finite vector space over a field $K$ with $\dim V=n$ and $\mathcal{B}$ be a basis of $V$. Then, the function
    $$\function{}{V^*}{\mathcal{M}_{1\times n}(K)}{\omega}{[\omega]_{\mathcal{B},1}}$$
    is an isomorphism. Therefore, $\dim V^*=\dim V$.
  \end{proposition}
  \begin{definition}
    We define the \emph{Kronecker delta} $\delta_{ij}$ as the function: $$\delta_{ij}=
      \begin{cases}
        0 & \text{if }i\ne j \\
        1 & \text{if }i=j
      \end{cases}
    $$
  \end{definition}
  \begin{definition}
    Let $V$ be a finite vector space and $\mathcal{B}=(\vf{v}_1,\ldots,\vf{v}_n)$ be a basis of $V$. We define the \emph{dual basis} $\mathcal{B}^*$ of $\mathcal{B}$ as the basis of $V^*$ formed by $(\eta_1,\ldots,\eta_n)$ where $$\eta_i(\vf{v}_j)=\delta_{ij}$$
  \end{definition}
  \begin{lemma}
    Let $V$ be a vector space over a field $K$, $\mathcal{B}$ be a basis of $V$ and $(\vf{v}_1^*,\ldots,\vf{v}_n^*)$ be the dual basis of $\mathcal{B}$. Then, $\forall \vf{v}\in V$: $$[\vf{v}]_\mathcal{B}=(\vf{v}_1^*(\vf{v}),\ldots,\vf{v}_n^*(\vf{v}))\in K^n$$
  \end{lemma}
  \begin{lemma}
    Let $V$ be a vector space over a field $K$, $\mathcal{B}=(\vf{v}_1,\ldots,\vf{v}_n)$ be a basis of $V$ and $\mathcal{B}^*$ be the dual basis of $\mathcal{B}$. Then, $\forall \omega\in V^*$: $$[\omega]_{\mathcal{B}^*}=(\omega(\vf{v}_1),\ldots,\omega(\vf{v}_n))\in K^n$$
  \end{lemma}
  \begin{definition}[Dual map]
    Let $U$, $V$ be two vector spaces over a field $K$ and $f\in \mathcal{L}(U,V)$. The function $f^*$ defined by
    $$\function{f^*}{U^*}{V^*}{\omega}{\omega\circ f}$$
    is a linear map and it's called \emph{dual map} of $f$.
  \end{definition}
  \begin{theorem}
    Let $U$, $V$ be two finite vector spaces, $\mathcal{B}$ and $\mathcal{B}'$ be bases of $U$ and $V$ respectively and $f\in\mathcal{L}(U,V)$. Then: $$[f^*]_{\mathcal{B}'^*,\mathcal{B}^*}={([f]_{\mathcal{B},\mathcal{B}'})}^\mathrm{T}$$
  \end{theorem}
  \subsubsection{Double dual space}
  \begin{definition}[Double dual space]
    Let $V$ be a vector space over a field $K$. The \emph{double dual space} $V^{**}$ of $V$ is defined as: $$V^{**}:={(V^*)}^*=\mathcal{L}(V^*,K)$$
  \end{definition}
  \begin{proposition}
    Let $V$ be a vector space over a field $K$ and $\vf{v}\in V$. We define the function:
    $$\function{\phi_{\vf{v}}}{V^*}{K}{\omega}{\omega(\vf{v})}$$
    which is linear. This map induces an injective linear map $\Phi$ defined by:
    $$\function{\Phi}{V}{V^{**}}{\vf{v}}{\phi_{\vf{v}}}$$
    Moreover, if $\dim V<\infty$, $\Phi$ is a natural isomorphism\footnote{This means that the definition of $\Phi$ does not depend on a choice of basis.}.
  \end{proposition}
  \subsubsection{Annihilator space}
  \begin{definition}
    Let $V$ be a vector space and $U\subseteq V^*$ be a vector subspace of $V^*$. We define the \emph{annihilator} of $U$ as:
    $$U^0=\{\vf{v}\in V:\omega(\vf{v})=0\;\forall\omega\in U\}$$
  \end{definition}
  \begin{lemma}
    Let $V$ be a vector space and $U\subseteq V^*$ be a vector subspace of $V^*$. If $U=\langle\omega_1,\ldots,\omega_n\rangle$, then $U^0$ is the set of solutions of the system:
    $$\left\{
      \begin{aligned}
        \omega_1(\vf{v}) & =0         \\
                         & \;\;\vdots \\
        \omega_n(\vf{v}) & =0
      \end{aligned}
      \right.
    $$
  \end{lemma}
  \begin{lemma}
    Let $V$ be a vector space and $U\subseteq V^*$ be a vector subspace of $V^*$. Then, $U^0$ is a vector subspace of $V^*$.
  \end{lemma}
  \begin{theorem}
    Let $V$ be a finite vector space and $U\subseteq V^*$ be a vector subspace of $V^*$. Then: $$\dim U^0+\dim U=\dim V$$
  \end{theorem}
  \begin{definition}
    Let $V$ be a vector space and $U\subseteq V$ be a vector subspace of $V$. We define the \emph{annihilator} of $U$ as:
    $$U^0=\{\omega\in V^*:\omega(\vf{v})=0\;\forall\vf{v}\in U\}$$
  \end{definition}
  \begin{lemma}
    Let $V$ be a vector space and $U\subseteq V$  be a vector subspace of $V$. If $U=\langle\vf{v}_1,\ldots,\vf{v}_n\rangle$, then: $$U^0=\{\omega\in V^*:\omega(\vf{v}_1)=\cdots=\omega(\vf{v}_n)=0\}$$
  \end{lemma}
  \begin{proposition}
    Let $V$ be a vector space. Then, whether $U\subseteq V$ or $U\subseteq V^*$, we have: $${(U^0)}^0=U$$
  \end{proposition}
  \subsection{Classification of endomorphisms}
  \begin{definition}
    Let $V$ be a vector space over a field $K$ and $\lambda\in K$. A \emph{homothety} of ratio $\lambda$ is a linear map $f:V\rightarrow V$ such that $f(\vf{v})=\lambda\vf{v}$ $\forall\vf{v}\in V$.
  \end{definition}
  \subsubsection{Similarity}
  \begin{definition}
    Let $V$ be a vector space and $f,g\in\mathcal{L}(V)$. We say that $f$ and $g$ are \emph{similar} if there are basis $\mathcal{B}$ and $\mathcal{B}'$ of $V$ such that $[f]_\mathcal{B}=[g]_{\mathcal{B}'}$.
  \end{definition}
  \begin{lemma}
    Let $V$ be a vector space, $\mathcal{B}$ and $\mathcal{B}'$ basis of $V$ and $f\in\mathcal{L}(V)$. If $\vf{M}=[f]_\mathcal{B}$, $\vf{N}=[f]_{\mathcal{B}'}$ and $\vf{P}=[\id]_{\mathcal{B},\mathcal{B}'}$, then: $$\vf{M}=\vf{P}^{-1}\vf{N}\vf{P}$$
  \end{lemma}
  \begin{definition}
    Let $K$ be a field. Two matrices $\vf{M}, \vf{N}\in\mathcal{M}_n(K)$ are \emph{similar} if there exists a matrix $\vf{P}\in\GL_n(K)$ such that $\vf{M}=\vf{P}^{-1}\vf{N}\vf{P}$.
  \end{definition}
  \begin{proposition}
    Let $V$ be a finite vector space and $f,g\in\mathcal{L}(V)$.
    \begin{enumerate}
      \item $f$ and $g$ are similar if and only if for all basis $\mathcal{B}$ of $V$ the matrices $[f]_\mathcal{B}$ and $[g]_\mathcal{B}$ are similar.
      \item $f$ and $g$ are similar if and only if there is an automorphism $h\in\mathcal{L}(V)$ such that $g=h^{-1}fh$.
    \end{enumerate}
  \end{proposition}
  \subsubsection{Diagonalization}
  \begin{definition}
    Let $K$ be a field. A matrix $\vf{A}=(a_{ij})\in\mathcal{M}_n(K)$ is \emph{diagonal} if $a_{ij}=0$ whenever $i\ne j$. That is, $\vf{A}$ is of the form:
    $$\vf{A}=
      \begin{pmatrix}
        a_{11} & 0      & \cdots & 0      \\
        0      & a_{22} & \ddots & \vdots \\
        \vdots & \ddots & \ddots & 0      \\
        0      & \cdots & 0      & a_{nn}
      \end{pmatrix}
    $$
    In this case, we denote $\vf{A}:=\diag(a_{11},\ldots,a_{nn})$.
  \end{definition}
  \begin{definition}
    Let $K$ be a field. A matrix $\vf{A}\in\mathcal{M}_n(K)$ is \emph{diagonalizable} if it is similar to diagonal matrix.
  \end{definition}
  \begin{definition}
    An endomorphism is \emph{diagonalizable} if its associated matrix in some basis is diagonalizable.
  \end{definition}
  \begin{definition}
    Let $V$ be a vector space over a field $K$ and $f\in\mathcal{L}(V)$. We say that a nonzero vector $\vf{v}\in V$ is an \emph{eigenvector} of $f$ with eigenvalue $\lambda\in K$ if $f(\vf{v})=\lambda \vf{v}$.
  \end{definition}
  \begin{lemma}
    Let $V$ be a vector space over a field $K$, $f\in\mathcal{L}(V)$ and $\lambda\in K$. The eigenvectors of $f$ of eigenvalue $\lambda$ are the nonzero vectors of the subspace $\ker(f-\lambda\id)$, called \emph{eigenspace} corresponding to $\lambda$.
  \end{lemma}
  \begin{lemma}
    Let $V$ be a vector space over a field $K$ with $\dim V=n$, $\mathcal{B}$ be a basis of $V$ and $f\in\mathcal{L}(V)$. Then, $\det([f-x\id]_\mathcal{B})$ is a polynomial on the variable $x$ of degree $n$ and with coefficients in $K$. Moreover, the dominant coefficient is $(-1)^n$ and the constant term is $\det([f]_\mathcal{B})$.
  \end{lemma}
  \begin{corollary}
    Let $V$ be a vector space of dimension $n$ and $f\in\mathcal{L}(V)$. Then, $f$ has at most $n$ distinct eigenvalues.
  \end{corollary}
  \begin{corollary}
    Let $V$ be a vector space over $\CC$ and $f\in\mathcal{L}(V)$. Then, $f$ has at least one eigenvalue.
  \end{corollary}
  \begin{definition}
    Let $K$ be a field and $\vf{A}\in\mathcal{M}_n(K)$. The polynomial $p_{\vf{A}}(\lambda)=\det(\vf{A}-\lambda \vf{I}_n)$ is called \emph{characteristic polynomial} of $\vf{A}$.
  \end{definition}
  \begin{proposition}
    Let $V$ be a vector space and $f\in\mathcal{L}(V)$. For all basis $\mathcal{B}$ of $V$, the characteristic polynomial of $[f]_\mathcal{B}$ is the same. Therefore, we denote it $p_f(\lambda)$ and we refer to it as \emph{characteristic polynomial} of $f$.
  \end{proposition}
  \begin{proposition}
    Let $V$ be a vector space and $f\in\mathcal{L}(V)$. Then, eigenvectors of $f$ of distinct eigenvalues are linearly independent.
  \end{proposition}
  \begin{corollary}
    Let $V$ be a vector space and $f\in\mathcal{L}(V)$. Suppose $\lambda_1,\ldots,\lambda_n$ are the distinct eigenvalues of $f$ and $V_{\lambda_1},\ldots,V_{\lambda_n}$ are their corresponded eigenspaces. Then, $$V_{\lambda_1}+\cdots+V_{\lambda_n}$$ is a direct sum.
  \end{corollary}
  \begin{proposition}
    Let $V$ be a finite vector space of dimension $n$, $f\in\mathcal{L}(V)$ and $\lambda$ be a root of multiplicity $m$ of the characteristic polynomial $p_f(x)$. Then: $$1\leq \dim(\ker(f-\lambda\id))\leq m$$
    The number $m$ is called \emph{algebraic multiplicity} of $\lambda$, whereas the value $\dim(\ker(f-\lambda\id))$ is called \emph{geometric multiplicity} of $\lambda$.
  \end{proposition}
  \begin{theorem}[Diagonalization theorem]
    Let $V$ be a finite vector space and $f\in\mathcal{L}(V)$. $f$ is diagonalizable if and only if:
    \begin{enumerate}
      \item $p_f(x)=(-1)^n(x-\lambda_1)^{m_1}\cdots(x-\lambda_k)^{m_k}$ with distinct $\lambda_1,\ldots,\lambda_k\in K$.
      \item $\dim(\ker(f-\lambda_i \id))=m_i$, $i=1,\ldots,k$.
    \end{enumerate}
  \end{theorem}
  \begin{corollary}
    Let $V$ be a finite vector space with $\dim V=n$ and $f\in\mathcal{L}(V)$. If $f$ has $n$ distinct eigenvalues, $f$ is diagonalizable.
  \end{corollary}
  \begin{proposition}
    Let $V$ be a finite vector space and $f,g\in\mathcal{L}(V)$ such that $f$ and $g$ are similar. Then: $$f\text{ is diagonalizable}\iff g\text{ is diagonalizable}$$
  \end{proposition}
  \begin{lemma}
    Let $K$ be a field and $\vf{A},\vf{B}\in\mathcal{M}_n(K)$ be similar matrices. Then, $\forall k\in\NN$, $\vf{A}^k$ and $\vf{B}^k$ are similar.
  \end{lemma}
  \begin{lemma}
    Let $V$ be a finite vector space over a field $K$ with $\dim V=n$ and $f\in\mathcal{L}(V)$. Then, the function $\phi_f:K[x]\rightarrow\mathcal{L}(V)$ defined by $$\phi_f(a_0+a_1x+\cdots+a_nx^n)=a_0+a_1f+\cdots+a_nf^n$$
    is linear and satisfies: $$\phi_f((pq)(x))=\phi_f(p(x))\phi_f(q(x))\quad\forall p(x),q(x)\in K[x]$$
  \end{lemma}
  \begin{definition}
    Let $V$ be a finite vector space with $\dim V=n$ and $f\in\mathcal{L}(V)$. The \emph{minimal polynomial} $m_f(x)\in K[x]$ of $f$ is the unique a polynomial satisfying:
    \begin{itemize}
      \item $m_f(f)=0$.
      \item $m_f$ is monic.
      \item $m_f$ is of minimum degree.
    \end{itemize}
  \end{definition}
  \begin{proposition}
    Let $V$ be a vector space over a field $K$ and $f\in\mathcal{L}(V)$. If $p(x)\in K[x]$ is such that $p(f)=0$, then $m_f(x)\mid p(x)$.
  \end{proposition}
  \subsubsection{Cayley-Hamilton theorem}
  \begin{theorem}[Cayley-Hamilton theorem]
    Let $K$ be a field, $n\geq 1$ and $\vf{A}\in\mathcal{M}_n(K)$. Then: $$m_{\vf{A}}(x)\mid p_{\vf{A}}(x)\mid m_{\vf{A}}(x)^n$$ Therefore $p_{\vf{A}}(\vf{A})=0$ and $m_{\vf{A}}(x)$ and $p_{\vf{A}}(x)$ have the same irreducible factors.
  \end{theorem}
  \begin{corollary}
    Let $K$ be a field and $\vf{A}\in\GL_n(K)$ be a matrix with $p_{\vf{A}}(x)=a_0+a_1x+\cdots+(-1)^nx^n$. Then: $$\vf{A}^{-1}=-\frac{1}{a_0}\left(\vf{A}^{n-1}+a_{n-1}\vf{A}^{n-2}+\cdots+a_2\vf{A}+a_1\vf{I}_n\right)$$
  \end{corollary}
  \begin{lemma}
    Let $V$ be a finite vector space over a field $K$, $\mathcal{B}$ be a basis of $V$ and $f\in\mathcal{L}(V)$. Then $\forall\lambda,\mu\in K$ and $\forall r,s\in\NN$:
    \begin{enumerate}
      \item $[f^r]_\mathcal{B}={\left([f]_\mathcal{B}\right)}^r$.
      \item $[\lambda f]_\mathcal{B}=\lambda[f]_\mathcal{B}$.
      \item $[\lambda f^r+\mu f^s]_\mathcal{B}=[\lambda f^r]_\mathcal{B}+[\mu f^s]_\mathcal{B}$.
    \end{enumerate}
  \end{lemma}
  \begin{lemma}
    Let $V$ be a finite vector space over a field $K$, $f\in\mathcal{L}(V)$ and $\vf{v}$ be an eigenvector of $f$ of eigenvalue $\lambda$. Then, $\forall p(x)\in K[x]$ we have: $$p(f)(\vf{v})=p(\lambda)\vf{v}$$
  \end{lemma}
  \begin{theorem}[Cayley-Hamilton theorem]
    Let $V$ be a finite vector space over a field $K$ such that $\dim V=n$ and $f\in\mathcal{L}(V)$. Then: $$m_f(x)\mid p_f(x)\mid m_f(x)^n$$
  \end{theorem}
  \begin{definition}
    A field $K$ satisfying that all polynomial with coefficient in $K$ of degree greater o equal to 1 factorizes as a product of linear factors is called an \emph{algebraically closed field}.
  \end{definition}
  \begin{definition}
    Let $V$ be a vector space and $f\in\mathcal{L}(V)$. We say that $U\subseteq V$ is an \emph{invariant subspace} of $V$ under $f$ if $f(U)\subseteq U$.
  \end{definition}
  \begin{lemma}
    Let $V$ be a vector space and $f\in\mathcal{L}(V)$.
    \begin{enumerate}
      \item If $U\subseteq V$ is an invariant subspace of $V$ under $f$, then: $$p_{f|_U}(x)\mid p_f(x)\footnote{Here $f|_U$ is the function $f$ restricted to the subspace $U$.}$$
      \item If $U_1$ and $U_2$ are invariant subspaces of $V$ under $f$ such that $V=U_1\oplus U_2$, then:
            \begin{itemize}
              \item $p_f(x)=p_{f|_{U_1}}(x)\cdot p_{f|_{U_2}}(x)$.
              \item $m_f(x)=\lcm(m_{f|_{U_1}}(x),m_{f|_{U_2}}(x))$.
            \end{itemize}
    \end{enumerate}
  \end{lemma}
  \begin{lemma}
    Let $V$ be a vector space, $f\in\mathcal{L}(V)$ and $a(x),b(x)\in K[x]$. Suppose $m(x)=\lcm(a(x),b(x))$ and $d(x)=\gcd(a(x),b(x))$. Then:
    \begin{enumerate}
      \item $\ker(a(f))+\ker(b(f))=\ker(m(f))$.
      \item $\ker(a(f))\cap\ker(b(f))=\ker(d(f))$.
    \end{enumerate}
    In particular, if $a(x)$ and $b(x)$ are coprime and $a(f)b(f)=0$, then: $$V=\ker(a(x))\oplus\ker(b(x))$$
  \end{lemma}
  \begin{theorem}
    Let $V$ be a finite vector space such that $\dim V=n$ and $f\in\mathcal{L}(V)$. If $p_f(x)={q_1(x)}^{n_1}\cdots q_r(x)^{n_r}$ and $m_f(x)={q_1(x)}^{m_1}\cdots {q_r(x)}^{m_r}$ with $q_i(x)$ distinct irreducible factors, then: $$V=\ker({q_1(f)}^{m_1})\oplus\cdots\oplus\ker({q_r(f)}^{m_r})$$ Moreover, $\dim\left(\ker({q_i(f)}^{m_i})\right)=n_i\deg(q_i(x))$.
  \end{theorem}
  \subsubsection{Jordan form}
  \begin{definition}
    Let $K$ be a field and $\vf{A}\in\mathcal{M}_n(K)$. A \emph{Jordan block} of $\vf{A}$ is a square submatrix composed by a value $\lambda\in K$ on the principal diagonal, ones on the diagonal just below the principal diagonal and zeros elsewhere. That is, a Jordan block is a matrix of the form:
    $$
      \begin{pmatrix}
        \lambda & 0       & 0       & \cdots & 0       \\
        1       & \lambda & 0       & \ddots & \vdots  \\
        0       & 1       & \lambda & \ddots & 0       \\
        \vdots  & \ddots  & \ddots  & \ddots & 0       \\
        0       & \cdots  & 0       & 1      & \lambda
      \end{pmatrix}
    $$
    A \emph{Jordan matrix} is a block diagonal matrix whose blocks are Jordan blocks.
  \end{definition}
  \begin{proposition}\label{LA_jordan}
    Let $V$ be a finite vector space over a field $K$ with $\dim V=n$ and $f\in\mathcal{L}(V)$. If $p_f(x)=\pm(x-\lambda_1)^{n_1}\cdots(x-\lambda_k)^{n_k}$, there exists a basis $\mathcal{B}$ of $V$ such that
    $$[f]_{\mathcal{B}}=
      \begin{pmatrix}
        \vf{J}_1 & \vf{0}   & \cdots & \vf{0}   \\
        \vf{0}   & \vf{J}_2 & \ddots & \vdots   \\
        \vdots   & \ddots   & \ddots & \vf{0}   \\
        \vf{0}   & \cdots   & \vf{0} & \vf{J}_r \\
      \end{pmatrix}
    $$
    where $\vf{J}_1,\ldots,\vf{J}_r$ are Jordan blocks associated with eigenvalues $\lambda_1,\ldots,\lambda_k$ satisfying:
    \begin{enumerate}
      \item\label{LA_diag1} For $i=1,\ldots,k$, the sum of the sizes of Jordan blocks associated with the eigenvalue $\lambda_i$ is $n_i$.
      \item\label{LA_diag2} The sizes of Jordan blocks are determined by $\dim(\ker((f-\lambda_i\id)^r))$, $r=1,\ldots,n_i-1$.
    \end{enumerate}
  \end{proposition}
  \begin{proposition}
    Let $V$ be a finite vector space over a field $K$ with $\dim V=n$ and $\vf{A}\in\mathcal{M}_n(K)$. If $p_{\vf{A}}(x)=\pm(x-\lambda_1)^{n_1}\cdots(x-\lambda_k)^{n_k}$, there exist a matrix $\vf{P}\in\GL_n(K)$ such that:
    $$\vf{J}:=\vf{P}^{-1}\vf{A}\vf{P}=
      \begin{pmatrix}
        \vf{J}_1 & \vf{0}   & \cdots & \vf{0}   \\
        \vf{0}   & \vf{J}_2 & \ddots & \vdots   \\
        \vdots   & \ddots   & \ddots & \vf{0}   \\
        \vf{0}   & \cdots   & \vf{0} & \vf{J}_r \\
      \end{pmatrix}
    $$
    where $\vf{J}_1,\ldots,\vf{J}_r$ are Jordan blocks associated with eigenvalues $\lambda_1,\ldots,\lambda_k$ satisfying \cref{LA_diag1,LA_diag2} of \cref{LA_jordan}. In that case, we say that $\vf{J}$ is the \emph{Jordan form} of $\vf{A}$.
  \end{proposition}
  \begin{theorem}
    Let $V$ be a vector space and $f,g\in\mathcal{L}(V)$ be such that $p_f(x)=(x-\lambda_1)^{n_1}\cdots(x-\lambda_k)^{n_k}$. If $g$ satisfies:
    \begin{enumerate}
      \item $p_f(x)=p_g(x)$
      \item $m_f(x)=m_g(x)$
      \item $\dim(\ker((f-\lambda \id)^r))=\dim(\ker((g-\lambda \id)^r))$ $\forall\lambda\in K$ $\forall r\geq 1$
    \end{enumerate}
    then $f$ is similar to $g$.
  \end{theorem}
  \subsection{Symmetric bilinear forms}
  \subsubsection{Basic definitions}
  \begin{definition}
    Let $U$, $V$, $W$ be three vector spaces over a field $K$. We say that a function $\varphi:U\times V\rightarrow W$ is \emph{bilinear} if $\forall\vf{u}_1,\vf{u}_2,\vf{u}\in U$, $\forall \vf{v}_1,\vf{v}_2,\vf{v}\in V$ and $\forall\lambda\in K$ we have:
    \begin{enumerate}
      \item $\varphi(\vf{u}_1+\vf{u}_2,\vf{v})=\varphi(\vf{u}_1,\vf{v})+\varphi(\vf{u}_2,\vf{v})$.
      \item $\varphi(\lambda \vf{u},\vf{v})=\lambda \varphi(\vf{u},\vf{v})$.
      \item $\varphi(\vf{u},\vf{v}_1+\vf{v}_2)=\varphi(\vf{u},\vf{v}_1)+\varphi(\vf{u},\vf{v}_2)$.
      \item $\varphi(\vf{u},\lambda \vf{v})=\lambda \varphi(\vf{u},\vf{v})$.
    \end{enumerate}
  \end{definition}
  \begin{definition}
    Let $V$ be a vector space over a field $K$. A \emph{bilinear form} from $V$ onto $K$ is a bilinear map $\varphi:V\times V\rightarrow K$.
  \end{definition}
  \begin{definition}
    Let $V$ be a vector space over a field $K$. A bilinear form $\varphi:V\times V\rightarrow K$ is \emph{symmetric} if $$\varphi(\vf{v}_1,\vf{v}_2)=\varphi(\vf{v}_2,\vf{v}_1)\quad\forall \vf{v}_1,\vf{v}_2\in V$$
  \end{definition}
  \subsubsection{Matrix associated with a bilinear form}
  \begin{definition}
    Let $V$ be a finite vector space over a field $K$, $\mathcal{B}=(\vf{v}_1,\ldots,\vf{v}_n)$ be a basis of $V$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. We define the \emph{matrix of the bilinear form} $\varphi$ with respect to the basis $\mathcal{B}$ as the matrix $[\varphi]_\mathcal{B}\in\mathcal{M}_n(K)$ defined as: $$[\varphi]_\mathcal{B}=
      \begin{pmatrix}
        \varphi(\vf{v}_1,\vf{v}_1) & \varphi(\vf{v}_1,\vf{v}_2) & \cdots & \varphi(\vf{v}_1,\vf{v}_n) \\
        \varphi(\vf{v}_2,\vf{v}_1) & \varphi(\vf{v}_2,\vf{v}_2) & \cdots & \varphi(\vf{v}_2,\vf{v}_n) \\
        \vdots                     & \vdots                     & \ddots & \vdots                     \\
        \varphi(\vf{v}_n,\vf{v}_1) & \varphi(\vf{v}_n,\vf{v}_2) & \cdots & \varphi(\vf{v}_n,\vf{v}_n) \\
      \end{pmatrix}$$
  \end{definition}
  \begin{lemma}
    Let $V$ be a finite vector space over a field $K$, $\mathcal{B}$ be a basis of $V$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. Then:
    $$\varphi(\vf{v}_1,\vf{v}_2)={\left([\vf{v}_1]_\mathcal{B}\right)}^\mathrm{T}[\varphi]_\mathcal{B}[\vf{v}_2]_\mathcal{B}\quad\forall\vf{v}_1,\vf{v}_2\in V$$
  \end{lemma}
  \begin{proposition}
    Let $V$ be a finite vector space over a field $K$, $\mathcal{B}$ be a basis of $V$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. Then: $$\varphi\text{ is symmetric}\iff[\varphi]_\mathcal{B}\text{ is symmetric}$$
  \end{proposition}
  \begin{proposition}
    Let $V$ be a finite vector space over a field $K$, $\mathcal{B}$ and $\mathcal{B}'$ be bases of $V$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. Then: $$[\varphi]_{\mathcal{B}'}={([\id]_{\mathcal{B}',\mathcal{B}})}^\mathrm{T}[\varphi]_\mathcal{B}[\id]_{\mathcal{B}',\mathcal{B}}$$
  \end{proposition}
  \subsubsection{Orthogonal basis}
  \begin{definition}\label{LA_isotrop}
    Let $V$ be a finite vector space over a field $K$, $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form and $\vf{v}_1,\vf{v}_2\in V$.
    \begin{itemize}
      \item We say that $\vf{v}_1$ and $\vf{v}_2$ are \emph{orthogonal} if $\varphi(\vf{v}_1,\vf{v}_2)=0$.
      \item If $\vf{v}_1\ne 0$, we say that $\vf{v}_1$ is \emph{isotropic} if $\varphi(\vf{v}_1,\vf{v}_1)=0$.
    \end{itemize}
  \end{definition}
  \begin{definition}
    Let $V$ be a finite vector space over a field $K$, $\mathcal{B}=(\vf{v}_1,\ldots,\vf{v}_n)$ be a basis of $V$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form.
    \begin{itemize}
      \item We say that $\mathcal{B}$ is \emph{orthogonal} with respect to $\varphi$ if $\varphi(\vf{v}_i,\vf{v}_j)=0$ $\forall i\ne j$.
      \item We say that $\mathcal{B}$ is \emph{orthonormal} with respect to $\varphi$ if $\varphi(\vf{v}_i,\vf{v}_j)=\delta_{ij}$.
    \end{itemize}
  \end{definition}
  \begin{theorem}
    Let $V$ be a finite vector space over a field $K$, $\mathcal{B}$ be a basis of $V$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. Then, $V$ has an orthogonal basis with respect to $\varphi$ and an orthonormal basis with respect to $\varphi$.
  \end{theorem}
  \begin{corollary}
    Let $K$ be a field with $\ch K\ne 2$ and $\vf{A}\in\mathcal{M}_n(K)$ be a symmetric matrix. Then, there exists a matrix $\vf{P}\in\GL_n(K)$ such that $\transpose{\vf{P}}\vf{A}\vf{P}$ is diagonal.
  \end{corollary}
  \subsubsection{Orthogonal decompositions}
  \begin{definition}\label{LA_singular}
    Let $V$ be a finite vector space over a field $K$, $U\subseteq V$ be a vector subspace of $V$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. We define the \emph{orthogonal complement} of $U$ as: $$U^\perp=\{\vf{v}\in V:\varphi(\vf{v},\vf{u})=0\;\forall\vf{u}\in U\}$$
  \end{definition}
  \begin{definition}
    Let $V$ be a finite vector space over a field $K$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. We define the \emph{radical} of $\varphi$ as: $$\rad\varphi=V^\perp$$ We say that $\varphi$ is \emph{nonsingular} if $\rad\varphi=\{0\}$.
  \end{definition}
  \begin{definition}
    Let $V$ be a finite vector space over a field $K$, $\varphi:V\times V\rightarrow K$ be a nonsingular symmetric bilinear form and $\vf{v}_0\in V$. We define $\varphi_{\vf{v}_0}:V\rightarrow K$, $\varphi_{\vf{v}_0}(\vf{v})=\varphi(\vf{v}_0,\vf{v})$. Then, the function
    $$\function{}{V}{V^*}{\vf{v}_0}{\varphi_{\vf{v}_0}}$$ is an isomorphism.
  \end{definition}
  \begin{definition}
    Let $V$ be a finite vector space over a field $K$, $U\subseteq V$ be a vector subspace of $V$ and $\varphi:V\times V\rightarrow K$ be a nonsingular symmetric bilinear form. Then:
    \begin{enumerate}
      \item $\dim V=\dim U+\dim U^\perp$.
      \item ${(U^\perp)}^\perp=U$.
      \item If $\varphi|_U$ is nonsingular, then $V=U\oplus U^\perp$.
    \end{enumerate}
  \end{definition}
  \begin{definition}
    Let $V$ be a finite vector space over a field $K$, $U_1,U_2\subseteq V$ be vector subspaces of $V$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. We say that the sum $U_1+U_2$ is \emph{orthogonal} if it is direct and $\varphi(\vf{u}_1,\vf{u}_2)=0$ $\forall \vf{u}_1\in U_1$ and $\vf{u}_2\in U_2$. In this case, we denote $U_1+U_2$ by $U_1\perp U_2$.
  \end{definition}
  \begin{proposition}
    Let $V$ be a finite vector space over a field $K$, $U_1,U_2\subseteq V$ be vector subspaces of $V$ such that $V=U_1\perp U_2$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. Then, $\forall \vf{v}\in V$ there exist unique $\vf{u}_1\in U_1$ and $\vf{u}_2\in U_2$ such that $\vf{v}=\vf{u}_1+\vf{u}_2$.
  \end{proposition}
  \begin{definition}\label{LA_perpendicular}
    Let $V$ be a finite vector space over a field $K$, $U_1,U_2\subseteq V$ be vector subspaces of $V$ such that $V=U_1\perp U_2$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. The function
    $$\function{\pi}{U_1\perp U_2}{U_i}{\vf{v}=\vf{u}_1+\vf{u}_2}{\vf{u}_i}$$
    for $i=1,2$ is called \emph{orthogonal projection} of $V$ onto $U_i$ according to the decomposition $V=U_1\perp U_2$.
  \end{definition}
  \begin{method}[Gram-Schmidt process]
    Let $V$ be a finite vector space over a field $K$, $\mathcal{B}=(\vf{v}_1,\ldots,\vf{v}_n)$ be a basis of $V$ and $\varphi:V\times V\rightarrow K$ be a symmetric bilinear form. $\forall\vf{u},\vf{v}\in V$, we define $$\text{proj}_{\vf{u}}(\vf{v})=\frac{\varphi(\vf{u},\vf{v})}{\varphi(\vf{u},\vf{u})}\vf{u}$$ We will create an orthogonal basis $(\vf{u}_1,\ldots,\vf{u}_n)$ of $V$ from $\mathcal{B}$. We define $\vf{u}_i$, $i=1,\ldots,n$, to be:
    \begin{align*}
      \vf{u}_1 & =\vf{v}_1                                                                   \\
      \vf{u}_2 & =\vf{v}_2-\text{proj}_{\vf{u}_1}(\vf{v}_2)                                  \\
      \vf{u}_3 & =\vf{v}_3-\text{proj}_{\vf{u}_1}(\vf{v}_3)-\text{proj}_{\vf{u}_2}(\vf{v}_3) \\
               & \;\;\vdots                                                                  \\
      \vf{u}_n & =\vf{v}_n-\sum_{i=1}^{n-1}\text{proj}_{\vf{u}_i}(\vf{v}_n)
    \end{align*}
    To obtain an orthogonal basis $(\vf{e}_1,\ldots,\vf{e}_n)$ of $V$ from $\mathcal{B}$, define $\vf{e}_i$, $i=1,\ldots,n$, to be: $$\vf{e}_i=\frac{\vf{u}_i}{\sqrt{\varphi(\vf{u}_i,\vf{u}_i)}}$$
  \end{method}
  \subsubsection{Sylvester's law of inertia}
  \begin{definition}
    An \emph{orthogonal geometry} over a field $K$ is a pair $(V,\varphi)$, where $V$ is a vector space over $K$ and $\varphi$ is a symmetric bilinear form over $V$.
  \end{definition}
  \begin{definition}\label{LA_isometry}
    Let $(V_1,\varphi_1)$, $(V_2,\varphi_2)$ be two orthogonal geometries over a field $K$. An \emph{isometry} from $(V_1,\varphi_1)$ to $(V_2,\varphi_2)$ is an isomorphism $f:V_1\rightarrow V_2$ such that $$\varphi_2(f(\vf{u}),f(\vf{v}))=\varphi_1(\vf{u},\vf{v})\quad\forall\vf{u},\vf{v}\in V_1$$ We say that $(V_1,\varphi_1)$ and $(V_2,\varphi_2)$ are \emph{isometric} if there exists an isometry between them.
  \end{definition}
  \begin{definition}
    Let $V$ be a vector space over a field $K$ and $\varphi_1$, $\varphi_2$ be symmetric bilinear forms. We say that $\varphi_1$ and $\varphi_2$ are \emph{equivalent} if and only if $(V,\varphi_1)$ and $(V,\varphi_2)$ are isometric.
  \end{definition}
  \begin{definition}
    Let $\vf{A},\vf{B}\in\mathcal{M}_n(\RR)$. We say that $\vf{A}$ and $\mathcal{B}$ are congruent if there exists a matrix $\vf{P}\in\GL_n(\RR)$ such that $$\vf{A}=\transpose{\vf{P}}\vf{B}\vf{P}$$
  \end{definition}
  \begin{proposition}
    Let $V$ be a finite vector space over a field $K$, $\mathcal{B}_1$ be a basis of $V$ and $\varphi_1$, $\varphi_2$ be symmetric bilinear forms. Then the following statements are equivalent:
    \begin{enumerate}
      \item The orthogonal geometries $(V,\varphi_1)$ and $(V,\varphi_2)$ are isometric.
      \item There exists a basis $\mathcal{B}_2$ of $V$ such that $[\varphi_1]_{\mathcal{B}_1}=[\varphi_2]_{\mathcal{B}_2}$.
      \item The matrices $[\varphi_1]_{\mathcal{B}_1}$ and $[\varphi_2]_{\mathcal{B}_2}$ are congruent.
    \end{enumerate}
  \end{proposition}
  \begin{theorem}[Sylvester's law of inertia]
    Let $V$ be a finite vector space over $\RR$ and $\varphi$ be a symmetric bilinear form over $V$. Then, there exists a basis $\mathcal{B}$ of $V$ such that:
    $$[\varphi]_\mathcal{B}=\diag\left(0,\overset{(r_0)}{\ldots},0,1,\overset{(r_+)}{\ldots},1,-1,\overset{(r_-)}{\ldots},-1\right)$$
    where in the diagonal there are $r_0$ zeros, $r_+$ ones and $r_-$ minus ones and the triplet $(r_0,r_+,r_-)$ doesn't depend on the basis $\mathcal{B}$.
  \end{theorem}
  \begin{definition}
    Let $V$ be a finite vector space over $\RR$ and $\varphi$ be a symmetric bilinear form over $V$. Let $\mathcal{B}$ be an orthogonal basis of $V$ with respect to $\varphi$. We define the \emph{rank} of $\varphi$ as: $$\rank \varphi=\rank ([\varphi]_\mathcal{B})$$ We define the \emph{signature} of $\varphi$ as: $$\sig\varphi=(r_+,r_-)$$ where $r_+$ is el number of positive real numbers on the diagonal of $[\varphi]_\mathcal{B}$ and $r_-$ is el number of negative real numbers on the diagonal of $[\varphi]_\mathcal{B}$.
  \end{definition}
  \begin{theorem}
    Let $(V_1,\varphi_1)$, $(V_2,\varphi_2)$ be two orthogonal geometries over $\RR$ of finite dimension. Then, $(V_1,\varphi_1)$ and $(V_2,\varphi_2)$ are isometric if and only if $\dim V_1=\dim V_2$ and $\sig \varphi_1=\sig \varphi_2$.
  \end{theorem}
  \subsubsection{Inner products}
  \begin{definition}
    Let $V$ be a finite vector space over $\RR$ and $\varphi$ be a symmetric bilinear form over $V$. We say that $\varphi$ is \emph{positive-definite} if $$\varphi(\vf{v},\vf{v})>0\quad\forall\vf{v}\in V\setminus\{0\}$$ We say that $\varphi$ is \emph{negative-definite} if $$\varphi(\vf{v},\vf{v})<0\quad\forall\vf{v}\in V\setminus\{0\}\footnote{The terms \emph{positive-semidefinite} and \emph{negative-semidefinite} are used when $\forall\vf{v}\in V\setminus\{0\}$, $\varphi(\vf{v},\vf{v})\geq 0$ or $\varphi(\vf{v},\vf{v})\leq 0$, respectively.}$$
  \end{definition}
  \begin{definition}\label{LA_inner}
    Let $V$ be a vector space over $\RR$. An \emph{inner product} over $V$ is a positive-definite symmetric bilinear form over $V$.
  \end{definition}
  \begin{definition}\label{LA_espai-euclidia}
    An \emph{Euclidean vector space} is a pair $(V,\varphi)$, where $V$ is a vector space over $\RR$ and $\varphi$ is an inner product over $V$.
  \end{definition}
  \begin{theorem}[Cauchy-Schwartz inequality]
    Let $(V,\varphi)$ be an Euclidean vector space. Then: $$\varphi(\vf{v}_1,\vf{v}_2)^2\leq \varphi(\vf{v}_1,\vf{v}_1)\varphi(\vf{v}_2,\vf{v}_2)\quad\forall \vf{v}_1,\vf{v}_2\in V$$
  \end{theorem}
  \begin{definition}
    Let $V$ be a vector space over $\RR$. A \emph{norm} on $V$ is a function
    $$\function{\|\cdot\|}{V}{\RR}{\vf{v}}{\|\vf{v}\|}$$
    such that:
    \begin{enumerate}
      \item $\|\vf{v}\|=0\iff \vf{v}=\vf{0}$ $\forall \vf{v}\in V$.
      \item $\|\lambda \vf{v}\|=|\lambda|\|\vf{v}\|$, $\forall \vf{v}\in V$, $\lambda\in\RR$.
      \item $\|\vf{v}_1+\vf{v}_2\|\leq\|\vf{v}_1\|+\|\vf{v}_2\|$, $\forall \vf{v}_1,\vf{v}_2\in V$\footnote{Note that $\forall\vf{v}\in V$ we have: $0=\|\vf{v}+(-\vf{v})\|\leq\|\vf{v}\|+\|-\vf{v}\|=2\|\vf{v}\|\implies\|\vf{v}\|\geq 0$.}.
    \end{enumerate}
  \end{definition}
  \begin{proposition}
    Let $(V,\varphi)$ be an Euclidean vector space. Then, the function
    $$\function{\|\cdot\|_\varphi}{V}{\RR}{\vf{v}}{\sqrt{\varphi(\vf{v},\vf{v})}}$$
    is a norm called \emph{norm associated with the inner product} $\varphi$.
  \end{proposition}
  \begin{definition}
    Let $(V,\varphi)$ be an Euclidean vector space and $\vf{v}_1,\vf{v}_2\in V\setminus\{0\}$. We define the \emph{angle} with respect to $\varphi$ between $\vf{v}_1$ and $\vf{v}_2$ as the unique $\theta\in[0,\pi]$ such that: $$\cos{\theta}=\frac{\varphi(\vf{v}_1,\vf{v}_2)}{\|\vf{v}_1\|_\varphi\|\vf{v}_2\|_\varphi}$$
  \end{definition}
  \subsubsection{Spectral theorem}
  \begin{definition}
    Let $(V,\varphi)$ be a finite Euclidean vector space and $f\in\mathcal{L}(V)$. Then, there exists a unique $f'\in\mathcal{L}(V)$ such that $$\varphi(f(\vf{v}_1),\vf{v}_2)=\varphi(\vf{v}_1,f'(\vf{v}_2))\quad\forall \vf{v}_1,\vf{v}_2\in V$$ This $f'$ is called \emph{adjoint} of $f$.
  \end{definition}
  \begin{definition}
    Let $(V,\varphi)$ be a finite Euclidean vector space and $f\in\mathcal{L}(V)$. $f$ is called \emph{auto-adjoint} if $f=f'$.
  \end{definition}
  \begin{lemma}
    Let $(V,\varphi)$ be a finite Euclidean vector space of dimension $n$ and $f\in\mathcal{L}(V)$ be auto-adjoint. Then, there exist $\lambda_1,\ldots,\lambda_n\in\RR$ such that $$p_f(x)=(x-\lambda_1 )\cdots(x-\lambda_n)$$
  \end{lemma}
  \begin{definition}
    Let $K$ be a field and $A\in\GL_n(K)$ be a matrix. We say that $A$ is \emph{orthogonal} if and only if $$\vf{P}\transpose{\vf{P}}=\transpose{\vf{P}}\vf{P}=\vf{I}_n$$ The set of orthogonal matrices of size $n$ over $K$ is denoted by $\mathcal{O}_n(K)$.
  \end{definition}
  \begin{theorem}[Spectral theorem]
    Let $(V,\varphi)$ be a finite Euclidean vector space and $f\in\mathcal{L}(V)$ be auto-adjoint. Then, $V$ has an orthonormal basis of eigenvectors of $f$. In particular, $f$ diagonalizes.
  \end{theorem}
  \begin{corollary}
    Let $K$ be a field. All symmetric matrices $A\in\mathcal{M}_n(K)$ are diagonalizable. More precisely, there exists $\vf{P}\in\mathcal{O}_n(K)$ such that $\transpose{\vf{P}}\vf{A}\vf{P}$ is diagonal.
  \end{corollary}
  \begin{definition}
    Let $A=(a_{ij})\in\mathcal{M}_{m\times n}(\CC)$. We define the \emph{complex conjugate} $\overline{\vf{A}}$ of $\vf{A}$ as $\overline{\vf{A}}=(\overline{a_{ij}})$.
  \end{definition}
  \begin{proposition}
    Let $\vf{A},\vf{B}\in\mathcal{M}_{m\times n}(\CC)$, $\vf{C}\in\mathcal{M}_{n\times p}(\CC)$ and $\lambda\in\CC$. Then:
    \begin{enumerate}
      \item $\overline{\vf{A}+\vf{B}}=\overline{\vf{A}}+\overline{\vf{B}}$.
      \item $\overline{\vf{A}\vf{C}}=\overline{\vf{A}}\cdot\overline{\vf{C}}$.
      \item $\overline{\lambda\cdot\vf{A}}=\overline{\lambda}\cdot\overline{\vf{A}}$.
    \end{enumerate}
  \end{proposition}
  \begin{corollary}
    Let $\vf{A}\in\mathcal{M}_n(\RR)$ be a symmetric matrix. Then, there exist $\lambda_1,\ldots,\lambda_n\in\RR$ such that $$p_{\vf{A}}(x)=(x-\lambda_1 )\cdots(x-\lambda_n)$$
  \end{corollary}
  \begin{theorem}[Descartes' rule of signs]
    Let $P(x)=a_0+\cdots+a_nx^n\in\RR[x]$:
    \begin{enumerate}
      \item The number of positive roots of $P(x)$ is at most equal to the number of sign variations in the sequence $[a_d,a_{d-1},\ldots,a_1,a_0]$.
      \item If $P(x)=a_n(x-\alpha_1)^{n_1}\cdots(x-\alpha_r)^{n_r}$, then the number of positive roots of $P(x)$ is equal to the number of sign variations in the sequence (having in account multiplicity).
    \end{enumerate}
  \end{theorem}
\end{multicols}
\end{document}