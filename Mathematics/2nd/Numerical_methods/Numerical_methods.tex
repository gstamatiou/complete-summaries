\documentclass[class=article,10pt,crop=false]{standalone}
\usepackage{standalone}
\usepackage{preamble}

\begin{document}
\begin{multicols}{2}[\section{Numerical methods}]
\subsection{Errors}
\subsubsection*{Floating-point representation}
\begin{theorem}
Let $b\in\mathbb{N}$, $b\geq 2$. Any real number $x\in\mathbb{R}$ can be represented of the form 
\begin{equation*}
    x=s\left(\sum_{i=1}^\infty\alpha_ib^{-i}\right)b^q,
\end{equation*} where $s\in\{-1,1\}$, $q\in\mathbb{Z}$ and $\alpha_i\in\{0,1,\ldots,b-1\}$. Moreover, this representation is unique if $\alpha_1\ne0$ and $\forall i_0\in\mathbb{N}$, $\exists i\geq i_0:\alpha_i\ne b-1$. We will write $$x=s(0.\alpha_1\alpha_2\cdots)_bb^q,$$ where the subscript $b$ in the parenthesis indicates that the digits $\alpha_1\alpha_2\alpha_3\cdots$ are in base $b$.
\end{theorem}
\begin{definition}[Floating-point representation]
Let $x$ be a real number. Then the floating-point representation of $x$ is $$x=s\left(\sum_{i=1}^t\alpha_ib^{-i}\right)b^q.$$ Here $s$ is called the \textit{sign}; $\sum_{i=1}^t\alpha_ib^{-i}$, the \textit{significant} or \textit{mantissa}, and $q$, the \textit{exponent}, limited to a prefixed range $q_\text{min}\leq q\leq q_\text{max}$. So, the floating-point representation of $x$ is $$x=smb^q=s(0.\alpha_1\alpha_2\cdots\alpha_t)_bb^q.$$ Finally we say a floating-point number is \textit{normalized} if $\alpha_1\ne0$.
\end{definition}
\begin{table}[ht]
    \centering
    \begin{tabular}{c|ccccc}
        Format & $b$ & $t$ & $q_\text{min}$ & $q_\text{max}$ & bits \\
        \hline\hline
        IEEE simple & 2 & 24 & -126 & 127 & 32\\
        IEEE double & 2 & 53 & -1022 & 1023 & 64
    \end{tabular}
    \caption{Parameters of IEEE simple and IEEE double formats.}
    \label{tab:my_label}
\end{table}
\begin{definition}
We denote by $F(b,t,q_\text{min},q_\text{max})$ the following set:
\begin{multline*}
    F(b,t,q_\text{min},q_\text{max})=\{0\}\cup\{\pm(0.\alpha_1\alpha_2\cdots\alpha_t)_bb^q:\\\alpha_i\in\{0,1,\ldots,b-1\},\alpha_1\ne0, q_\text{min}\leq q\leq q_\text{max}\}.
\end{multline*}
\end{definition}
\begin{definition}
Let $x\in\mathbb{R}$ such that $x=s(0.\alpha_1\alpha_2\cdots)_bb^q$ with $q_\text{min}\leq q\leq q_\text{max}$. We say the \textit{floating-point representation by truncation of $x$} is $$fl_T(x)=s(0.\alpha_1\alpha_2\cdots\alpha_t)_bb^q.$$ We say the \textit{floating-point representation by rounding of $x$} is \begin{multline*}
fl_R(x)=\\=\left\{\begin{array}{lcl}
        s(0.\alpha_1\cdots\alpha_t)_bb^q & \text{if} & 0\leq\alpha_{t+1}<b/2,\\
        s(0.\alpha_1\cdots\alpha_{t-1}(\alpha_t+1))_bb^q & \text{if} & b/2\leq\alpha_{t+1}\leq b-1.
    \end{array}\right.
\end{multline*}
\end{definition}
\begin{definition}
Given a value $x\in\mathbb{R}$ and its approximation $\Tilde{x}$, the \textit{absolute error} is $$\Delta x:=|x-\Tilde{x}|.$$ If $x\ne 0$, the \textit{relative error} is $$\delta x:=\frac{|x-\Tilde{x}|}{x}.$$ If $x$ is unknown, we will take $$\delta x\approx\frac{|x-\Tilde{x}|}{\Tilde{x}}.$$
\end{definition}
\begin{definition}
Let $\Tilde{x}$ be an approximation of $x$. If $\Delta x\leq\frac{1}{2}10^t$, we say \textit{$\Tilde{x}$ has $t$ correct decimal digits}. If $x=sm10^q$ with $0.1\leq m<1$, $\Tilde{x}=s\Tilde{m}10^q$ and $$t_r:=\max\{i\in\mathbb{Z}:|m-\Tilde{m}|\leq\frac{1}{2}10^{-i}\},$$ then we say that $\Tilde{x}$ \textit{has $t_r$ significant digits}.
\end{definition}
Let $x\in\mathbb{R}$ such that $x=s(0.\alpha_1\alpha_2\cdots)_bb^q$ with $\alpha_1\ne0$ and $q_\text{min}\leq q\leq q_\text{max}$. Then, we have:
\begin{align*}
    \left|fl_T(x)-x\right|\leq b^{q-t},\quad&\quad \left|fl_R(x)-x\right|\leq\frac{1}{2}b^{q-t}.\\
    \left|\frac{fl_T(x)-x}{x}\right|\leq b^{1-t},\quad&\quad \left|\frac{fl_R(x)-x}{x}\right|\leq\frac{1}{2}b^{1-t}.\\
\end{align*}
\begin{definition}
The \textit{machine epsilon $\epsilon$} is defined as $$\epsilon:=\min\{\varepsilon>0:fl(1+\varepsilon)\ne 1\}.$$
\end{definition}
\begin{prop}
For a machine working by truncation, $\epsilon=b^{1-t}$. For a machine working by rounding, $\epsilon=\frac{1}{2}b^{1-t}$.
\end{prop}
\subsubsection*{Propagation of errors}
\begin{prop}[Propagation of absolute errors]
Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be a function of class $\mathcal{C}^2$. If $\Delta x_j$ is the absolute error of the variable $x_j$ and $\Delta f(x)$ is the absolute error of the function $f$ evaluated at the point $x=(x_1,\ldots,x_n)$, we have $$|\Delta f(x)|\lesssim\sum_{j=1}^n\left|\frac{\partial f}{\partial x_j}(x)\right||\Delta x_j|.\footnote{The symbol $\lesssim$ means that we are omitting terms of order $\Delta x_j\Delta x_k$ and higher.}$$ The coefficients $\displaystyle\frac{\partial f}{\partial x_j}(x)$ are called \textit{absolute condition numbers of the problem}. 
\end{prop}
\begin{prop}[Propagation of relative errors]
Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be a function of class $\mathcal{C}^2$. If $\delta x_j$ is the relative error of the variable $x_j$ and $\delta f(x)$ is the relative error of the function $f$ evaluated at the point $x=(x_1,\ldots,x_n)$, we have $$|\delta f(x)|\lesssim\sum_{j=1}^n\frac{\left|\frac{\partial f}{\partial x_j}(x)\right|\left|x\right|}{\left|f(x)\right|}|\delta x_j|.$$ The coefficients $\displaystyle \left|\frac{\partial f}{\partial x_j}(x)\right|\left|x\right|/\left|f(x)\right|$ are called \textit{relative condition numbers of the problem}. 
\end{prop}
\subsubsection*{Algorithms}
\begin{definition}
An algorithm is said to be \textit{numerically stable} if  errors in the input lessen in significance as the algorithm executes, having little effect on the final output. On the other hand, an algorithm is said to be \textit{numerically unstable} if errors in the input cause a considerably larger error in the final output.
\end{definition}
\begin{definition}
A problem with a low condition number is said to be \textit{well-conditioned}. Conversely, a problem with a high condition number is said to be \textit{ill-conditioned}.
\end{definition}
\subsection{Zeros of functions}
\begin{definition}
Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function. We say $\alpha$ is a \textit{zero} or \textit{solution to the equation $f(x)=0$} if $f(\alpha)=0$.
\end{definition}
\begin{definition}
Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function. We say $\alpha$ is a \textit{zero of multiplicity $m$} if $$f(\alpha)=f'(\alpha)=\cdots=f^{(m-1)}(\alpha)=0\quad\text{and}\quad f^{(m)}\ne0.$$ If $m=1$, the zero is called \textit{simple}; if $m=2$, \textit{double}...
\end{definition}
\subsubsection*{Methods}
\begin{theorem}[Bisection method]
Let $f:[a_0,b_0]\rightarrow\mathbb{R}$ be a continuous function and suppose $f(a_0)f(b_0)<0$. By Bolzano's theorem there is a $\alpha\in[a_0,b_0]$ such that $f(\alpha)=0$. Suppose $\alpha$ is unknown. Given a $\varepsilon>0$, we want to approximate the value of $\alpha$ with $\Tilde{\alpha}$, such that $|\alpha-\Tilde{\alpha}|<\varepsilon$. Consider the sequence of intervals $[a_n,b_n]$, $n\geq 0$, defined as follows. First let $$c_n=\frac{a_n+b_n}{2}.$$ If $c_n=\alpha$ we are done. If not, then let
$$[a_{n+1},b_{n+1}]=\left\{\begin{array}{ccc}
    [a_n,c_n] & \text{if} & f(a_n)f(c_n)<0, \\
    \left[c_n,b_n\right] & \text{if} & f(a_n)f(c_n)>0.
\end{array}\right.$$ Observe the length of the interval $[a_n,b_n]$ is $\frac{b_0-a_0}{2^n}$. If we take $\Tilde{\alpha}:=c_n$, we have: $$\frac{b_0-a_0}{2^{n+1}}<\varepsilon\implies n>\frac{\log\left(\frac{b_0-a_0}{\varepsilon}\right)}{\log 2}-1.$$
\end{theorem}
\begin{theorem}[\textit{Regula falsi} method]
Let $f:[a_0,b_0]\rightarrow\mathbb{R}$ be a continuous function and suppose $f(a_0)f(b_0)<0$. By Bolzano's theorem there is a $\alpha\in[a_0,b_0]$ such that $f(\alpha)=0$. Suppose $\alpha$ is unknown. In each step $n$ of the algorithm we will approximate $\alpha$ by $$c_n=b_k-f(b_k)\frac{b_k-a_k}{f(b_k)-f(a_k)}=\frac{a_kf(b_k)-b_kf(a_k)}{f(b_k)-f(a_k)}.$$ If $c_n=\alpha$ we are done. If not, let
$$[a_{n+1},b_{n+1}]=\left\{\begin{array}{ccc}
    [a_n,c_n] & \text{if} & f(a_n)f(c_n)<0, \\
    \left[c_n,b_n\right] & \text{if} & f(a_n)f(c_n)>0,
\end{array}\right.$$ and iterate the process again.
\end{theorem}
\begin{theorem}[Secant method]
Let $f:[a_0,b_0]\rightarrow\mathbb{R}$ be a continuous function and suppose $f$ has a zero at an unknown point $\alpha$. We want to approximate the value of $\alpha$. Suppose we have two different initial approximations $x_0$, $x_1$. Then for each step $n$ we obtain a new approximation $x_{n+2}$, given by: $$x_{n+2}=\frac{x_nf(x_{n+1})-x_{n+1}f(x_n)}{f(x_{n+1})-f(x_n)}.$$
\end{theorem}
\begin{theorem}[Newtonâ€“Raphson method]
Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function of class $\mathcal{C}^1$ and suppose $f$ has a zero at an unknown point $\alpha$. We want to approximate the value of $\alpha$. Suppose we have an initial approximation $x_0$. Then for each step $n$ we obtain a new approximation $x_{n+1}$, given by: $$x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}.$$
\end{theorem}
\begin{theorem}[Chebyshev method]
Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function of class $\mathcal{C}^2$ and suppose $f$ has a zero at an unknown point $\alpha$. We want to approximate the value of $\alpha$. Suppose we have an initial approximation $x_0$. Then for each step $n$ we obtain a new approximation $x_{n+1}$, given by: $$x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}-\frac{1}{2}\frac{\left[f(x_n)\right]^2f''(x_n)}{\left[f'(x_n)\right]^3}.$$
\end{theorem}
\subsubsection*{Fixed point methods}
\begin{definition}
Let $g:[a,b]\rightarrow[a,b]\subset\mathbb{R}$ be a function. A point $\alpha\in[a,b]$ is \textit{n-periodic} if $g^n(\alpha)=\alpha$ and $g^j(\alpha)\ne\alpha$ for $j=1,\ldots,n-1$\footnote{Note that 1-periodic points are the fixed points of $f$.}.
\end{definition}
\begin{theorem}[Fixed point theorem]
Let $(M,d)$ be a metric space and $g:M\rightarrow M$ a contraction. Then $f$ has a unique fixed point $\alpha\in M$ and for every $x_0\in E$, $$\alpha=\lim_{n\to\infty}x_n,\quad\text{where}\quad x_n=g(x_{n-1})\quad\forall n\in\mathbb{N}.$$
\end{theorem}
\begin{prop}
Let $(M,d)$ be a metric space and $g:M\rightarrow M$ a contraction of constant $k$. Then if we want to approximate a fixed point $\alpha$ by the iteration $x_n=g(x_{n-1})$, we have:
\begin{align*}
    d(x_n,\alpha)&\leq\frac{k^n}{1-k}d(x_1,x_0)\quad&\text{(a priori estimation)}\\
    d(x_n,\alpha)&\leq\frac{k}{1-k}d(x_n,x_{n-1})\quad&\text{(a posteriori estimation)}
\end{align*}
\end{prop}
\begin{corollary}
Let $g:\mathbb{R}\rightarrow\mathbb{R}$ be a function of class $\mathcal{C}^1$. Suppose $\alpha$ is a fixed point of $g$ and $|g'(\alpha)|<1$. Then, there exists $\varepsilon>0$ such that $g$ is a contraction on $[\alpha-\varepsilon,\alpha+\varepsilon]$. In particular, the iteration $x_{n+1}=g(x_n)$ converges to $\alpha$.
\end{corollary}
\begin{definition}
Let $g:\mathbb{R}\rightarrow\mathbb{R}$ be a function of class $\mathcal{C}^1$ and $\alpha$ be a fixed point of $g$. We say $\alpha$ is an \textit{attractor fixed point} if $|g'(\alpha)|<1$. In this case, any iteration $x_{n+1}=g(x_n)$ converges to $\alpha$. Otherwise, we say $\alpha$ is a \textit{repulsor fixed point} if $|g'(\alpha)|>1$. In this case, any iteration $x_{n+1}=g(x_n)$ doesn't converge to $\alpha$.
\end{definition}
\textcolor{green}{POSAR DIAGRAMES TERANYINA I ESCALA}
\begin{definition}[Order of convergence]
Let $(x_n)$ be a sequence of real numbers that converges to $\alpha\in\mathbb{R}$. We say $(x_n)$ has \textit{order of convergence $p\in\mathbb{R}\setminus\{0\}$} if exists $C>0$ such that: $$\lim_{n\to\infty}\frac{|x_{n+1}-\alpha|}{|x_n-\alpha|^p}=C.$$ The constant $C$ is called \textit{asymptotic error constant}. For the case $p=1$, we need $C<1$. In this case the convergence is called \textit{linear convergence}; for $p=2$, is called \textit{quadratic convergence}, and for $p=3$, \textit{cubic convergence}. If it's satisfied that $$\lim_{n\to\infty}\frac{|x_{n+1}-\alpha|}{|x_n-\alpha|^p}=0$$ for some $p\in\mathbb{R}$, we say the sequence has \textit{order of convergence at least $p$}.
\end{definition}
\begin{theorem}
Let $g:\mathbb{R}\rightarrow\mathbb{R}$ be a function of class $\mathcal{C}^p$ and let $\alpha$ be a fixed point of $g$. Suppose $$g'(\alpha)=g''(\alpha)=\cdots=g^{(p-1)}(\alpha)=0$$ with $g'(\alpha)$ if $p=1$. Then the iteration $x_{n+1}=g(x_n)$ has order of convergence at least $p$. If, moreover, $g^{(p)}(\alpha)\ne0$ then the previous iteration has order of convergence $p$ with asymptotic error constant $C=\frac{|g^{(p)}(\alpha)|}{p!}$.
\end{theorem}
\begin{theorem}
Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function of class $\mathcal{C}^2$ and let $\alpha$ be a simple zero of $f$. If $f''(\alpha)\ne0$, then Newton's method for finding $\alpha$ has a quadratic convergence with asymptotic error constant $C=\frac{1}{2}\left|\frac{f''(\alpha)}{f'(\alpha)}\right|$.\par If $f\in\mathcal{C}^{m+2}$, and $\alpha$ is a zero of multiplicity $m>1$, then Newton's method has a linear convergence.
\end{theorem}
\begin{theorem}
Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function of class $\mathcal{C}^3$ and let $\alpha$ be a simple zero of $f$. Then Chebyshev's method for finding $\alpha$ has at least a cubic convergence.
\end{theorem}
\begin{definition}
We define the \textit{computational efficiency} as a function $E(p,t)$, where $t$ is the time taken for each iteration of the method and $p$ is the order of convergence of the method. $E(p,t)$ must satisfy the following properties:
\begin{enumerate}
    \item $E(p,t)$ is increasing with respect to the variable $p$ and decreasing with respect to $t$.
    \item $E(p,t)=E(mt,p^m)$ for every $m\in\mathbb{R}$.
\end{enumerate}
Examples of such functions are the following: $$E(p,t)=\frac{\log p}{t},\quad E(p,t)=p^{1/t}.$$ 
\end{definition}
\end{multicols}
\end{document}
