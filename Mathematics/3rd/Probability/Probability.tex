\documentclass[../../../main_math.tex]{subfiles}
% 3 line break in Bernoulli, Binomial and Geometric distributions.
% break in Continuity from below theorem

\begin{document}
\changecolor{P}
\begin{multicols}{2}[\section{Probability}]
  \subsection{Probabilistic models}
  \subsubsection{\texorpdfstring{$\sigma$}{sigma}-algebras}
  \begin{definition}[Algebra]
    Let $\Omega$ be a set and $\mathcal{A}\subset\mathcal{P}(\Omega)$. We say that $\mathcal{A}$ is an \emph{algebra} over $\Omega$ if:
    \begin{enumerate}
      \item $\Omega\in\mathcal{A}$.
      \item If $A\in\mathcal{A}$, then $A^c\in\mathcal{A}$.
      \item If $A,B\in\mathcal{A}$, then $A\cup B\in\mathcal{A}$.
    \end{enumerate}
  \end{definition}
  \begin{proposition}
    Let $\mathcal{A}$ be an algebra over a set $\Omega$. Then:
    \begin{enumerate}
      \item $\varnothing\in\mathcal{A}$.
      \item If $A,B\in\mathcal{A}$, then $A\cap B\in\mathcal{A}$.
      \item For all $n\in\NN$, if $A_1,\ldots,A_n\in\mathcal{A}$, then: $$\bigcup_{i=1}^nA_i\in\mathcal{A}\quad\text{and}\quad\bigcap_{i=1}^nA_i\in\mathcal{A}$$
    \end{enumerate}
  \end{proposition}
  \begin{definition}[$\sigma$-algebra]
    Let $\Omega$ be a set and $\mathcal{A}\subset\mathcal{P}(\Omega)$. We say that $\mathcal{A}$ is a \emph{$\sigma$-algebra} over $\Omega$ if:
    \begin{enumerate}
      \item $\Omega\in\mathcal{A}$.
      \item If $A\in\mathcal{A}$, then $A^c\in\mathcal{A}$.
      \item If $A_1,A_2,\ldots\in\mathcal{A}$, then: $$\bigcup_{n=1}^\infty A_n\in\mathcal{A}$$
    \end{enumerate}
  \end{definition}
  \begin{proposition}
    Let $\Omega$ be a set, $I$ be an index set and $\{\mathcal{A}_i:i\in I\}$ be a collection of $\sigma$-algebras. Then, $\bigcap_{i\in I} \mathcal{A}_i$ is a $\sigma$-algebra.
  \end{proposition}
  \begin{proposition}
    Let $\mathcal{A}$ be an $\sigma$-algebra over a set $\Omega$. Then:
    \begin{enumerate}
      \item $\varnothing\in\mathcal{A}$.
      \item If $A_1,A_2,\ldots\in\mathcal{A}$, then: $$\bigcap_{n=1}^\infty A_n\in\mathcal{A}$$
      \item For all $n\in\NN$, if $A_1,\ldots,A_n\in\mathcal{A}$, then: $$\bigcup_{i=1}^nA_i\in\mathcal{A}\quad\text{and}\quad\bigcap_{i=1}^nA_i\in\mathcal{A}$$
    \end{enumerate}
  \end{proposition}
  \begin{definition}
    Let $\Omega$ be a set. The \emph{trivial $\sigma$-algebra} is the smallest $\sigma$-algebra over $\Omega$, that is, $\{\varnothing,\Omega\}$.
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set. The \emph{discrete $\sigma$-algebra} is the largest $\sigma$-algebra over $\Omega$, that is, $\mathcal{P}(\Omega)$.
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set and $A\subseteq\Omega$ be a subset. The \emph{$\sigma$-algebra generated} by $A$, $\sigma(A)$, is the smallest $\sigma$-algebra over $\Omega$ containing $A$, that is: $$\sigma(A)=\{\varnothing,\Omega,A,A^c\}$$
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set and $\mathcal{C}\subseteq\mathcal{P}(\Omega)$ be a subset. The \emph{$\sigma$-algebra generated} by $\mathcal{C}$, $\sigma(\mathcal{C})$, is the smallest $\sigma$-algebra over $\Omega$ containing all the elements of $\mathcal{C}$. Moreover, if $\{\mathcal{A}_n:\mathcal{C}\subseteq\mathcal{A}_n,1\leq n\leq N\}$, $N\in\NN\cup\{\infty\}$, are all the $\sigma$-algebras over $\Omega$ containing $\mathcal{C}$, then:
    $$\sigma(\mathcal{C})=\bigcap_{n=1}^N\mathcal{A}_n$$
  \end{definition}
  \begin{theorem}
    Let $\Omega$ be a set and $\mathcal{C},\mathcal{B}\subseteq\mathcal{P}(\Omega)$ be subsets. Suppose:
    \begin{enumerate}
      \item $\mathcal{B}$ is a $\sigma$-algebra.
      \item $\mathcal{C}\subseteq\mathcal{B}$.
    \end{enumerate}
    Then, $\sigma(\mathcal{C})\subseteq\mathcal{B}$.
  \end{theorem}
  \begin{definition}\label{P:borel}
    Let $(\Omega,\mathcal{T})$ be a topological space. The \emph{Borel $\sigma$-algebra} over $(\Omega,\mathcal{T})$, $\mathcal{B}((\Omega,\mathcal{T}))$, is the $\sigma$-algebra generated by the open sets of $(\Omega,\mathcal{T})$: $$\mathcal{B}((\Omega,\mathcal{T})):=\sigma(\mathcal{T})$$
    In particular, the Borel $\sigma$-algebra over $\RR$ (together with the usual topology) is: $$\mathcal{B}(\RR):=\sigma(\{U\subseteq\RR:U\text{ is open}\})$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{T})$ be a topological space. Then: $$\mathcal{B}((\Omega,\mathcal{T}))=\sigma(\{C\subseteq\Omega:C\text{ is closed}\})$$
  \end{proposition}
  \begin{proposition}
    Consider the Borel $\sigma$-algebra over $\RR$, $\mathcal{B}(\RR)$. Then:
    \begin{enumerate}
      \item $\mathcal{B}(\RR)=\sigma(\{(a,b)\subset\RR: a,b\in\RR, a<b\})$
      \item $\mathcal{B}(\RR)=\sigma(\{[a,b]\subset\RR: a,b\in\RR, a<b\})$
      \item $\mathcal{B}(\RR)=\sigma(\{[a,b)\subset\RR: a,b\in\RR, a<b\})$
      \item $\mathcal{B}(\RR)=\sigma(\{(a,b]\subset\RR: a,b\in\RR, a<b\})$
      \item $\mathcal{B}(\RR)=\sigma(\{(a,\infty)\subset\RR: a\in\RR\})$
      \item $\mathcal{B}(\RR)=\sigma(\{(-\infty,a)\subset\RR: a\in\RR\})$
      \item $\mathcal{B}(\RR)=\sigma(\{[a,\infty)\subset\RR: a\in\RR\})$
      \item $\mathcal{B}(\RR)=\sigma(\{(-\infty,a]\subset\RR: a\in\RR\})$
    \end{enumerate}
  \end{proposition}
  \subsubsection{Probability}
  \begin{definition}[Sample space]
    The \emph{sample space} $\Omega$ of an experiment is the set of all possible outcomes of that experiment.
  \end{definition}
  \begin{definition}[Kolmogorov axioms]
    Let $\Omega$ be a set and $\mathcal{A}$ be a $\sigma$-algebra over $\Omega$. A \emph{probability} is any function $$\Prob:\mathcal{A}\longrightarrow[0,\infty)$$ satisfying the following properties:
    \begin{itemize}
      \item $\Prob(\Omega)=1$.
      \item \emph{$\sigma$-additivity}: If $\{A_n:n\geq1\}\subset\mathcal{A}$ are pairwise disjoint, then: $$\Prob\left(\bigsqcup_{n=1}^\infty A_n\right)=\sum_{n=1}^\infty \Prob(A_n)$$
    \end{itemize}
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set and $\mathcal{A}$ be a $\sigma$-algebra over $\Omega$. An \emph{event} $A\in\mathcal{A}$ is a subset of $\Omega$ for which we want to calculate the probability.
  \end{definition}
  \begin{definition}
    A \emph{probability space} is a triplet $(\Omega,\mathcal{A},\Prob)$ where $\Omega$ is any set, $\mathcal{A}$ is a $\sigma$-algebra over $\Omega$ and $\Prob$ is a probability over $\mathcal{A}$.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A,B\in\mathcal{A}$. Then, we have the following properties:
    \begin{enumerate}
      \item $\Prob(\varnothing)=0$.
      \item If $A_i\in\mathcal{A}$, $i=1,\ldots,n$, is a finite set of pairwise disjoint events, then: $$\Prob\left(\bigsqcup_{i=1}^n A_i\right)=\sum_{i=1}^n \Prob(A_i)$$
      \item $\Prob(A\setminus B) =\Prob(A)-\Prob(A\cap B)$.
      \item If $B\subset A$, then $\Prob(A\setminus B)=\Prob(A)-\Prob(B)$.
      \item If $B\subset A$, then $\Prob(B)\leq \Prob(A)$.
      \item $\Prob(A)\leq 1$.
      \item $\Prob(A^c)=1-\Prob(A)$.
      \item $\Prob(A\cup B) =\Prob(A)+\Prob(B)-\Prob(A\cap B)$.
      \item If $A_1,\ldots,A_n\in\mathcal{A}$, then:
            \begin{multline*}
              \Prob\left(\bigcup_{i=1}^n A_i\right)=\sum_{i=1}^n \Prob(A_i)-\\-\sum_{\substack{i,j=1\\i<j}}^n\Prob(A_i\cap A_j)+\sum_{\substack{i,j,k=1\\i<j<k}}^n\Prob(A_i\cap A_j\cap A_k)-\cdots+\\+{(-1)}^{n+1}\Prob(A_1\cap\cdots\cap A_n)
            \end{multline*}
      \item If $A_1,\ldots,A_n\in\mathcal{A}$, then:
            \begin{multline*}
              \Prob\left(\bigcap_{i=1}^n A_i\right)=\sum_{i=1}^n \Prob(A_i)-\\-\sum_{\substack{i,j=1\\i<j}}^n\Prob(A_i\cup A_j)+\sum_{\substack{i,j,k=1\\i<j<k}}^n\Prob(A_i\cup A_j\cap A_k)-\cdots+\\+{(-1)}^{n+1}\Prob(A_1\cup\cdots\cup A_n)
            \end{multline*}
      \item \emph{Finite subadditivity}: If $A_1,\ldots,A_n\in\mathcal{A}$, then: $$\Prob\left(\bigcup_{i=1}^n A_i\right)\leq\sum_{i=1}^n \Prob(A_i)$$
    \end{enumerate}
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space such that $\Omega$ is finite and all its elements are equiprobable. Let $A\in\mathcal{A}$ be an event. Then: $$\Prob(A)=\frac{|A|}{|\Omega|}$$
  \end{proposition}
  \begin{theorem}[Continuity from below]
    Let\\ $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(A_n)\subset\mathcal{A}$ be an increasing sequence of events, that is: $$A_1\subseteq A_2\subseteq\cdots\subseteq A_n\subseteq\cdots$$ Let $A:=\bigcup_{n=1}^\infty A_n$. Then: $$\Prob(A):=\lim_{n\to\infty}\Prob(A_n)$$
  \end{theorem}
  \begin{corollary}[Continuity from above]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(A_n)\subset\mathcal{A}$ be a decreasing sequence of events, that is: $$A_1\supseteq A_2\supseteq\cdots\supseteq A_n\supseteq\cdots$$ Let $A:=\bigcap_{n=1}^\infty A_n$. Then: $$\Prob(A):=\lim_{n\to\infty}\Prob(A_n)$$
  \end{corollary}
  \begin{proposition}[Countable subadditivity]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(A_n)\subset\mathcal{A}$ be a sequence of events. Then: $$\Prob\left(\bigcup_{n=1}^\infty A_n\right)\leq\sum_{n=1}^\infty \Prob(A_n)$$
  \end{proposition}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(A_n)\subset\mathcal{A}$ be a sequence of events with probability 0. Then: $$\Prob\left(\bigcup_{n=1}^\infty A_n\right)=0$$
  \end{corollary}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(A_n)\subset\mathcal{A}$ be a sequence of events with probability 1. Then: $$\Prob\left(\bigcap_{n=1}^\infty A_n\right)=1$$
  \end{corollary}
  \subsubsection{Conditional probability}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$ be an event such that $\Prob(A)>0$. The \emph{conditional probability} that $B\in\mathcal{A}$ occurs given that $A$ occurs is defined as: $$\Prob(B\mid A):=\frac{\Prob(A\cap B)}{\Prob(A)}$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$ be an event such that $\Prob(A)>0$. Then, the function
    $$
      \function{\Prob(\cdot\mid A)}{\mathcal{A}}{[0,\infty]}{B}{\Prob(B\mid A)}
    $$
    is a probability.
  \end{proposition}
  \begin{proposition}[Compound probability formula]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$ be an event such that $\Prob(A)>0$. Then, $\forall B\in\mathcal{A}$: $$\Prob(A\cap B)=\Prob(B\mid A)\Prob(A)$$
  \end{proposition}
  \begin{proposition}[Generalized compound probability formula]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A_1,\ldots,A_n\in\mathcal{A}$, $n\geq 2$, be events such that $\Prob(A_1\cap\cdots\cap A_{n-1})>0$. Then:
    \begin{multline*}
      \Prob(A_1\cap\cdots\cap A_n)=\Prob(A_1)\Prob(A_2\mid A_1)\Prob(A_3\mid A_2\cap A_1)\cdots\\\cdots \Prob(A_n\mid A_1\cap\cdots\cap A_{n-1})
    \end{multline*}
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A=\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be a collection of events. We say that $A$ is a \emph{partition} of $\Omega$ if: $$\Omega=\bigsqcup_{n=1}^NA_n$$
  \end{definition}
  \begin{proposition}[Law of total probability]\label{P:totalprob}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be a partition of $\Omega$ such that $\Prob(A_n)>0$ for all $1\leq n\leq N$. Then, $\forall A\in\mathcal{A}$: $$\Prob(A)=\sum_{n=1}^N\Prob(A_n)\Prob(A\mid A_n)$$
  \end{proposition}
  \begin{proof}
    \begin{multline*}
      \Prob(A)=\Prob(A\cap \Omega)=\Prob\left(\bigsqcup_{n=1}^N(A\cap A_n)\right)=\\
      =\sum_{n=1}^{N}\Prob(A\cap A_n)=\sum_{n=1}^N\Prob(A_n)\Prob(A\mid A_n)
    \end{multline*}
  \end{proof}
  \begin{proposition}[Bayes' formula]\label{P:bayes}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be a partition of $\Omega$ such that $\Prob(A_n)>0$ for all $1\leq n\leq N$. Let $A\in\mathcal{A}$ with $\Prob(A)>0$. Then, $\forall k\leq N$: $$\Prob(A_k\mid A)=\frac{\Prob(A_k)\Prob(A\mid A_k)}{\sum_{n=1}^N\Prob(A_n)\Prob(A\mid A_n)}$$
  \end{proposition}
  \subsubsection{Independence of events}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that $A,B\in\mathcal{A}$ are \emph{independent events} if $$\Prob(A\cap B)=\Prob(A)\Prob(B)$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Then:
    \begin{enumerate}
      \item $\varnothing$ and $\Omega$ are independent of any event.
      \item If $A\in\mathcal{A}$ satisfies either $\Prob(A)=0$ or $\Prob(A)=1$, then $A$ is independent of any other event $B\in\mathcal{A}$.
      \item If an event $A\in\mathcal{A}$ is independent of itself, then either $\Prob(A)=0$ or $\Prob(A)=1$.
    \end{enumerate}
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A,B\in\mathcal{A}$ be two events. The following statements are equivalent:
    \begin{itemize}
      \item $A$ and $B$ are independent.
      \item $A^c$ and $B$ are independent.
      \item $A$ and $B^c$ are independent.
      \item $A^c$ and $B^c$ are independent.
    \end{itemize}
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $n\in\NN$. We say that $A_1,\ldots,A_n\in\mathcal{A}$ are \emph{independent events} if for any $i_1,\ldots,i_k\in\{1,\ldots,n\}$, we have: $$\Prob\left(\bigcap_{r=1}^kA_{i_r}\right)=\prod_{r=1}^k\Prob(A_{i_r})$$
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $I$ be an arbitrary index set. We say that $\{A_i:i\in I\}\subset\mathcal{A}$ are \emph{independent events} if for any finite subset $\{A_{i_1},\ldots, A_{i_k}:i_r\in I\text{ for }r=1,\ldots,k\}$ of different events, we have: $$\Prob\left(\bigcap_{r=1}^kA_{i_r}\right)=\prod_{r=1}^k\Prob(A_{i_r})$$
  \end{definition}
  \subsection{Lebesgue integration}
  \begin{definition}
    Let $A\subset\RR^n$ be a subset. Then, $A$ is a \emph{null set} (or a \emph{set of zero-content}) if $\forall \varepsilon>0$ there exists a collection $\{R_k\subset\RR^n:R_k\text{ is a rectangle }\forall k\in\NN\}$ of rectangles such that:
    $$A\subset\bigcup_{k=1}^\infty R_k\qquad\text{and}\qquad\sum_{k=1}^\infty\text{vol}(R_k)<\varepsilon$$
  \end{definition}
  \begin{definition}
    Let $E$ be a set and $\mathcal{E}$ be a $\sigma$-algebra over $E$. We say that the function:
    $$
      \function{\mu}{\mathcal{E}}{[0,\infty]}{A}{\mu(A)}
    $$
    is a \emph{measure} if:
    \begin{enumerate}
      \item There exists $A\in\mathcal{E}$ such that $\mu(A)<\infty$.
      \item If $\{A_n\in\mathcal{E}:n\in\NN\}$ is a collection of pairwise disjoint sets, then: $$\mu\left(\bigcup_{n=1}^\infty A_n\right)=\sum_{n=1}^\infty \mu(A_n)$$
    \end{enumerate}
    The triplet $(E,\mathcal{E},\mu)$ is called a \emph{measurable space}.
  \end{definition}
  \begin{definition}
    The \emph{$\sigma$-algebra of all Lebesgue measurable sets in $\RR^n$}, $\mathcal{L}_n\subset\mathcal{P}(\RR^n)$, is defined as:
    \begin{equation*}
      \mathcal{L}_n:=\{A\subseteq\RR^n:A=B\cup N\}
    \end{equation*}
    where $B\in\mathcal{B}(\RR^n)$ and $N$ is a null set.
  \end{definition}
  \begin{theorem}
    We can extend the concept of volume on rectangles in $\RR^n$ to all the elements in $\mathcal{L}_n$. This extension is called \emph{Lebesgue measure} (or simply \emph{volume}) in $\RR^n$.
  \end{theorem}
  \begin{definition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space and $f:E\rightarrow\RR$ be a function. We say that $f$ is \emph{measurable} if $\forall B\in\mathcal{B}(\RR)$ we have $f^{-1}(B)\in\mathcal{E}$. The \emph{Lebesgue integral} of $f$ over $E$ with respect to $\mu$ is denoted by: $$\int_Ef\dd\mu$$
  \end{definition}
  \begin{proposition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space and $f:E\rightarrow\RR$ be a measurable function such that $f(x)\geq 0$ $\forall x\in E$. Then, we can always define the integral $$\int_Ef\dd\mu$$ taking into account that may be $+\infty$.
  \end{proposition}
  \begin{definition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space and $f:E\rightarrow\RR$ be a measurable function. We say that $f$ is \emph{Lebesgue integrable} with respect to $\mu$ if: $$\int_E|f|\dd\mu<\infty$$
    Moreover if $G\in\mathcal{E}$, we define: $$\int_Gf\dd\mu:=\int_Ef\indi{G}\dd\mu$$
  \end{definition}
  \begin{proposition}
    Consider the measurable space $(\RR^n,\mathcal{L}_n,m_n)$, where $m_n$ is the volume in $\RR^n$. Let $f:\RR^n\rightarrow\RR$ be a Riemann integrable function satisfying: $$\int_{\RR^n}|f(x_1,\ldots,x_n)|\dd{x_1}\cdots\dd{x_n}<\infty$$
    Then, $f$ is Lebesgue integrable and: $$\int_{\RR^n}|f(x_1,\ldots,x_n)|\dd{x_1}\cdots\dd{x_n}=\int_{\RR^n}f\dd{m_n}$$
  \end{proposition}
  \begin{theorem}[Tonelli's theorem]
    Let $f:\RR^2\rightarrow\RR$ be a non-negative Lebesgue measurable function. Then:
    \begin{align*}
      \int_{\RR^2}f(x,y)\dd{x}\dd{y} & =\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}f(x,y)\dd{x}\right)\dd{y} \\
                                     & =\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}f(x,y)\dd{y}\right)\dd{x}
    \end{align*}
  \end{theorem}
  \begin{theorem}[Fubini's theorem]
    Let $f:\RR^2\rightarrow\RR$ be a Lebesgue measurable function such that at least one of the following integrals is finite.
    \begin{gather*}
      \int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}|f(x,y)|\dd{x}\right)\dd{y}\\
      \int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}|f(x,y)|\dd{y}\right)\dd{x}
    \end{gather*}
    Then, $f$ is Lebesgue integrable and:
    \begin{align*}
      \int_{\RR^2}f(x,y)\dd{x}\dd{y} & =\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}f(x,y)\dd{x}\right)\dd{y} \\
                                     & =\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}f(x,y)\dd{y}\right)\dd{x}
    \end{align*}
  \end{theorem}
  \subsection{Random variables and random vectors}
  \subsubsection{Random variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. A \emph{real random variable} (or simply \emph{random variable}) $X$ is a function $X:\Omega\rightarrow\RR$ satisfying for all $B\in\mathcal{B}(\RR)$: $$X^{-1}(B)=\{\omega\in\Omega:X(\omega)\in B\}\in\mathcal{A}$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\mathcal{C}$ be a collection of subsets of $\RR$ such that $\mathcal{B}(\RR)=\sigma(\mathcal{C})$ and let $X:\Omega\rightarrow\RR$ be a function. Then, $X$ is a random variable if and only if $X^{-1}(B)\in\mathcal{A}$, $\forall B\in \mathcal{C}$.
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $a,b\in\RR$ and $B\in\mathcal{B}(\RR)$. We define the following set:
    $$\{X\in B\}:=\{\omega\in\Omega:X(\omega)\in B\}=X^{-1}(B)$$
    In particular:
    \begin{align*}
      \{X\leq a\}   & :=\{\omega\in\Omega:X(\omega)\leq a\}=X^{-1}((-\infty,a]) \\
      \{X> b\}      & :=\{\omega\in\Omega:X(\omega)>b\}=X^{-1}((b,\infty))      \\
      \{a<X\leq b\} & :=\{\omega\in\Omega:a< X(\omega)\leq b\}=X^{-1}([b,a))    \\
      \{X=a\}       & :=\{\omega\in\Omega:X(\omega)=a\}=X^{-1}(\{a\})
    \end{align*}
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$, $Y$ be random variables and $a\in\RR$. Then:
    \begin{enumerate}
      \item $X+Y$ is also a random variable.
      \item $aX$ is also a random variable.
      \item $XY$ is also a random variable.
      \item $\frac{1}{X}$ is also a random variable if $X(\omega)\ne 0$ $\forall \omega\in\Omega$.
    \end{enumerate}
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of random variables. Then, the following quantities are also random variables provided that they are finite for all $\omega\in\Omega$:
    \begin{enumerate}
      \item $\sup X_n$
      \item $\inf X_n$
      \item $\displaystyle\limsup_{n\to\infty}X_n$
      \item $\displaystyle\liminf_{n\to\infty} X_n$
    \end{enumerate}
  \end{proposition}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of random variables such that $\forall\omega\in\Omega$ the following limit exists and it is finite: $$X(\omega):=\lim_{n\to\infty}X_n(\omega)$$
    Then, $X$ is a random variable.
  \end{corollary}
  \subsubsection{Distribution of a random variable}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. The \emph{distribution} of a random variable $X$ is the function:
    $$
      \function{\Prob_X}{\mathcal{B}(\RR)}{[0,1]}{B}{\Prob(\{X\in B\})\footnotemark}
    $$
  \end{definition}
  \begin{proposition}\footnotetext{From now on, in order to simplify the notation, we will write $\Prob(X\in B):=\Prob(\{X\in B\})$.}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Then, for any random variable $X$, the function $\Prob_X$ is a probability over $\mathcal{B}(\RR)$. Hence, $(\RR,\mathcal{B}(\RR),\Prob_X)$ is a probability space.
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that two random variables $X$, $Y$ are \emph{equal in distribution} (denoted by $X\overset{\text{d}}{=}Y$) if they satisfy: $$\Prob_X(B)=\Prob_Y(B)\qquad\forall B\in\mathcal{B}(\RR)$$ That is, $X\overset{\text{d}}{=}Y$ if they have the same distribution functions.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that two random variables $X$, $Y$ are \emph{equal almost surely} (denoted by $X\overset{\text{a.s.}}{=}Y$) if $\Prob(X=Y)=1$, or equivalently, if $\Prob(X\ne Y)=0$.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be two random variables such that $X\overset{\text{a.s.}}{=}Y$. Then, $X\overset{\text{d}}{=}Y$.
  \end{proposition}
  \begin{definition}[Cumulative distribution function]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable. We define the \emph{cumulative distribution function} (\emph{cdf}) as:
    $$
      \function{F_X}{\RR}{[0,1]}{x}{\Prob(X\leq x)=\Prob_X((-\infty,x])}
    $$
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a random variable and $F_X$ be its cdf. Then:
    \begin{enumerate}
      \item If $x<y$, then $F_X(x)\leq F_X(y)$.
      \item $F_X$ is \emph{càdlàg}\footnote{From French ``continue à droite, limite à gauche'' (right continuous with left limits).}.
      \item $\displaystyle\lim_{x\to -\infty}F_X(x)=0$ and $\displaystyle\lim_{x\to \infty}F_X(x)=1$.
    \end{enumerate}
    Reciprocally, if there is a function $F$ satisfying these properties\footnote{Such kind of functions are called \emph{distribution functions}.}, then there exists a random variable $X$ on $(\Omega,\mathcal{A},\Prob)$ such that $F$ is its cdf.
  \end{theorem}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a random variable and $F_X$ be its cdf. Then:
    \begin{enumerate}
      \item $F_X$ has at most a countable number of discontinuities.
      \item $\forall x,y\in\RR$ such that $s<t$, we have:
            \begin{align*}
              \Prob(x<X\leq y)     & =F(y)-F(x)                                 \\
              \Prob(x<X<t)         & =\lim_{t\to y^-}F_X(y)-F(x)                \\
              \Prob(x\leq X\leq y) & =F(y)-\lim_{t\to x^-}F_X(x)                \\
              \Prob(x\leq X< y)    & =\lim_{t\to y^-}F_X(y)-\lim_{t\to x^-}F(x)
            \end{align*}
      \item $\displaystyle\forall x\in\RR$, $\displaystyle \Prob(X<x)=\lim_{t\to x^-}F_X(t)$.
      \item For all $x\in\RR$: $$\Prob(X=x)=F_X(x)-\lim_{t\to x^-}F_X(t)$$ Hence, $F_X$ is discontinuous at $x\iff\Prob(X=x)>0$.
    \end{enumerate}
  \end{proposition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Then, the cdf completely determine a distribution of a random variable $X$. That is, if $X$ and $Y$ are random variables such that $F_X(t)=F_Y(t)$ $\forall t\in\RR$, then $X\overset{\text{d}}{=}Y$.
  \end{theorem}
  \subsubsection{Discrete random  variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that a random variable $X$ is \emph{discrete} if there exists a finite or countable set $S\subset\RR$ such that $\Prob(X\in S)=1$\footnote{By agreement, we will suppose that $S$ only contains points $x$ such that $\Prob(X=x)>0$.}. In that case, $S$ is called the \emph{support} of $X$\footnote{In general we will denote $S$ by $S_X$.}.
  \end{definition}
  \begin{definition}[Probability mass function]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a discrete random variable with support points $S_X=\{x_i:i\in I\}$, where $I\subseteq\NN$. The \emph{probability mass function} (\emph{pmf}) of the random variable $X$ is:
    $$
      \function{p_X}{S_X}{[0,1]}{x_i}{\Prob(X=x_i)}
    $$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a discrete random variable with support points $S_X=\{x_i:i\in I\}$ and $p_X$ be its pmf. Then:
    \begin{enumerate}
      \item $p_X(x_i)>0$ $\forall i\in I$.
      \item $\displaystyle\sum_{i\in I}p_X(x_i)=1$.
      \item $\forall B\in\mathcal{B}(\RR)$, we have: $$\Prob(X\in B)=\sum_{i\in I:x_i\in B}p_X(x_i)$$
    \end{enumerate}
  \end{proposition}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a discrete random variable with support points $S_X=\{x_i:i\in I\}$, $F_X$ be its cdf and $p_X$ be its pmf. Then $\forall x\in\RR$ we have: $$F_X(x)=\Prob(X\leq x)=\sum_{i\in I:x_i\leq x}p_X(x_i)$$
  \end{corollary}
  \begin{definition}[Degenerated distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. The \emph{degenerated distribution} consists in taking a constant random variable $X$ so that $$\Prob(X=a)=1$$ for some $a\in\RR$. Here we have $S_X=\{a\}$.
  \end{definition}
  \begin{definition}[Bernoulli distribution]
    Let\\ $(\Omega,\mathcal{A},\Prob)$ be a probability space. The \emph{Bernoulli distribution} is the one whose random variable $X$ can only take two values (1 and 0)\footnote{Also called \emph{success}/\emph{true} or \emph{failure}/\emph{false}, respectively.} with probabilities $p$ and $q:=1-p$: $$\Prob(X=0)=p\qquad \Prob(X=1)=q$$ Here we have $S_X=\{0,1\}$. If $X$ follows a Bernoulli distribution of parameter $p$, we will write $X\sim \text{Ber}(p)$.
  \end{definition}
  \begin{definition}[Discrete uniform distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. The \emph{discrete uniform distribution} is the one whose random variable $X$ takes values on $S_X=\{x_1,\ldots,x_n\}$ each of these with probability $1/n$: $$\Prob(X=x_i)=\frac{1}{n}\qquad \forall i=1,\ldots,n$$ If $X$ follows a discrete uniform distribution, we will write $X\sim U(\{x_1,\ldots,x_n\})$. The probability space $(S,\mathcal{P}(S),\Prob_X)$ is an \emph{equiprobable space}.
  \end{definition}
  \begin{definition}[Binomial distribution]
    Let\\ $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$. Suppose $\Prob(A)=p$. The \emph{binomial distribution} is the one whose random variable $X$ is the number of successes of $A$ in a sequence of $n$ repetitions. Thus, $S_X=\{0,1,\ldots,n\}$ and: $$\Prob(X=k)=\binom{n}{k}p^k{(1-p)}^{n-k}\qquad \forall k=0,1,\ldots,n$$ If $X$ follows a binomial distribution of parameters $n$ and $p$, we will write $X\sim \text{B}(n,p)$\footnote{Note that, a Bernoulli distribution of parameter $p$ may be considered as a Binomial distribution of parameters $n=1$ and $p$. Hence, $\text{Ber}(p)=\text{B}(1,p)$.}.
  \end{definition}
  \begin{definition}[Poisson distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\lambda\in\RR_{>0}$. The \emph{Poisson distribution} of parameter $\lambda$ is the one whose random variable $X$ has support $S_X=\NN\cup\{0\}$ and: $$\Prob(X=k)=\exp{-\lambda}\frac{\lambda^k}{k!}\qquad \forall k\in\NN\cup\{0\}$$ If $X$ follows a Poisson distribution of parameter $\lambda$, we will write $X\sim \text{Pois}(\lambda)$.
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Let $(p_n)\subset(0,1)$ be a sequence such that: $$\lim_{n\to\infty}np_n=\lambda>0$$
    For each $n\geq 1$, consider $X_n\sim \text{B}(n,p_n)$. Then, $\forall k\in\NN\cup\{0\}$ we have: $$\lim_{n\to\infty}\Prob(X_n=k)=\lim_{n\to\infty}\binom{n}{k}{p_n}^k{(1-p_n)}^{n-k}=\exp{-\lambda}\frac{\lambda^k}{k!}$$
  \end{theorem}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and suppose $n\in\NN$ and $p\in (0,1)$ are such that $n\gg 1$ and $p\ll 1$. Then, $\text{B}(n,p)\simeq\text{Pois}(np)$\footnote{In practice, the approximation is good enough for $n\geq 10$ and $p\leq 0.05$.}.
  \end{corollary}
  \begin{definition}[Geometric distribution]
    Let\\ $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$. Suppose $\Prob(A)=p$. The \emph{geometric distribution} is the one whose random variable $X$ is the number of trials needed to get one success. Thus, $S_X=\NN$ and: $$\Prob(X=k)={(1-p)}^{k-1}p\qquad \forall k\in\NN$$ If $X$ follows a geometric distribution of parameter $p$, we will write $X\sim \text{Geo}(p)$.
  \end{definition}
  \begin{definition}[Discrete memorylessness property]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a discrete random variable whose support is $\NN$ and such that $\Prob(X >m)>0$ $\forall m\in\NN$. The distribution of $X$ is \emph{memoryless} if $\forall m,n\in\NN$, we have: $$\Prob(X>m+n\mid X >m)=\Prob(X>n)$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a discrete random variable and $p\in(0,1)$. Then, $X\sim\text{Geo}(p)$ if and only if the distribution of $X$ is memoryless.
  \end{proposition}
  \begin{definition}[Hypergeometric distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Suppose we have a population of size $N$ of whom $K$ have a special feature (success). Let $X$ be the random variable that counts the number of successes that we have obtained in $n$ draws (without replacement). Thus, the support of $X$ is: $$S_X=\{\max\{n+K-N,0\},\ldots,\min\{n,K\}\}$$ And the pmf is given by: $$\Prob(X=k)=\frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}$$ This type of distribution is called \emph{hypergeometric distribution} and it is denoted by $X\sim \text{HG}(N,p,n)$, where $p=\frac{K}{N}$ is the proportion of successes in the population.
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X\sim \text{HG}(N,p,n)$ such that when $N\to\infty$, $p$ remains constant. Then:
    $$\lim_{N\to\infty}\Prob(X=k)=\lim_{N\to\infty}\frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}=\binom{n}{k}p^k{(1-p)}^{n-k}$$ which is the pmf of a binomial distribution $\text{B}(n,p)$.
  \end{theorem}
  \begin{definition}[Negative binomial distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$. Suppose $\Prob(A)=p$. The \emph{negative binomial distribution} is the one whose random variable $X$ is the number of trials needed to get $r\geq 1$ successes. Thus, $S_X=\{r,r+1,\ldots\}$ and: $$\Prob(X=k)=\binom{k-1}{r-1}p^r{(1-p)}^{k-r}\qquad \forall k\geq r$$ If $X$ follows a negative binomial distribution of parameters $r$ and $p$, we will write $X\sim \text{NB}(r,p)$.
  \end{definition}
  \subsubsection{Absolutely continuous random variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that a random variable $X$ is \emph{absolutely continuous} if there exists a function $f:\RR\rightarrow\RR$ satisfying:
    \begin{enumerate}
      \item $f(x)\geq 0$, $\forall x\in\RR$.
      \item $f$ is integrable over $\RR$ and: $$\int_{-\infty}^{+\infty}f(x)\dd{x}=1$$
      \item For all $a,b\in\RR\cup\{\pm\infty\}$ with $a\leq b$, we have: $$\Prob(a\leq X\leq b)=\int_a^bf(x)\dd{x}$$
    \end{enumerate}
    The function $f$, denoted by $f_X$, is called \emph{probability density function} (\emph{pdf}) of $X$. In general, a function satisfying the first two properties is called a \emph{density function}.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be an absolutely continuous random variable and $F_X$ be its cdf. Then:
    \begin{enumerate}
      \item $\Prob(X=a)=0,\quad\forall a\in \RR$.
      \item $\displaystyle \Prob(X\in B)=\int_Bf_X(x)\dd{x},\quad\forall B\in\mathcal{B}(\RR)$.
      \item $\displaystyle F_X(b)=\Prob(X\leq b)=\int_{-\infty}^bf_X(x)\dd{x},\quad\forall b\in\RR$.
      \item $F_X$ is continuous on $\RR$.
      \item If $a,b\in\RR$ are such that $a<b$, then:
            \begin{multline*}
              \Prob(a<X<b)=\Prob(a\leq X<b)=\\=\Prob(a<X\leq b)=\Prob(a\leq X\leq b)
            \end{multline*}
    \end{enumerate}
  \end{proposition}
  \begin{definition}[Continuous uniform distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that an absolutely continuous random variable $X$ follows a \emph{continuous uniform distribution} on $(a,b)$ (also $[a,b]$), and we denoted it by $X\sim U(a,b)$, if $X$ has the pdf $$f_X(x)=\frac{1}{b-a}\indi{(a,b)}(x)$$ where $\indi{(a,b)}$ is the indicator function. Therefore, its cdf is:
    $$F_X(x)=\frac{x-a}{b-a}\indi{(a,b)}(x)+\indi{[b,\infty)}(x)$$
  \end{definition}
  \begin{definition}[Exponential distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that an absolutely continuous random variable $X$ follows an \emph{exponential distribution} of parameter $\lambda>0$, and we denoted it by $X\sim \text{Exp}(\lambda)$, if $X$ has the pdf: $$f_X(x)=\lambda\exp{-\lambda x}\indi{(0,\infty)}$$ Furthermore, its cdf is:
    $$F_X(x)=(1-\exp{-\lambda x})\indi{(0,\infty)}(x)$$
  \end{definition}
  \begin{definition}[Continuous memorylessness property]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be an absolutely continuous random variable such that $\Prob(X >s)>0$ $\forall s\in\RR_{\geq 0}$. The distribution of $X$ is \emph{memoryless} if $\forall s,t\in\RR_{\geq 0}$, we have: $$\Prob(X>s+t\mid X >s)=\Prob(X>t)$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be an absolutely continuous random variable and $\lambda\in\RR_{>0}$. Then, $X\sim\text{Exp}(\lambda)$ if and only if the distribution of $X$ is memoryless.
  \end{proposition}
  \begin{definition}[Standard normal distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that an absolutely continuous random variable $Z$ follows a \emph{standard normal distribution}, and we denoted it by $Z\sim N(0,1)$, if $Z$ has the pdf: $$f_X(x)=\frac{1}{\sqrt{2\pi}}\exp{\frac{-x^2}{2}}$$
  \end{definition}
  \begin{definition}[Normal distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\mu\in\RR$ and $\sigma\in\RR_{>0}$. We say that an absolutely continuous random variable $X$ follows a \emph{normal distribution}, and we denoted it by $X\sim N(\mu,\sigma^2)$, if $X$ has the pdf: $$f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\frac{-{(x-\mu)}^2}{2\sigma^2}}$$ $\mu$ is called the \emph{mean} or \emph{expectation} of $X$; $\sigma^2$, its \emph{variance}, and $\sigma$, its \emph{standard deviation}.
  \end{definition}
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \includestandalone[mode=image|tex,width=0.95\linewidth]{Images/normal}
      \captionof{figure}{Probability density function of a normal distribution}
    \end{minipage}
  \end{center}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X\sim N(\mu,\sigma^2)$ and $Z\sim N(0,1)$ be absolutely continuous random variables. Then: $$\mu+\sigma Z\overset{\text{d}}{=} X$$ Therefore, $\forall x\in\RR$ we have: $$\Prob(X\leq x)=\Prob\left(Z\leq\frac{x-\mu}{\sigma}\right)$$ In this case, $Z$ is called the \emph{standardized form} of $X$.
  \end{proposition}
  \begin{definition}[Gamma distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that an absolutely continuous random variable $X$ follows a \emph{gamma distribution} of parameters $\alpha,\beta\in\RR_{>0}$, and we denoted it by $X\sim \text{Gamma}(\alpha,\beta)$, if $X$ has the pdf: $$f_X(x)=\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}\exp{-\beta x}\indi{(0,\infty)}(x)$$
    The parameter $\alpha$ is called \emph{shape}; $\beta$, \emph{rate}, and $1/\beta$, \emph{scale}.
  \end{definition}
  \begin{definition}
    Let $a,b\in\RR_{>0}$. The \emph{beta function} is defined as\footnote{Beta function should not be confused with binomial distribution, although we have used the same notation.}: $$\text{B}(a,b):=\int_0^1x^{a-1}{(1-x)}^{b-1}\dd{x}$$
  \end{definition}
  \begin{proposition}
    For all $a,b\in\RR_{>0}$, we have: $$\text{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$$
  \end{proposition}
  \begin{definition}[Beta distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that an absolutely continuous random variable $X$ follows a \emph{beta distribution} of parameters $a,b\in\RR_{>0}$, and we denoted it by $X\sim \text{Beta}(a,b)$, if $X$ has the pdf: $$f_X(x)=\frac{1}{\text{B}(a,b)}x^{a-1}{(1-x)}^{b-1}\indi{(0,1)}(x)$$
  \end{definition}
  \begin{definition}[Cauchy distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\mu\in\RR$ and $\sigma\in\RR_{>0}$. We say that an absolutely continuous random variable $X$ follows a \emph{Cauchy distribution} of parameters $x_0\in\RR$ and $\gamma\in\RR_{>0}$, and we denoted it by $X\sim \text{C}(x_0,\gamma)$, if $X$ has the pdf: $$f_X(x)=\frac{1}{\pi\gamma\left[1+{\left(\frac{x-x_0}{\gamma}\right)}^2\right]}$$
  \end{definition}
  \begin{definition}
    A \emph{mixed random variable} is a random variable whose cdf is neither piecewise-constant (a discrete random variable) nor absolutely  continuous.
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable with cdf $F_X$. Suppose that:
    \begin{enumerate}
      \item $F_X$ is continuous.
      \item $F_X$ is differentiable at any point except for, maybe, a finite number of points.
      \item $F_X$ is continuously differentiable at any point except for, maybe, a finite number of points.
    \end{enumerate}
    Then: $$F_X(x)=\int_{-\infty}^xF'(t)\dd{t}\qquad\forall x\in\RR$$
    That is, ${F_X}'$ is the pdf of $X$.
  \end{theorem}
  \subsubsection{Transformations of random variables}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be an absolutely continuous random variable with pdf $f_X$ and $U$, $V$ be open sets such that $\Prob(X\in U)=1$. Let $h:U\rightarrow V$ be a diffeomorphism of class $\mathcal{C}^1$. Then, $Y:=h(X)$ is also an absolutely continuous random variable and: $$f_Y(y)=f_X(h^{-1}(y))|(h^{-1})'(y)|\indi{V}(y)$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be an absolutely continuous random variable with pdf $f_X$, $U_1,\ldots, U_k$ be pairwise disjoint open intervals such that $\Prob(X\in U_1\sqcup \cdots\sqcup U_k)=1$. Let $h:U_1\sqcup \cdots\sqcup U_k\rightarrow \RR$ and denote $h_i=h|_{U_i}$. Then, if $h_i:U_i\rightarrow V_i$ are  diffeomorphisms of class $\mathcal{C}^1$ for $i=1,\ldots,k$, then $Y:=h(X)$ is also an absolutely continuous random variable and: $$f_Y(y)=\sum_{i=1}^kf_X({h_i}^{-1}(y))|({h_i}^{-1})'(y)|\indi{V_i}(y)$$
  \end{proposition}
  \subsubsection{Random vectors}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. A \emph{random vector} $\vf{X}$ is a function $\vf{X}=(X_1,\ldots,X_n):\Omega\rightarrow\RR^n$ satisfying for all $B\in\mathcal{B}(\RR^n)$: $$\{\vf{X}\in B\}=\{\omega\in\Omega:\vf{X}(\omega)\in B\}\in\mathcal{A}$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. $\vf{X}=(X_1,\ldots,X_n):\Omega\rightarrow\RR^n$ is a random vector if and only if $X_i:\Omega\rightarrow\RR$ is a random variable for $i=1,\ldots,n$.
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be a random vector. For all $B_1\times\cdots\times B_n\in\mathcal{B}(\RR^n)$, we have that: $$\{\vf{X}\in B_1\times\cdots\times B_n\}=\{X_1\in B_1\}\cap\cdots\cap\{X_n\in B_n\}$$ We will denote: $$\{X_1\in B_1,\ldots,X_n\in B_n\}:=\{X_1\in B_1\}\cap\cdots\cap\{X_n\in B_n\}$$
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be a random vector. Then, the \emph{distribution} of $\vf{X}$ is the function:
    $$
      \function{\Prob_{\vf{X}}}{\mathcal{B}(\RR^n)}{[0,1]}{B}{\Prob(\vf{X}\in B)}
    $$
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be a random vector. We say that $\vf{X}$ is \emph{discrete} if there exists a finite or countable subset $S\subset\RR^n$ such that $\Prob(\vf{X}\in S)=1$\footnote{By agreement, we will suppose that $S$ only contains points $x$ such that $\Prob(\vf{X}=x)>0$.}. In that case, $S$ is called the \emph{support} of $\vf{X}$\footnote{In general we will denote $S$ by $S_{\vf{X}}$. Moreover, note that $S_{\vf{X}}=S_{X_1}\times\cdots\times S_{X_n}$, where $S_{X_i}$ is the support of the random variable $X_i$.}.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be a random vector. Then, $\vf{X}$ is discrete if and only if $X_i$ is a discrete random variable for $i=1,\ldots,n$.
  \end{proposition}
  \begin{definition}[Joint probability mass function]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be a discrete random vector. Then, the \emph{joint probability mass function} (\emph{joint pmf}) of $\vf{X}$ is:
    $$
      \function{p_{\vf{X}}}{S_{X_1}\times\cdots\times S_{X_n}}{[0,1]}{(x_1,\ldots,x_n)}{\Prob(X_1=x_1,\ldots,X_n=x_n)}
    $$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}$ be a discrete random vector. Then, the joint pmf of $\vf{X}$ determines the distribution of $\vf{X}$.
  \end{proposition}
  \begin{definition}[Marginal probability mass functions]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be a discrete random vector with support $S_{\vf{X}}=S_{X_1}\times\cdots\times S_{X_n}$. Then, the \emph{marginal probability mass functions} (\emph{marginal pmf}) of $\vf{X}$ are:
    \begin{align*}
      p_{X_i}(x_i) & =\Prob(X_i=x_i)                 \\
                   & =\sum_{\substack{y_j\in S_{X_j} \\j\ne i}}p_{\vf{X}}(y_1,\ldots,y_{i-1},x_i,y_{i+1},\ldots,y_n)
    \end{align*}
    for $i=1,\ldots,n$.
  \end{definition}
  \begin{definition}[Multinomial distrbution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A_1,\ldots,A_r\in\mathcal{A}$. Suppose $\Prob(A_i)=p_i$ for $i=1,\ldots,r$ such that $p_1+\cdots+p_r=1$. The \emph{multinomial distribution} is the one whose $i$-th random variable $X_i$ is the number of successes of $A_i$ in a sequence of $n$ repetitions, for $i=1,\ldots,r$, that is, $X_i\sim\text{B}(n,p_i)$. Thus, for all $n_1,\ldots,n_r\in\{0,1,\ldots,n\}$ such that $n_1+\cdots+n_r=n$ we have: $$\Prob(X_1=n_1,\ldots,X_r=n_r)=\frac{n!}{n_1!\cdots n_r!} {p_1}^{n_1}\cdots{p_r}^{n_r}$$ If $\vf{X}=(X_1,\ldots,X_r)$ follows a multinomial distribution of parameters $n$ and $p_1,\ldots,p_r$, we will write $\vf{X}\sim \text{Mult}(n;p_1,\ldots,p_r)$.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be a random vector. We say that $\vf{X}$ is \emph{absolutely continuous} if exists a function $f:\RR^n\rightarrow\RR$ such that:
    \begin{enumerate}
      \item $f(x)\geq 0$, $\forall x\in\RR^n$.
      \item \hfill $$\int_{-\infty}^{+\infty}\overset{(n)}{\cdots}\int_{-\infty}^{+\infty} f(x_1,\ldots,x_n)\dd{x_1}\cdots\dd{x_n}=1$$
      \item For all $B\in\mathcal{B}(\RR^n)$ we have: $$\Prob(\vf{X}\in B)=\int_Bf(x)\dd{x}$$
    \end{enumerate}
    The function $f$, denoted by $f_{\vf{X}}$, is called \emph{joint probability density function} (\emph{joint pdf}) of $\vf{X}$.
  \end{definition}
  \begin{proposition}[Marginal probability density functions]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector with density $f_{\vf{X}}$. Then, $X_i$ is an absolutely random variable with pdf:
    \begin{multline*}
      f_{X_i}(x_i)=\int_{-\infty}^{+\infty}\overset{(n-1)}{\cdots}\int_{-\infty}^{+\infty} f(x_1,\ldots,x_n)\dd{x_1}\cdots\dd{x_{i-1}}\cdot\\\cdot\dd{x_{i+1}}\cdots\dd{x_n}
    \end{multline*}
    for $i=1,\ldots, n$. These functions $f_{X_i}$ are called \emph{marginal probability density functions} (\emph{marginal pdf}) of $\vf{X}$.
  \end{proposition}
  \begin{definition}[Multivariate standard normal distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector. We say that $\vf{X}$ follows a \emph{multivariate normal distribution}, denoted by $\vf{X}\sim N(0,1)$, if $\vf{X}$ has a joint pdf: $$f_{\vf{X}}(x_1,\ldots,x_n)=\frac{1}{{(2\pi)}^{\frac{n}{2}}}\exp{-\frac{{x_1}^2+\cdots+{x_n}^2}{2}}$$ Moreover, $X_i\sim N(0,1)$ for $i=1,\ldots,n$.
  \end{definition}
  \begin{definition}[Multivariate uniform distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector. We say that $\vf{X}$ has a \emph{multivariate uniform distribution} over $B\in\mathcal{B}(\RR^n)$, with $\vol(B)<\infty$, if it has joint pdf: $$f_{\vf{X}}(x)=\frac{1}{\vol(B)}\indi{B}(x)$$ If $\vf{X}=(X_1,\ldots,X_r)$ follows a multivariate uniform, we will write $\vf{X}\sim U(B)$.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be a random vector. The \emph{multivariate cumulative distribution function} (\emph{multivariate cdf}) of $\vf{X}$ is defined as: $$F_{\vf{X}}(x_1,\ldots,x_n):=\Prob(X_1\leq x_1,\ldots,X_n\leq x_n)$$ for $(x_1,\ldots,x_n)\in\RR^n$.
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}$ be a random vector. Then, $F_{\vf{X}}$ determines the distribution of $\vf{X}$.
  \end{theorem}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be a random vector. Then, the multivariate cdf $F_{\vf{X}}$ of $\vf{X}$ has the following properties:
    \begin{enumerate}
      \item It is monotonically increasing in each of its variables\footnote{For the 2-dimensional case, we have that for all $x<x'$ and $y<y'$: $$F_{\vf{X}}(x',y')-F_{\vf{X}}(x,y')-F_{\vf{X}}(x',y)+F_{\vf{X}}(x,y)\geq 0$$ This positive quantity is called \emph{increment} of $F_{\vf{X}}$ in the rectangle $(x,x']\times(y,y']$. In general a function $f:\RR^2\rightarrow\RR$ satisfying $$f(x',y')-f(x,y')-f(x',y)+f(x,y)\geq 0\qquad\forall x<x'\text{ and }\forall y<y'$$ is said to be \emph{increasing}.}.
      \item It is right-continuous in each of its variables.
      \item For all $i=1,\ldots,n$ we have:
            \begin{gather*}
              \lim_{x_i\to-\infty}F_{\vf{X}}(x_1,\ldots,x_n)=0\\
              \lim_{x_1,\ldots,x_n\to+\infty}F_{\vf{X}}(x_1,\ldots,x_n)=1
            \end{gather*}
      \item For all $i=1,\ldots,n$ we have: $$\lim_{x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_n\to+\infty}F_{\vf{X}}(x_1,\ldots,x_n)=F_{X_i}(x_i)$$
      \item If $\vf{X}$ is absolutely continuous, then:
            \begin{multline*}
              F_{\vf{X}}(x_1,\ldots,x_n)=\\=\int_{-\infty}^{x_1}\cdots\int_{-\infty}^{x_n} f_{\vf{X}}(s_1,\ldots,s_n)\dd{s}_1\cdots\dd{s}_n
            \end{multline*}
      \item If $\vf{X}$ is absolutely continuous, then:
            \begin{multline*}
              \Prob(a_1<X_1<b_1,\ldots,a_n<X_n<b_n)=\\=\int_{a_1}^{b_1}\cdots\int_{a_n}^{b_n} f_{\vf{X}}(s_1,\ldots,s_n)\dd{s}_1\cdots\dd{s}_n
            \end{multline*}
    \end{enumerate}
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector. If $\vf{X}$ has a continuous joint pdf, then: $$f_{\vf{X}}(x_1,\ldots,x_n)=\frac{\partial^nF_{\vf{X}}}{\partial x_1\cdots\partial x_n}(x_1,\ldots,x_n)$$
  \end{proposition}
  \subsubsection{Transformations of random vectors}
  \begin{definition}
    We say that a function $\vf{h}:\RR^n\rightarrow\RR^m$ is \emph{Borel measurable} if $\forall B\in\mathcal{B}(\RR^m)$ we have: $$\vf{h}^{-1}(B)\in\mathcal{B}(\RR^n)$$
  \end{definition}
  \begin{proposition}
    Let $\vf{h}:\RR^n\rightarrow\RR^m$ be a continuous function. Then, $\vf{h}$ is Borel measurable.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}=(X_1,\ldots,X_n)$ be a random vector and $\vf{h}:\RR^n\rightarrow\RR^m$ be a Borel measurable function. Then, $\vf{Y}:=\vf{h}(\vf{X})$ is also a random vector.
  \end{proposition}
  \begin{proposition}\label{P:transRV}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $U,V\subseteq\RR^n$ be open sets and $\vf{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector with joint pdf $f_{\vf{X}}$ such that $\Prob(\vf{X}\in U)=1$. Let $\vf{h}:U\rightarrow V$ be a diffeomorphism of class $\mathcal{C}^1$. Then, $\vf{Y}:=\vf{h}(\vf{X})$ is absolutely continuous and $$f_{\vf{Y}}(y)=f_{\vf{X}}(\vf{h}^{-1}(y))|J\vf{h}^{-1}(y)|\indi{V}(y)$$
    where $J\vf{h}^{-1}(y)=\det\vf{D}\vf{h}^{-1}(y)$ is the Jacobian of $\vf{h}^{-1}$ evaluated at $y$.
  \end{proposition}
  \begin{definition}[Multivariate normal distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}\sim N(0,1)$ be an absolutely continuous random vector and $\vf{h}:\RR^n\rightarrow\RR^n$ be a function defined as $$\vf{h}(\vf{x})=\vf{A}\vf{x}+\vf{b}\qquad\vf{x}\in\RR^n\footnote{Here, we have thought $(x_1,\ldots,x_n)$ as the vector $\vf{x}$ in $\RR^n$.}$$ where $\vf{A}\in\GL_n(\RR)$ and $\vf{b}\in\RR^n$. Then, $\vf{Y}=\vf{h}(\vf{X})$ is an absolutely continuous random vector and it has joint pdf: $$f_{\vf{Y}}(\vf{y})=\frac{1}{{(2\pi)}^{\frac{n}{2}}}\frac{1}{\det \vf{A}}\exp{-\frac{{\|\vf{A}^{-1}(\vf{y}-\vf{b})\|}^2}{2}}$$
    In this case, we write $\vf{Y}\sim N(\vf{b},\vf{A}\transpose{\vf{A}})$. The vector $\vf{b}$ is called \emph{mean vector} and the matrix $\vf{A}\transpose{\vf{A}}$, \emph{covariance matrix}.
  \end{definition}
  \subsubsection{Independent random variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be random variables. We say that they are \emph{independent} if $\forall B_1,\ldots,B_n\in\mathcal{B}(\RR)$, we have: $$\Prob(X_1\in B_1,\ldots,X_n\in B_n)=\prod_{i=1}^n\Prob(X_i\in B_i)$$ That is, the events $\{X_1\in B_1\},\ldots,\{X_n\in B_n\}$ are independent. Moreover if they have the same distribution, we will say that $X_1,\ldots,X_n$ are \emph{independent and identically distributed} (abbreviated as \emph{i.i.d.}).
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X_1,\ldots,X_n$ be independent random variables and $g_1,\ldots,g_n:\RR\rightarrow\RR$ be Borel measurable functions. Then, $Y_1=g_1(X_1),\ldots,Y_n=g_n(X_n)$ are also independent random variables.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be random variables. Then, $X_1,\ldots,X_n$ are independent if and only if $$F_{(X_1,\ldots,X_n)}(x_1,\ldots,x_n)=F_{X_1}(x_1)\cdots F_{X_n}(x_n)$$ for all $(x_1,\ldots,x_n)\in\RR^n$.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be a discrete random vector with support $S_{\vf{X}}$. Then, $X_1,\ldots,X_n$ are independent if and only if $$p_{\vf{X}}(x_1,\ldots,x_n)=p_{X_1}(x_1)\cdots p_{X_n}(x_n)$$ for all $(x_1,\ldots,x_n)\in S_{\vf{X}}$.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vf{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector. Then, $X_1,\ldots,X_n$ are independent if and only if $$f_{\vf{X}}(x_1,\ldots,x_n)=f_{X_1}(x_1)\cdots f_{X_n}(x_n)$$ for all $(x_1,\ldots,x_n)\in \RR^n$, except for, maybe, a null set.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be absolutely continuous and independent random variables. Then, $\vf{X}:=(X_1,\ldots,X_n)$ is an absolutely continuous random vector and $$f_{\vf{X}}(x_1,\ldots,x_n)=f_{X_1}(x_1)\cdots f_{X_n}(x_n)$$ for all $(x_1,\ldots,x_n)\in \RR^n$, except for, maybe, a null set.
  \end{proposition}
  \subsubsection{Conditional distributions}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X,Y)$ be a discrete random vector with support $S_X\times S_Y$ and $y\in S_Y$. The \emph{conditional probability mass function} of $X$ given $Y=y$ is defined as: $$p_{X\mid Y}(x\mid y):=\Prob(X=x\mid Y=y)=\frac{p_{(X,Y)}(x,y)}{p_Y(y)}$$ for all $x\in S_X$.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X,Y)$ be a discrete random vector with support $S_X\times S_Y$. Then, the pmf of $Y$ together with the pmf of $X$ conditioned to $Y=y$ determine the pmf of $X$ in the following way: $$\Prob(X=x)=\sum_{y\in S_Y}p_{X\mid Y}(x\mid y)p_Y(y)\quad\forall x\in S_X$$
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X,Y)$ be an absolutely continuous random vector and $y\in S_Y$. The \emph{conditional probability density function} of $X$ given $Y=y$ is defined as: $$f_{X\mid Y}(x\mid y):=
      \begin{cases}
        \frac{f_{(X,Y)}(x,y)}{f_Y(y)} & \text{if }f_Y(y)>0 \\
        a                             & \text{if }f_Y(y)=0
      \end{cases}
    $$ where $x\in\RR$ and $a\in\RR$ is an arbitrary value\footnote{Usually chosen equal to 0.}.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X,Y)$ be an absolutely continuous random vector, $y\in S_Y$ and $a,b\in\RR\cup\{\pm\infty\}$ such that $a<b$. Then: $$\Prob(X\in(a,b)\mid Y=y)=\int_a^bf_{X\mid Y}(x\mid y)\dd{x}$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X,Y)$ be an absolutely continuous random vector. Then, the pdf of $Y$ together with the pdf of $X$ conditioned to $Y=y$ determine the pdf of $X$ in the following way: $$f_X(x)=\int_{-\infty}^{+\infty}f_{X\mid Y}(x\mid y)f_Y(y)\dd{y}\quad\forall x\in\RR$$
  \end{proposition}
  \subsection{Expectation}\label{P:exp}
  \subsubsection{Expectation of simple random variables}
  \begin{definition}
    A \emph{simple random variable} is a random variable that takes a finite or countable number of values\footnote{Note that a simple random variable is a particular case of a discrete random variable.}.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a simple random variable whose outcomes are $\{x_i:i\in I\}$\footnote{Note that we can write $X$ as $X=\sum_{i\in I}x_i\indi{\{X=x_i\}}$, where the events $\{X=x_i\}$, $i\in I$, form a partition of $\Omega$.}, where $I$ is a finite or countable index set. We say that $X$ has \emph{finite expectation} or that it is \emph{integrable} if: $$\sum_{i\in I}|x_i|\Prob(X=x_i)<\infty$$
    If so, we define the \emph{expectation} of $X$ as: $$\Exp_\text{s}(X):=\sum_{i\in I}x_i\Prob(X=x_i)$$ If the series of above is not absolutely convergent, we will say that $X$ is \emph{not integrable}.
  \end{definition}
  \begin{lemma}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and suppose that a random variable $X$ can be expressed as $$X=\sum_{n=1}^Na_n\indi{A_n}$$ where $N\in\NN\cup\{\infty\}$, $\{A_n:n=1,\ldots,N\}\subseteq\mathcal{A}$ is a partition of $\Omega$ and $\{a_n:n=1,\ldots,N\}$ are not necessarily distinct values. Then, $X$ has finite expectation if and only if $$\sum_{n=1}^N|a_n|\Prob(A_n)<\infty$$ and in that case, $\Exp_\text{s}(X)=\sum_{n=1}^Na_n\Prob(A_n)$.
  \end{lemma}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be simple and integrable random variables. Then, $X+Y$ is also a simple and integrable random variable and: $$\Exp_\text{s}(X+Y)=\Exp_\text{s}(X)+\Exp_\text{s}(Y)$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a simple and integrable random variable and $c\in\RR$. Then, $cX$ is also a simple and integrable random variable and: $$\Exp_\text{s}(cX)=c\Exp_\text{s}(X)$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a simple random variable such that $|X|\leq C$ for some $C\in\RR$. Then, $X$ is integrable and: $$\abs{\Exp_\text{s}(X)}\leq C$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable. For all $n\in\NN$ and all $\omega\in\Omega$ $\exists! k\in\ZZ$ such that $X(\omega)\in\left[\frac{k}{2^n},\frac{k+1}{2^n}\right)$. We define $X_n(\omega):=\frac{k}{2^n}$. Then, $$X_n=\sum_{k\in\ZZ}\frac{k}{2^n}\indi{\left\{\frac{k}{2^n}\leq X< \frac{k+1}{2^n}\right\}}$$ is a simple random variable such that $X_n(\omega)\leq X(\omega)$ $\forall n\in\NN$ and $\omega\in \Omega$\footnote{The partition $\displaystyle\RR=\bigsqcup_{k\in\ZZ}\left[\frac{k}{2^n},\frac{k+1}{2^n}\right)$ is called \emph{dyadic partition of order $n$}.}.
  \end{proposition}
  \subsubsection{Extension of the expectation}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable. Then, there exists a sequence $(X_n)$ of simple random variables that converges uniformly to $X$, that is: $$\lim_{n\to\infty}\sup\{|X_n(\omega)-X(\omega)|:\omega\in\Omega\}=0$$ Furthermore, $\forall\omega\in\Omega$ and $\forall n\in\NN$ we have that $X_n(\omega)\leq X_{n+1}(\omega)$.
  \end{proposition}
  \begin{theorem}\label{P:theorem-exp}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable such that there exists a sequence $(X_n)$ of simple and integrable random variables that converges uniformly to $X$. Then, the following statements are satisfied:
    \begin{enumerate}
      \item The limit $\displaystyle\lim_{n\to\infty}\Exp_\text{s}(X_n)$ exists.
      \item The limit $\displaystyle\lim_{n\to\infty}\Exp_\text{s}(X_n)$ does not depend on the sequence $(X_n)$.
      \item If $X$ is simple, then $\displaystyle\Exp_\text{s}(X)=\lim_{n\to\infty}\Exp_\text{s}(X_n)$.
    \end{enumerate}
  \end{theorem}
  \begin{definition}[Expectation]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable. We say that $X$ has \emph{finite expectation} or that it is \emph{integrable} if there exists a sequence $(X_n)$ of simple and integrable random variables that converges uniformly to $X$. In that case, we define the \emph{expectation} of $X$ as: $$\Exp(X):=\lim_{n\to\infty}\Exp_\text{s}(X_n)\footnote{\mcref{P:theorem-exp} proves that this definition is well-defined.}$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be random variables and $c\in\RR$. Then:
    \begin{enumerate}
      \item If $X$ and $Y$ are integrable, then $X+Y$ is also integrable and: $$\Exp(X+Y)=\Exp(X)+\Exp(Y)$$
      \item If $X$ is integrable, then $cX$ is also integrable and: $$\Exp(cX)=c\Exp(X)$$
      \item If $|X|\leq C$ for some $C\in \RR$, then $X$ is integrable.
      \item If $X$ is integrable and $X\geq 0$, then $\Exp(X)\geq 0$.
      \item If $X$ and $Y$ are integrable and $Y\geq X$, then $\Exp(Y)\geq \Exp(X)$.
      \item If $m\leq X\leq M$ for some $m, M\in\RR$, then $X$ is integrable and $$m\leq \Exp(X)\leq M$$
      \item \emph{Comparison test}: If $X$ is integrable and $|Y|\leq X$, then $Y$ is integrable.
      \item $X$ is integrable $\iff |X|$ is integrable.
      \item If $\Prob(A)=0$ for some $A\in\mathcal{A}$, then for any random variable $X$, we have that $X\indi{A}$ is integrable and $\Exp(X\indi{A})=0$.
      \item If $X\overset{\text{a.s.}}{=}Y$ and one of them is integrable, then so will be the other one and, furthermore, $\Exp(X)=\Exp(Y)$.
    \end{enumerate}
  \end{proposition}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a random variable and $A\in\mathcal{A}$. Then, $\Exp(\indi{\{X\in A\}})=\Prob(X\in A)$.
  \end{corollary}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable with support $\NN\cup\{0\}$. Then: $$\Exp(X)=\sum_{k=0}^\infty\Prob(X>k)$$
  \end{proposition}
  \begin{proof}
    \begin{multline*}
      \sum_{k=0}^\infty\Prob(X>k)=\sum_{k=0}^\infty\sum_{n=k+1}^{\infty}\Prob(X=n)=\\=\sum_{n=1}^\infty\sum_{k=0}^{n-1}\Prob(X=n)=\sum_{n=1}^\infty n\Prob(X=n)= \Exp(X)
    \end{multline*}
  \end{proof}
  \begin{theorem}[Monotone convergence theorem]\label{P:monotone}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be an increasing sequence of non-negative random variables such that $\displaystyle\lim_{n\to\infty}X_n\overset{\text{a.s.}}{=}X$\footnote{See \mcref{P:as-conv}.}, for some random variable $X$. Then: $$\lim_{n\to\infty} \Exp(X_n)=\Exp(X)$$
  \end{theorem}
  \begin{sproof}
    Check the proof of \mnameref{RFA:monotone}.
  \end{sproof}
  \begin{theorem}[Dominated convergence theorem]\label{P:dominated}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be sequence of random variables such that $\displaystyle\lim_{n\to\infty}X_n\overset{\text{a.s.}}{=}X$, for some random variable $X$. Suppose that there exists an integrable random variable $Y$ such that $$|X_n|\leq Y\quad\forall n\geq 1$$ Then: $$\lim_{n\to\infty} \Exp(X_n)=\Exp(X)$$
  \end{theorem}
  \begin{sproof}
    Check the proof of \mnameref{RFA:dominated}.
  \end{sproof}
  \begin{theorem}[Fatou's lemma]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of non-negative random variables. Then: $$\Exp(\liminf_{n\to\infty}X_n)\leq \liminf_{n\to\infty}\Exp(X_n)$$
  \end{theorem}
  \begin{sproof}
    Check the proof of \mnameref{RFA:fatou}.
  \end{sproof}
  \subsubsection{Expectation of absolutely continuous random variables}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be an absolutely continuous random variable with density $f_X$. Then, $X$ is integrable if and only if $$\int_{-\infty}^{+\infty}|x|f_X(x)\dd{x}<\infty$$
    In that case, we have: $$\Exp(X)=\int_{-\infty}^{+\infty}xf_X(x)\dd{x}<\infty$$
  \end{theorem}
  \subsubsection{Expectation of transformations of random vectors}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}=(X_1,\ldots,X_n)$ be a discrete random vector with support $S_{\vf{X}}$ and $h:\RR^n\rightarrow\RR$ be a function. Then, $Y:=h(\vf{X})$ is an integrable random variable if and only if $$\sum_{(x_1,\ldots,x_n)\in S_{\vf{X}}}|h(x_1,\ldots,x_n)|\Prob(X_1=x_1,\ldots,X_n=x_n)<\infty$$
    In that case, we have:
    \begin{multline*}
      \Exp(Y)=\\=\sum_{(x_1,\ldots,x_n)\in S_{\vf{X}}}h(x_1,\ldots,x_n)\Prob(X_1=x_1,\ldots,X_n=x_n)
    \end{multline*}
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector with density $f_{\vf{X}}$ and $h:\RR^n\rightarrow\RR$ be a Borel measurable function. Then, $Y:=h(\vf{X})$ is an integrable random variable if and only if $$\int_{\RR^n}|h(x_1,\ldots,x_n)|f_{\vf{X}}(x_1,\ldots,x_n)\dd{x_1}\cdots\dd{x_n}<\infty$$
    In that case, we have:
    $$
      \Exp(Y)=\int_{\RR^n}h(x_1,\ldots,x_n)f_{\vf{X}}(x_1,\ldots,x_n)\dd{x_1}\cdots\dd{x_n}$$
  \end{proposition}
  \subsubsection{Expectation of non-negative and mixed random variables}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X\geq 0$ be a non-negative random variable and $(X_n)\geq 0$ be a sequence of simple random variables that converges uniformly to $X$. Then:
    \begin{itemize}
      \item If $X_n$ is integrable $\forall n\in\NN$, then: $$\Exp(X)=\lim_{n\to\infty}\Exp(X_n)$$
      \item If $\exists m\in\NN$ such that $X_m$ isn't integrable, then: $$\Exp(X)=\infty$$
    \end{itemize}
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a mixed random variable. Suppose that the discrete part of $X$ has support $S_X$ and ``pmf'' $p_X(x)$\footnote{We write pmf in quotation marks because $p_X(x)$ is not exactly a probability mass function since $\sum_{x\in S_X}p_X(x)<1$.}. Moreover, suppose that the absolutely continuous part of $X$ has ``pdf'' $f_X(x)$\footnote{Again, we write pdf in quotation marks because $f_X(x)$ is not exactly a probability density function since $\int_{-\infty}^{+\infty}f_X(x)\dd{x}<1$.} We say that $X$ has \emph{finite expectation} if: $$\sum_{x\in S}|x|p_X(x)+\int_{-\infty}^{+\infty}|x|f_X(x)\dd{x}<\infty$$
    If so, we define the \emph{expectation} of $X$ as: $$\Exp(X):=\sum_{x\in S_X}xp_X(x)+\int_{-\infty}^{+\infty}xf_X(x)\dd{x}$$
  \end{definition}
  \subsubsection{Moments}
  \begin{definition}[Moment]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a random variable and $k\in\NN$. We say that $X$ has \emph{finite moment of order $k$} (or \emph{finite $k$-th moment}) if $X^k$ has finite expectation. We denote by $\mu_k$ the $k$-th moment of $X$: $$\mu_k=\Exp(X^k)$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$, $Y$ be random variables such that they have finite $k$-th moment. Then:
    \begin{enumerate}
      \item $X$ and $Y$ have finite $r$-th moment $\forall r\in\{1,\ldots,k\}$.
      \item $X+Y$ has finite $k$-th moment.
    \end{enumerate}
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$, $Y$ be random variables such that they have finite $2k$-th moment. Then, $XY$ has finite $k$-th moment.
  \end{proposition}
  \begin{theorem}[Cauchy-Schwarz inequality]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be two random variables such that $\Exp(X^2)<\infty$. Then: $$\Exp(|XY|)\leq {\left(\Exp(X^2)\Exp(Y^2)\right)}^{1/2}$$
  \end{theorem}
  \begin{definition}[Variance]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable such that $\Exp(X^2)<\infty$. We define the \emph{variance} of $X$ as: $$\Var(X):=\Exp\left({(X-\Exp(X))}^2\right)\geq 0$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable such that $\Exp(X^2)<\infty$. Then: $$\Var(X)=\Exp(X^2)-{\Exp(X)}^2$$
  \end{proposition}
  \begin{definition}[Standard deviation]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable such that $\Exp(X^2)<\infty$. We define the \emph{standard deviation} (or \emph{standard error}) of $X$ as: $$\sigma(X):=\sqrt{\Var(X)}\footnote{Therefore, $\Var(X)$ is sometimes expressed as $\sigma^2(X)$.}$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable such that $\Exp(X^2)<\infty$. If $\Var(X)=0$, then $X\overset{\text{a.s.}}{=}\Exp(X)$.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be independent random variables with finite expectation. Then, $XY$ has finite expectation and: $$\Exp(XY)=\Exp(X)\Exp(Y)$$
  \end{proposition}
  \begin{definition}[Covariance]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be two random variables with finite 2nd moments. We define the \emph{covariance} between $X$ and $Y$ as: $$\cov(X,Y):=\Exp\left([X-\Exp(X)][Y-\Exp(Y)]\right)$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be two random variables with finite 2nd moments. Then: $$\cov(X,Y)=\Exp(XY)-\Exp(X)\Exp(Y)$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be independent random variables with finite expectation. Then, $\cov(X,Y)$ is well-defined and $\cov(X,Y)=0$.
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that two random variables $X$, $Y$ are \emph{uncorrelated} if $\cov(X,Y)=0$.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a random variable such that $\Exp(X^2)<\infty$ and $a,b\in\RR$. Then: $$\Var(aX+b)=a^2\Var(X)$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be random variables with finite 2nd moments. Then:
    $$\Var(X_1+\cdots+X_n)=\sum_{i=1}^n\Var(X_i)+2\sum_{1\leq i<j\leq n}\cov(X_i,X_j)$$
  \end{proposition}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be random variables with finite 2nd moments such that they are pairwise uncorrelated. Then:
    $$\Var(X_1+\cdots+X_n)=\sum_{i=1}^n\Var(X_i)$$
  \end{corollary}
  \begin{definition}[Pearson correlation coefficient]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be non-constant random variables with finite 2nd moments. We define the \emph{Pearson correlation coefficient} (or simply \emph{correlation coefficient})\footnote{The correlation coefficient measures the linear correlation between two random variables.} as: $$\rho(X,Y):=\frac{\cov(X,Y)}{\sigma(X)\sigma(Y)}=\frac{\cov(X,Y)}{\sqrt{\Var(X)}\sqrt{\Var(Y)}}$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$, $Y$ be non-constant random variables with finite 2nd moments and $\rho:=\rho(X,Y)$. Then:
    \begin{enumerate}
      \item $|\rho|\leq 1$
      \item If $\rho=1$, then $\exists a,b\in\RR$ with $a>0$ such that $Y=aX+b$.
      \item If $\rho=-1$, then $\exists a,b\in\RR$ with $a<0$ such that $Y=aX+b$.
    \end{enumerate}
  \end{proposition}
  \begin{theorem}[Markov's inequality]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X\geq 0$ be a non-negative random variable with finite expectation and $\lambda\in\RR_{>0}$. Then:
    $$\Prob(X>\lambda)\leq \Prob(X\geq \lambda)\leq \frac{\Exp(X)}{\lambda}$$
  \end{theorem}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X\geq 0$ be a non-negative random variable such that $\Exp(X)=0$. Then, $X\overset{\text{a.s.}}{=}0$.
  \end{corollary}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $p\in\RR_{>0}$ and $X$ be a random variable such that $\Exp({|X|}^p)<\infty$. Then, for all $a\in\RR_{>0}$: $$\Prob(|X|\geq a)\leq \frac{\Exp({|X|}^p)}{a^p}$$
  \end{corollary}
  \begin{corollary}[Chebyshev's inequality]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a random variable such that $\Exp(X^2)<\infty$ and $\delta\in\RR_{>0}$. Then: $$\Prob(|X-\Exp(X)|\geq\delta)\leq\frac{\Var(X)}{\delta^2}$$ Furthermore, if $\sigma:=\sigma(X)$ and we take $\delta=k\sigma$, $k\in\RR_{>0}$, then: $$\Prob(|X-\Exp(X)|<k\sigma)\geq1-\frac{1}{k^2}$$
  \end{corollary}
  \begin{definition}[Moment-generating function]\label{P:moment-generating}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable. The \emph{moment-generating function} of $X$ is the function $\psi_X$ defined as:
    $$\function{\psi_X}{\RR}{\RR_{>0}\cup\{+\infty\}}{t}{\Exp(\exp{tX})}$$
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$, $Y$ be random variables such that $\psi_X(t),\psi_Y(t)<+\infty$ in a neighbourhood of 0 and such that $\psi_X$, $\psi_Y$ are equal in another neighbourhood of 0. Then, $X\overset{\text{d}}{=}Y$.
  \end{theorem}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable such that $\psi_X(t)<+\infty$ in a neighbourhood of 0. Then, $X$ has moment of order $k$ $\forall k\in\NN$ and: $$\Exp(X^k)={\psi_X}^{(k)}(0)$$
  \end{theorem}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$, $Y$ be independent random variables. Then, for all $t$ such that $\psi_X(t),\psi_Y(t)<+\infty$, the function $\psi_{X+Y}(t)$ is finite and: $$\psi_{X+Y}(t)=\psi_X(t)\psi_Y(t)$$
  \end{theorem}
  \begin{proof}
    $$\psi_{X+Y}(t)=\Exp(\exp{t(X+Y)})=\Exp(\exp{tX})\Exp(\exp{tY})=\psi_X(t)\psi_Y(t)$$
  \end{proof}
  where the second equality is because of the independence of $X$ and $Y$.
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $p\in(0,1)$, $\lambda\in\RR_{>0}$ and $X_1,\ldots,X_n$ be \iid random variables.
    \begin{itemize}
      \item If $X_1\sim\text{Ber}(p)$, then $X_1+\cdots+X_n\sim\text{B}(n,p)$
      \item If $X_1\sim\text{Geo}(p)$, then $X_1+\cdots+X_n\sim\text{NB}(n,p)$
      \item If $X_1\sim\text{Exp}(\lambda)$, then $X_1+\cdots+X_n\sim\text{Gamma}(n,\lambda)$
    \end{itemize}
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_i\sim\text{Pois}(\lambda_i)$ be random variables for some $\lambda_i>0$, $i=1,\ldots,n$. Suppose that $X_1,\ldots,X_n$ are independent. Then:
    $$X_1+\cdots+X_n\sim\text{Pois}(\lambda_1+\cdots+\lambda_n)$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\mu_i\in\RR$ and $\sigma_i\in\RR$ for $i=1,\ldots,n$. Let $X_i\sim N(\mu_i,{\sigma_i}^2)$ be independent random variables for $i=1,\ldots,n$. Then: $$X_1+\cdots+X_n\sim N(\mu_1+\cdots+\mu_n,{\sigma_1}^2+\cdots+{\sigma_n}^2)$$
  \end{proposition}
  \subsubsection{Conditional expectation}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X,Y)$ be a discrete random vector with support $S_X\times S_Y$ and $y\in S_Y$. The \emph{conditional expectation} of $X$ given $Y=y$ is defined as: $$\Exp(X\mid Y=y):=\sum_{x\in S_X}x\Prob(X=x\mid Y=y)$$ provided that the series is absolutely convergent. More generally, if $\vf{X}$ is a discrete random vector with support $S_{\vf{X}}$ and $h:S_{\vf{X}}\rightarrow\RR$ is a function, then the conditional expectation of $h(\vf{X})$ given $Y=y$ is defined as: $$\Exp(h(\vf{X})\mid Y=y):=\sum_{\vf{x}\in S_{\vf{X}}}h(\vf{x})\Prob(\vf{X}=\vf{x}\mid Y=y)$$ provided that the series is absolutely convergent.
  \end{definition}
  \begin{proposition}[Law of total expectation]\label{P:totalexp}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}$ be a discrete random vector with support $S_{\vf{X}}$, $Y$ be a random variable with support $S_Y$ and $h:S_{\vf{X}}\rightarrow\RR$ be a function. If $h(\vf{X})$ has finite expectation, then: $$\Exp(h(\vf{X}))=\sum_{y\in S_Y}\Exp(h(\vf{X})\mid Y=y)\Prob(Y=y)$$
  \end{proposition}
  \begin{proof}
    We have that:
    \begin{align*}
      \Exp(h(\vf{X})) & =\sum_{\vf{x}\in S_{\vf{X}}}h(\vf{x})\Prob(\vf{X}=\vf{x})                                  \\
                      & =\sum_{\vf{x}\in S_{\vf{X}}}\sum_{y\in S_Y}h(\vf{x})\Prob(\vf{X}=\vf{x}\mid Y=y)\Prob(Y=y) \\
                      & =\sum_{y\in S_Y}\sum_{\vf{x}\in S_{\vf{X}}}h(\vf{x})\Prob(\vf{X}=\vf{x}\mid Y=y)\Prob(Y=y) \\
                      & =\sum_{y\in S_Y}\Exp(h(\vf{X})\mid Y=y)\Prob(Y=y)
    \end{align*}
    where in the second equality we have used the \mnameref{P:totalprob} and in the third step we can rearrange the terms due to the finite expectation of $h(\vf{X})$.
  \end{proof}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X,Y)$ be an absolutely continuous random vector with support $S_X\times S_Y$ and $y\in S_Y$. The \emph{conditional expectation} of $X$ given $Y=y$ is defined as: $$\Exp(X\mid Y=y):=\int_{S_{X}}xf_{X\mid Y}(x\mid y)\dd{x}$$ provided that the integral is absolutely convergent. More generally, if $\vf{X}$ is an absolutely continuous random vector with support $S_{\vf{X}}$ and $h:S_{\vf{X}}\rightarrow\RR$ is a function, then the conditional expectation of $h(\vf{X})$ given $Y=y$ is defined as: $$\Exp(h(\vf{X})\mid Y=y):=\int_{S_{\vf{X}}}h(\vf{x})f_{\vf{X}\mid Y}(\vf{x}\mid y)\dd{\vf{x}}$$ provided that the integral is absolutely convergent.
  \end{definition}
  \begin{proposition}[Law of total expectation]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vf{X}$ be an absolutely continuous random vector with support $S_{\vf{X}}$, $Y$ be a random variable with support $S_Y$ and $h:S_{\vf{X}}\rightarrow\RR$ be a function. If $h(\vf{X})$ has finite expectation, then: $$\Exp(h(\vf{X}))=\int_{-\infty}^{+\infty}\Exp(h(\vf{X})\mid Y=y)f_Y(y)\dd{y}$$
  \end{proposition}
  \begin{sproof}
    Adapt the proof of \mnameref{P:totalexp}.
  \end{sproof}
  \begin{center}
    \def\arraystretch{1.25}
    \begin{tabular}{|c|c|c|}
      \hline
      $X$                          & $\displaystyle \Exp(X)$       & $\displaystyle \Var(X)$                     \\
      \hline
      $c\in\RR$                    & $\displaystyle c$             & 0                                           \\
      %\hline
      $U(\{x_1,\ldots,x_n\})$      & $\frac{1}{n}\sum_{i=1}^n x_i$ & $\frac{1}{n}\sum_{i=1}^n ({x_i}^2-x_i)$     \\
      %\hline
      $\text{B}(n,p)$              & $\displaystyle np$            & $\displaystyle np(1-p)$                     \\
      %\hline
      $\text{Pois}(\lambda)$       & $\displaystyle \lambda$       & $\displaystyle \lambda$                     \\
      %\hline
      $\text{Geo}(p)$              & $\displaystyle 1/p$           & $\displaystyle \frac{1-p}{p^2}$             \\
      %\hline
      $\text{HG}(N,p,n)$           & $\displaystyle np$            & $\displaystyle np(1-p)\frac{N-n}{N-1}$      \\
      %\hline
      $\text{NB}(r,p)$             & $\displaystyle \frac{r}{p}$   & $\displaystyle n\frac{1-p}{p^2}$            \\
      %\hline
      $U(a,b)$                     & $\displaystyle (a+b)/2$       & $\displaystyle {(b-a)}^2/12$                \\
      %\hline
      $\text{Exp}(\lambda)$        & $\displaystyle 1/\lambda$     & $\displaystyle 1/\lambda^2$                 \\
      %\hline
      $N(\mu,\sigma^2)$            & $\displaystyle \mu$           & $\displaystyle \sigma^2$                    \\
      %\hline
      $\text{Gamma}(\alpha,\beta)$ & $\displaystyle \alpha/\beta$  & $\displaystyle \alpha/\beta^2$              \\
      %\hline
      $\text{Beta}(a,b)$           & $\displaystyle \frac{a}{a+b}$ & $\displaystyle \frac{ab}{{(a+b)}^2(a+b+1)}$ \\
      %\hline
      $\text{C}(x_0,\gamma)$       & $\displaystyle +\infty$       & $\displaystyle +\infty$                     \\
      \hline
    \end{tabular}
    \captionof{table}{Expectations and variances of common distributions.}
  \end{center}
  \subsection{Convergence of random variables}
  \subsubsection{Convergence in probability}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. We say that $(X_n)$ \emph{converges in probability} to $X$, and we denote it by $X_n\overset{\Prob}{\longrightarrow} X$, if $\forall \varepsilon>0$ we have: $$\lim_{n\to\infty}\Prob(|X_n-X|\geq \varepsilon)=0$$ Or equivalently: $$\lim_{n\to\infty}\Prob(|X_n-X|< \varepsilon)=1$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Then, the limit in probability is unique almost surely. That is, if $(X_n)$ is a sequence of random variables and $X$, $Y$ are a random variables such that $X_n\overset{\Prob}{\longrightarrow} X$ and $X_n\overset{\Prob}{\longrightarrow} Y$, then $\Prob(X\ne Y)=0$.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables, $X$ be a random variable such that $X_n\overset{\Prob}{\longrightarrow} X$ and $f:\RR\rightarrow\RR$ be continuous function. Then, $f(X_n)\overset{\Prob}{\longrightarrow} f(X)$.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $({X_1}_n),\ldots,({X_m}_n)$ be $m$ sequences of random variables, $X_1,\ldots,X_m$ be a random variable such that ${X_i}_n\overset{\Prob}{\longrightarrow} X_i$ $\forall i=1,\ldots,m$ and $f:\RR^m\rightarrow\RR$ be continuous function. Then: $$f({X_1}_n,\ldots,{X_m}_n)\overset{\Prob}{\longrightarrow} f(X_1,\ldots,X_m)$$
  \end{proposition}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $({X_1}_n),\ldots,({X_m}_n)$ be $m$ sequences of random variables, $X_1,\ldots,X_m$ be a random variable such that ${X_i}_n\overset{\Prob}{\longrightarrow} X_i$ $\forall i=1,\ldots,m$ and $f:\RR^m\rightarrow\RR$ be continuous function. Then:
    \begin{itemize}
      \item ${X_1}_n+\cdots+{X_m}_n\overset{\Prob}{\longrightarrow}X_1+\cdots+X_m$.
      \item ${X_1}_n\cdots{X_m}_n\overset{\Prob}{\longrightarrow}X_1\cdots X_m$.
    \end{itemize}
  \end{corollary}
  \begin{lemma}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Then, the set $\mathcal{L}^0$ of all random variables of $(\Omega,\mathcal{A},\Prob)$ is a vector space. Moreover, the relation $\sim$ defined in $\mathcal{L}^0$ as $$X\sim Y\iff X\overset{\text{a.s.}}{=} Y\quad\forall X,Y\in\mathcal{L}^0$$ is an equivalence relation. The quotient set $\quot{\mathcal{L}^0}{\!\sim}$ is denoted by $L^0$\footnote{If $X\in\mathcal{L}^0$, we will use the same notation for its equivalence class.}.
  \end{lemma}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We define the function $d_\Prob$ in $L^0$ as:
    $$\function{d_\Prob}{L^0\times L^0}{\RR}{(X,Y)}{\Exp\left(\frac{|X-Y|}{1+|X-Y|}\right)}$$
    Then, $(L^0,d_\Prob)$ is a metric space.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. Then:
    $$X_n\overset{\Prob}{\longrightarrow} X\iff\lim_{n\to\infty} d_\Prob(X_n,X)=0$$
    Because of that, the convergence in probability is said to be \emph{metrizable}.
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. We say that $(X_n)$ satisfies the \emph{Cauchy condition in probability} (or is \emph{Cauchy in probability}) if $\forall \varepsilon >0$ we have: $$\lim_{n,m\to\infty}\Prob(|X_n-X_m|\geq \varepsilon)=0$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. Then:
    $$X_n\overset{\Prob}{\longrightarrow} X\iff(X_n)\text{ is Cauchy in probability}$$
    Thus, $(L^0,d_\Prob)$ is a complete metric space.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable such that $X_n\overset{\Prob}{\longrightarrow} X$. Then, all subsequence $(X_{n_k})$ of $(X_n)$ converges in probability to $X$.
  \end{proposition}
  \subsubsection{Almost surely convergence}
  \begin{definition}\label{P:as-conv}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. We say that $(X_n)$ \emph{converges almost surely} to $X$, and we denote it by $X_n\overset{\text{a.s.}}{\longrightarrow} X$, if $$\Prob\left(\lim_{n\to\infty}X_n=X\right)=1$$ That is, $$\lim_{n\to\infty}X_n(\omega)=X(\omega)$$ for all $\omega\in\Omega$ except for maybe a set of probability zero. Another equivalent expression is the following one: $X_n\overset{\text{a.s.}}{\longrightarrow} X$ if and only if $$\Prob\left(\bigcap_{\varepsilon\in\QQ_{>0}}\bigcup_{n=1}^\infty\bigcap_{k=n}^\infty\{\omega\in\Omega:|X_k(\omega)-X(\omega)|<\varepsilon\}\right)=1$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. Then, $X_n\overset{\text{a.s.}}{\longrightarrow} X$ if and only if $\forall \varepsilon>0$ we have: $$\lim_{n\to\infty}\Prob\left(\bigcup_{k=n}^\infty\{|X_k-X|\geq\varepsilon\}\right)=0$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. Then:
    $$X_n\overset{\text{a.s.}}{\longrightarrow} X\implies X_n\overset{\Prob}{\longrightarrow} X$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables, $X$ be a random variable such that $X_n\overset{\text{a.s.}}{\longrightarrow} X$ and $f:\RR\rightarrow\RR$ be continuous function. Then, $f(X_n)\overset{\text{a.s.}}{\longrightarrow} f(X)$.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. Suppose that $X_n\overset{\Prob}{\longrightarrow} X$. Then, there exists a subsequence $(X_{n_k})$ of $(X_n)$ such that $X_{n_k}\overset{\text{a.s.}}{\longrightarrow} X$.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. Suppose that $\forall\varepsilon >0$ we have: $$\sum_{n=1}^\infty\Prob(|X_n-X|\geq\varepsilon)<\infty$$
    Then, $X_n\overset{\text{a.s.}}{\longrightarrow} X$.
  \end{proposition}
  \begin{definition}
    Let $\Omega$ be a set and $(A_n)\subset\Omega$ be a sequence of subsets. We define the \emph{limit superior} of $(A_n)$ as: $$\limsup_{n\to\infty}A_n:=\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty A_k$$
    That is: $$\omega\in\limsup_{n\to\infty}A_n\iff\forall n\geq 1\ \exists k\geq n\text{ such that }\omega\in A_k$$ We can express that as:
    $$\limsup_{n\to\infty}A_n=\{\omega\in\Omega:\omega\in A_n\text{ infinitely often}\}$$
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set and $(A_n)\subset\Omega$ be a sequence of subsets. We define the \emph{limit inferior} of $(A_n)$ as: $$\liminf_{n\to\infty}A_n:=\bigcup_{n=1}^\infty\bigcap_{k=n}^\infty A_k$$
    That is: $$\omega\in\liminf_{n\to\infty}A_n\iff\exists n\geq 1\text{ such that }\forall k\geq n,\ \omega\in A_k$$ We can express that as:
    $$\liminf_{n\to\infty}A_n=\{\omega\in\Omega:\omega\in A_n\text{ eventually}\}$$
  \end{definition}
  \begin{proposition}
    Let $\Omega$ be a set and $(A_n)\subset\Omega$ be a sequence of subsets. Then:
    \begin{enumerate}
      \item $\displaystyle\liminf_{n\to\infty}A_n\subseteq\limsup_{n\to\infty}A_n$
      \item $\displaystyle{\left(\limsup_{n\to\infty}A_n\right)}^c=\liminf_{n\to\infty}{A_n}^c$
    \end{enumerate}
  \end{proposition}
  \begin{definition}
    Let $\Omega$ be a set and $(A_n)\subset\Omega$ be a sequence of subsets. We say that $(A_n)$ has \emph{limit} if: $$\liminf_{n\to\infty}A_n=\limsup_{n\to\infty}A_n$$
    In that case, $\displaystyle A:=\limsup_{n\to\infty}A_n$ is called the limit of the sequence.
  \end{definition}
  \begin{lemma}[First Borel-Cantelli lemma]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(A_n)\subset\mathcal{A}$ be a sequence of events such that: $$\sum_{n=1}^\infty\Prob(A_n)<\infty$$
    Then, $\displaystyle\Prob\left(\limsup_{n\to\infty} A_n\right)=0$.
  \end{lemma}
  \begin{lemma}[Second Borel-Cantelli lemma]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(A_n)\subset\mathcal{A}$ be a sequence of independent events such that: $$\sum_{n=1}^\infty\Prob(A_n)=\infty$$
    Then, $\displaystyle\Prob\left(\limsup_{n\to\infty} A_n\right)=1$.
  \end{lemma}
  \subsubsection{Convergence in mean}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $p\geq 1$, $(X_n)$ be a sequence of random variables such that $\Exp({|X_n|}^p)<\infty$ and $X$ be a random variable such that $\Exp({|X|}^p)<\infty$. We say that $(X_n)$ \emph{converges in the $p$-th mean} to $X$, and we denote it by $X_n\overset{L^p}{\longrightarrow} X$, if  $$\lim_{n\to\infty}\Exp({|X_n-X|}^p)=0$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $p\geq 1$, $(X_n)$ be a sequence of random variables such that $\Exp({|X_n|}^p)<\infty$ and $X$ be a random variable such that $\Exp({|X|}^p)<\infty$. Then:
    $$X_n\overset{L^p}{\longrightarrow} X\implies X_n\overset{\Prob}{\longrightarrow} X$$
  \end{proposition}
  \begin{theorem}[Dominated convergence theorem]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $p\geq 1$, $X$ be a random variable and $(X_n)$ be a sequence of random variables such that $X_n\overset{\Prob}{\longrightarrow}X$ or $X_n\overset{\text{a.s.}}{\longrightarrow}X$. Suppose that there exists a random variable $Y$ such that $|X_n|\leq Y\ \forall n\geq 1$ and $\Exp({|Y|}^p)<\infty$. Then, $X_n\overset{L^p}{\longrightarrow} X$.
  \end{theorem}
  \begin{lemma}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $p\geq 1$. Define the set $\mathcal{L}^p$ of all random variables of $(\Omega,\mathcal{A},\Prob)$ such that $\Exp({|X|}^p)<\infty$. Then, $\mathcal{L}^p$ is a vector space. Moreover, the relation $\sim$ defined in $\mathcal{L}^p$ as $$X\sim Y\iff X\overset{\text{a.s.}}{=} Y\quad\forall X,Y\in\mathcal{L}^p$$ is an equivalence relation. The quotient set $\quot{\mathcal{L}^p}{\!\sim}$ is denoted by $L^p$\footnote{If $X\in\mathcal{L}^p$, we will use the same notation for its equivalence class.}.
  \end{lemma}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $p\geq 1$. We define the function:
    $$
      \function{{\|\cdot\|}_p}{L^p}{\RR}{X}{\Exp({|X|}^p)}
    $$
    Then, $(L^p,{\|\cdot\|}_p)$ is a normed vector space. Moreover, the norm ${\|\cdot\|}_p$ induces a distance $d_p$ defined as:
    $$d_p(X,Y):={\|X-Y\|}_p\quad\forall X,Y\in L^p$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $p\geq 1$, $(X_n)\subset L^p$ be a sequence of random variables and $X\in L^p$. Then:
    $$X_n\overset{L^p}{\longrightarrow} X\iff\lim_{n\to\infty}d_p(X_n,X)=0$$
    Therefore, the convergence in $p$-th mean is metrizable.
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $p\geq 1$, $(X_n)\subset L^p$ be a sequence of random variables and $X\in L^p$. We say that $(X_n)$ satisfies the \emph{Cauchy condition in $p$-th mean} (or is \emph{Cauchy in $p$-th mean}) if: $$\lim_{n,m\to\infty}\Exp({|X_n-X_m|}^p)=0$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $p\geq 1$, $(X_n)\subset L^p$ be a sequence of random variables and $X\in L^p$. Then:
    $$X_n\overset{L^p}{\longrightarrow} X\iff(X_n)\text{ is Cauchy in $p$-th mean}$$
    Thus, $L^p$ is a Banach space and $L^2$ is a Hilbert space.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $p\geq 1$, $X,Y\in L^p$ and $(X_n),(Y_n)\subset L^p$ be sequences of random variables such that $X_n\overset{L^p}{\longrightarrow} X$ and $Y_n\overset{L^p}{\longrightarrow} Y$. Then: $$X_n+Y_n\overset{L^p}{\longrightarrow} X+Y$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X,Y\in L^2$ and $(X_n),(Y_n)\subset L^2$ be sequences of random variables such that $X_n\overset{L^2}{\longrightarrow} X$ and $Y_n\overset{L^2}{\longrightarrow} Y$. Then: $$X_nY_n\overset{L^1}{\longrightarrow} XY$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $p\geq 1$, $X\in L^p$ and $(X_n)\subset L^p$ be a sequence of random variables such that $X_n\overset{L^p}{\longrightarrow} X$. Then:
    \begin{enumerate}
      \item $\displaystyle\lim_{n\to\infty}{\|X_n\|}_p={\|X\|}_p$
      \item If $1\leq r< p$, then: $$X_n\overset{L^p}{\longrightarrow} X\implies X_n\overset{L^r}{\longrightarrow} X$$
      \item If $p=1$, then: $$\lim_{n\to\infty}\Exp(X_n)=\Exp(X)$$
    \end{enumerate}
  \end{proposition}
  \subsubsection{Convergence in distribution}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. We say that $(X_n)$ \emph{converges in distribution} to $X$, and we denote it by $X_n\overset{\text{d}}{\longrightarrow} X$, if $\forall B\in\mathcal{B}(\RR)$ such that $\Prob(X\in\Fr B)=0$ we have: $$\lim_{n\to\infty}\Prob(X_n\in B)=\Prob(X\in B)$$
  \end{definition}
  \begin{definition}
    Let $A\subseteq\RR$ be a set and $f:A\rightarrow\RR$ be a function. We denote by $C(f)$ the set of points where $f$ is continuous.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables with cdfs $F_{X_n}$ $\forall n\in\NN$ and $X$ be a random variable be a random variable with cdf $F_X$. Then: $$X_n\overset{\text{d}}{\longrightarrow} X\iff\lim_{n\to\infty} F_{X_n}(t)=F_X(t)\quad\forall t\in C(F_X)\footnote{That is, $\displaystyle X_n\overset{\text{d}}{\longrightarrow} X\iff\forall t\in\RR\text{ such that }\Prob(X=t)=0\text{ we have }\lim_{n\to\infty}\Prob(X_n\leq t)=\Prob(X\leq t)$.}$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$, $Y$ be random variables such that $X_n\overset{\text{d}}{\longrightarrow} X$ and $X_n\overset{\text{d}}{\longrightarrow} Y$. Then, $X\overset{\text{d}}{=}Y$.
  \end{proposition}
  \begin{theorem}[Skorokhod's representation theorem]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable be such that $X_n\overset{\text{d}}{\longrightarrow} X$. Then, there exists a probability space $(\Omega',\mathcal{A}',\Prob')$, and random variables $({X_n'})$ and $X$ defined on $\Omega'$ such that:
    \begin{enumerate}
      \item $X_n\overset{\text{d}}{=}{X_n'}$ $\forall n\geq 1$
      \item $X\overset{\text{d}}{=}X'$
      \item ${X_n'}\overset{\text{a.s.}}{\longrightarrow}X'$
    \end{enumerate}
  \end{theorem}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. Then, $X_n\overset{\text{d}}{\longrightarrow} X$ if and only if for any continuous and bounded function $f:\RR\rightarrow\RR$ we have: $$\lim_{n\to\infty}\Exp(f(X_n))=\Exp(f(X))$$
  \end{theorem}
  \begin{lemma}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable be such that both $(X_n)$ and $X$ take values in $\NN$. Then, $X_n\overset{\text{d}}{\longrightarrow} X$ if and only if $\forall k\in\NN$, we have: $$\lim_{n\to\infty} \Prob(X_n=k)=\Prob(X=k)$$
  \end{lemma}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables, $X$ be a random variable and $a\in\RR$. Then:
    \begin{enumerate}
      \item $X_n\overset{\Prob}{\longrightarrow} X\implies X_n\overset{\text{d}}{\longrightarrow} X$
      \item $X_n\overset{\text{d}}{\longrightarrow} a\implies X_n\overset{\Prob}{\longrightarrow} a$
    \end{enumerate}
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables, $X$ be a random variable such that $X_n\overset{\text{d}}{\longrightarrow} X$ and $f:\RR\rightarrow\RR$ be a continuous function. Then, $f(X_n)\overset{\text{d}}{\longrightarrow}f(X)$.
  \end{proposition}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. Then:
    \begin{enumerate}
      \item $X_n+a\overset{\text{d}}{\longrightarrow}X+a$
      \item $aX_n\overset{\text{d}}{\longrightarrow}aX$
    \end{enumerate}
  \end{corollary}
  \begin{theorem}[Slutsky's theorem]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$, $(Y_n)$ be sequences of random variables and $X$ be a random variable and $a\in\RR$ such that $X_n\overset{\text{d}}{\longrightarrow} X$ and $Y_n\overset{\text{d}}{\longrightarrow} a$. Then:
    \begin{enumerate}
      \item $X_n+Y_n\overset{\text{d}}{\longrightarrow} X+ a$
      \item $X_nY_n\overset{\text{d}}{\longrightarrow} aX$
      \item $\frac{X_n}{Y_n}\overset{\text{d}}{\longrightarrow} \frac{X}{a}$ provided that $a\ne 0$.
    \end{enumerate}
  \end{theorem}
  \subsection{Laws of large numbers}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of random variables. We define the sequence of partial sums $(S_n)$ as: $$S_n:=\sum_{i=1}^nX_i$$
  \end{definition}
  \subsubsection{Weak laws}
  \begin{theorem}[Weak law]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of \iid random variables with finite 2nd moment. Then: $$\frac{S_n}{n}\overset{\Prob}{\longrightarrow}\Exp(X_1)\quad\text{and}\quad\frac{S_n}{n}\overset{L^2}{\longrightarrow}\Exp(X_1)$$
  \end{theorem}
  \begin{theorem}[Weak law]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of pairwise uncorrelated random variables with finite 2nd moment. Suppose that: $$\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n\Exp(X_i)=\mu<\infty\;\,\text{and}\;\,\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n\Var(X_i)=0$$ Then: $$\frac{S_n}{n}\overset{\Prob}{\longrightarrow}\mu\quad\text{and}\quad\frac{S_n}{n}\overset{L^2}{\longrightarrow}\mu$$
  \end{theorem}
  \subsubsection{Strong laws}
  \begin{theorem}[Kolmogorov's strong law]\label{P:stronglawKolmo}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of \iid random variables.
    \begin{enumerate}
      \item If $\Exp(X_1)<\infty$, then: $$\frac{S_n}{n}\overset{\text{a.s.}}{\longrightarrow}\Exp(X_1)$$
      \item If $\Exp(X_1)=\infty$, then: $$\limsup_{n\to\infty}\frac{|S_n|}{n}\overset{\text{a.s.}}{=}+\infty$$
    \end{enumerate}
  \end{theorem}
  \begin{theorem}[Strong law]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of \iid random variables such that $\Exp({X_1}^4)<\infty$. Then: $$\frac{S_n}{n}\overset{\text{a.s.}}{\longrightarrow}\Exp(X_1)$$
  \end{theorem}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $A\in\mathcal{A}$. Let $(X_n)$ be a sequence of \iid random variables such that $X_n\sim\text{Ber}(\Prob(A))$ $\forall n\in\NN$. Then: $$\frac{S_n}{n}\overset{\text{a.s.}}{\longrightarrow}\Prob(A)$$
  \end{corollary}
  \begin{definition}
    Let $x\in[0,1)$ and $b\in\NN_{\geq 2}$. Suppose the expression of $x$ in base $b$ is $x_b=0.a_1a_2a_3\cdots$. Let $N_{x,b}(k,n)$ denote the number of times the digit $k\in\{0,1,\ldots,b-1\}$ appears in the decimal expansion of $x_b$ in the first $n$ digits. We say that $x$ is \emph{simply normal} if there exists $b\in\NN_{\geq 2}$ such that $$\lim_{n\to\infty}\frac{N_{x,b}(k,n)}{n}=\frac{1}{b}\quad\forall k\in\{0,1,\ldots,b-1\}$$
    We say that $x$ is \emph{normal} if $$\lim_{n\to\infty}\frac{N_{x,b}(k,n)}{n}=\frac{1}{b}\quad\forall k\in\{0,1,\ldots,b-1\}, \forall b\in\NN_{\geq 2}$$
  \end{definition}
  \begin{theorem}[Borel's theorem]
    All the numbers in $[0,1)$, except for a null set, are normal.
  \end{theorem}
  \subsection{Central limit theorem}
  \subsubsection{Characteristic function}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. A \emph{complex random variable} is a function $Z:\Omega\rightarrow\CC$ such that $\Re(Z)$ and $\Im(Z)$ are real random variables. Therefore, $Z$ may be written as $Z=X+\ii Y$, where $X$ and $Y$ are real random variables.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $Z=X+\ii Y$ be a complex random variable. Then\footnote{Here we have only exposed two properties of the expectation of a complex random variable but in general all the properties of the expectation that we've already seen in \mcref{P:exp} can be extended conveniently to complex random variables.}:
    \begin{enumerate}
      \item $\Exp(Z)=\Exp(X)+\ii\Exp(Y)$
      \item $\overline{\Exp(Z)}=\Exp(\overline{Z})$
      \item $|\Exp(Z)|\leq \Exp(|Z|)$
    \end{enumerate}
  \end{proposition}
  \begin{definition}[Characteristic function]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a real random variable. The \emph{characteristic function} of $X$ is the function $\varphi_X$ defined as:
    $$\function{\varphi_X}{\RR}{\CC}{t}{\Exp(\exp{\ii tX})}$$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a discrete random variable with support $S_X$. Then:
    $$\varphi_X(t)=\sum_{x\in S_X}\exp{\ii tx}\Prob(X=x)$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be an absolutely continuous random variable with density $f_X$. Then:
    $$\varphi_X(t)=\int_{-\infty}^{+\infty}\exp{\ii tx}f_X(x)\dd{x}$$
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable. Then:
    \begin{enumerate}
      \item $\varphi_X(0)=1$
      \item $\abs{\varphi_X(t)}\leq 1\ \forall t\in\RR$
      \item $\overline{\varphi_X(t)}=\varphi_X(-t)\ \forall t\in\RR$
      \item If $Y=aX+b$ for some $a,b\in \RR$, then: $$\varphi_Y(t)=\exp{\ii t b}\varphi_X(at)\quad\forall t\in\RR$$
      \item $\varphi_X$ is uniformly continuous.
    \end{enumerate}
  \end{proposition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be independent random variables. Let $Y:=\sum_{i=1}^n X_i$. Then: $$\varphi_Y(t)=\prod_{i=1}^n\varphi_{X_i}(t)\quad\forall t\in\RR$$
  \end{theorem}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be random variables. Then: $$X\overset{\text{d}}{=}Y\iff \varphi_X(t)=\varphi_Y(t)$$
  \end{theorem}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X_n)$ be a sequence of random variables and $X$ be a random variable. Then:
    $$X_n\overset{\text{d}}{\longrightarrow} X\iff\lim_{n\to\infty}\varphi_{X_n}(t)=\varphi_X(t)\quad\forall t\in\RR$$
  \end{theorem}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be random variables with finite $n$-th moment for some $n\in\NN$. Then, there exists the derivative of order $n$ of $\varphi_X$ and it satisfies: $${\varphi_X}^{(n)}(t)=\ii^n\Exp\left(X^n\exp{\ii t X}\right)\quad\forall t\in\RR$$ In particular, ${\varphi_X}^{(n)}(0)=\ii^n\Exp\left(X^n\right)$.
  \end{proposition}
  \subsubsection{Central limit theorem}
  \begin{theorem}[Lévy-Lindeberg central limit theorem]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of \iid random variables with finite 2nd moments. Let $\mu:=\Exp(X_1)$ and $\sigma^2:=\Var(X_1)$. Then: $$\frac{S_n-n\mu}{\sigma\sqrt{n}}\overset{\text{d}}{\longrightarrow} Z$$
    where $Z\sim N(0,1)$.
  \end{theorem}
  \begin{theorem}[Lyapunov central limit theorem]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of independent random variables each with finite expectation $\mu_i:=\Exp(X_i)$ and variance ${\sigma_i}^2:=\Var(X_i)$ $\forall i=1,\ldots,n$. Then: $$\frac{\sum_{i=1}^n(X_i-\mu_i)}{\sqrt{\sum_{i=1}^n{\sigma_i}^2}}\overset{\text{d}}{\longrightarrow} Z$$
    where $Z\sim N(0,1)$.
  \end{theorem}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)$ be a sequence of \iid random variables with finite 2nd moments. Let $\mu:=\Exp(X_1)$ and $\sigma^2:=\Var(X_1)$. Then, $\forall s,t\in\RR$ such that $s<t$ we have $$\lim_{n\to\infty}\Prob\left(s<\frac{S_n-n\mu}{\sigma\sqrt{n}}\leq t\right)=F_Z(t)-F_Z(s)$$ where $Z\sim N(0,1)$.
  \end{corollary}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ and $X_1,\ldots,X_n$ be random variables. We define the \emph{sample mean} of $X_1,\ldots,X_n$ as: $$\overline{X}_n:=\frac{1}{n}S_n=\frac{1}{n}\sum_{i=1}^nX_i$$
    If the value of $n$ is fixed, we denoted $\overline{X}_n$ by $\overline{X}$.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{A},\Prob)$ and $X_1,\ldots,X_n$ be \iid random variables. Then: $$\Exp(\overline{X}_n)=\Exp(X_1)\quad\text{and}\quad\Var(\overline{X}_n)=\frac{1}{n}\Var(X_1)$$
  \end{proposition}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be \iid random variables. Let $\mu:=\Exp(X_1)$ and $\sigma^2:=\Var(X_1)$. Then: $$\overline{X}_n\overset{\text{d}}{\simeq }N\left(\mu,\frac{\sigma^2}{n}\right)\quad\text{ for $n$ large enough}$$
  \end{corollary}
  \begin{corollary}[De Moivre-Laplace theorem]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be \iid random variables such that $X_n\sim\text{Ber}(p)$ $\forall n\in\NN$. Then: $$\text{B}(n,p)\overset{\text{d}}{\simeq }N\left(np,np(1-p)\right)\quad\text{ for $n$ large enough}\footnote{In practice, the approximation is good enough for $np(1-p)\geq 18$.}$$
  \end{corollary}
  \begin{definition}[Continuity correction]
    The \emph{continuity correction} is an adjustment that is made when a discrete distribution is approximated by a continuous distribution. For example if $X\sim\text{B}(n,p)$ is a random variable and $np(1-p)$ is large enough, then $\Prob(X\leq k)$ is well approximated by $\Prob(Z\leq k+\frac{1}{2})$, where $Z\sim N(0,1)$ which is even better than the approximation given by $\Prob(Z\leq k)$.
  \end{definition}
\end{multicols}
\begin{center}
  \def\arraystretch{1.5}
  \begin{tabular}{|c|c|c|}
    \hline
    $X$                          & Moment-generating function                                                             & Characteristic function                                         \\
    \hline
    $c\in\RR$                    & $\displaystyle \exp{tc}$                                                               & $\displaystyle \exp{\ii tc}$                                    \\
    %\hline
    $U(\{x_1,\ldots,x_n\})$      & $\frac{1}{n}\sum_{i=1}^n \exp{tx_i}$                                                   & $\frac{1}{n}\sum_{i=1}^n \exp{\ii tx_i}$                        \\
    %\hline
    $\text{B}(n,p)$              & $\displaystyle {(p\exp{t}+1-p)}^n$                                                     & $\displaystyle {(p\exp{\ii t}+1-p)}^n$                          \\
    %\hline
    $\text{Pois}(\lambda)$       & $\displaystyle \exp{\lambda(\exp{t}-1)}$                                               & $\displaystyle \exp{\lambda(\exp{\ii t}-1)}$                    \\
    %\hline
    $\text{Geo}(p)$              & $\displaystyle \frac{p\exp{t}}{1-(1-p)\exp{t}}$                                        & $\displaystyle \frac{p\exp{\ii t}}{1-(1-p)\exp{\ii t}}$         \\
    %\hline
    $\text{NB}(r,p)$             & $\displaystyle {\left(\frac{1-p}{1-p\exp{t}}\right)}^r$ for $t<-\ln p$                 & $\displaystyle {\left(\frac{1-p}{1-p\exp{\ii t}}\right)}^r$     \\
    %\hline
    $U(a,b)$                     & $\displaystyle \begin{cases}
                                                      \frac{\exp{tb}-\exp{ta}}{t(b-a)} & \text{if }t\ne 0 \\
                                                      1                                & \text{if }t=0
                                                    \end{cases}$ & $\displaystyle \begin{cases}
                                                                                    \frac{\exp{\ii tb}-\exp{\ii ta}}{\ii t(b-a)} & \text{if }t\ne 0 \\
                                                                                    1                                            & \text{if }t=0
                                                                                  \end{cases}.$                     \\
    %\hline
    $\text{Exp}(\lambda)$        & $\displaystyle \frac{\lambda}{\lambda-t}$  for $t<\lambda$                             & $\displaystyle \frac{\lambda}{\lambda-\ii t}$                   \\
    %\hline
    $N(\mu,\sigma^2)$            & $\displaystyle \exp{\mu t+\frac{\sigma^2t^2}{2}}$                                      & $\displaystyle \exp{\ii \mu t-\frac{\sigma^2t^2}{2}}$           \\
    %\hline
    $\text{Gamma}(\alpha,\beta)$ & $\displaystyle{\left(\frac{\beta}{\beta-t}\right)}^\alpha$  for $t<\beta$              & $\displaystyle {\left(\frac{\beta}{\beta-\ii t}\right)}^\alpha$ \\
    %\hline
    $\text{C}(x_0,\gamma)$       & Does not exist                                                                         & $\displaystyle \exp{\ii t x_0-\gamma|t|}$                       \\
    \hline
  \end{tabular}
  \captionof{table}{Moment-generating functions and characteristic functions of common distributions.}
\end{center}
\end{document}