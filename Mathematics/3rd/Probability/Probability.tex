\documentclass[../../../main.tex]{subfiles}

% 3 line brake in Bernoulli, Binomial and Geometric distributions.

\begin{document}
\begin{multicols}{2}[\section{Probability}]
  \subsection{Probabilistic models}
  \subsubsection{\texorpdfstring{$\sigma$}{sigma}-algebras}
  \begin{definition}[Algebra]
    Let $\Omega$ be a set and $\mathcal{A}\subset\mathcal{P}(\Omega)$. We say that $\mathcal{A}$ is an \textit{algebra over $\Omega$} if:
    \begin{enumerate}
      \item $\Omega\in\mathcal{A}$.
      \item If $A\in\mathcal{A}$, then $A^c\in\mathcal{A}$.
      \item If $A,B\in\mathcal{A}$, then $A\cup B\in\mathcal{A}$.
    \end{enumerate}
  \end{definition}
  \begin{prop}
    Let $\mathcal{A}$ be an algebra over a set $\Omega$. Then:
    \begin{enumerate}
      \item $\varnothing\in\mathcal{A}$.
      \item If $A,B\in\mathcal{A}$, then $A\cap B\in\mathcal{A}$.
      \item For all $n\in\NN$, if $A_1,\ldots,A_n\in\mathcal{A}$, then: $$\bigcup_{i=1}^nA_i\in\mathcal{A}\quad\text{and}\quad\bigcap_{i=1}^nA_i\in\mathcal{A}$$
    \end{enumerate}
  \end{prop}
  \begin{definition}[$\sigma$-algebra]
    Let $\Omega$ be a set and $\mathcal{A}\subset\mathcal{P}(\Omega)$. We say that $\mathcal{A}$ is an \textit{$\sigma$-algebra over $\Omega$} if:
    \begin{enumerate}
      \item $\Omega\in\mathcal{A}$.
      \item If $A\in\mathcal{A}$, then $A^c\in\mathcal{A}$.
      \item If $A_1,A_2,\ldots\in\mathcal{A}$, then: $$\bigcup_{n=1}^\infty A_n\in\mathcal{A}$$
    \end{enumerate}
  \end{definition}
  \begin{prop}
    Let $\Omega$ be a set, $I$ be an index set and $\{\mathcal{A}_i:i\in I\}$ be a collection of $\sigma$-algebras. Then, $\bigcap_{i\in I} \mathcal{A}_i$ is a $\sigma$-algebra.
  \end{prop}
  \begin{prop}
    Let $\mathcal{A}$ be an $\sigma$-algebra over a set $\Omega$. Then:
    \begin{enumerate}
      \item $\varnothing\in\mathcal{A}$.
      \item If $A_1,A_2,\ldots\in\mathcal{A}$, then: $$\bigcap_{n=1}^\infty A_n\in\mathcal{A}$$
      \item For all $n\in\NN$, if $A_1,\ldots,A_n\in\mathcal{A}$, then: $$\bigcup_{i=1}^nA_i\in\mathcal{A}\quad\text{and}\quad\bigcap_{i=1}^nA_i\in\mathcal{A}$$
    \end{enumerate}
  \end{prop}
  \begin{definition}
    Let $\Omega$ be a set. The \textit{trivial $\sigma$-algebra} is the smallest $\sigma$-algebra over $\Omega$, that is, $\{\varnothing,\Omega\}$.
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set. The \textit{discrete $\sigma$-algebra} is the largest $\sigma$-algebra over $\Omega$, that is, $\mathcal{P}(\Omega)$.
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set and $A\subseteq\Omega$ be a subset. The \textit{$\sigma$-algebra generated by $A$, $\sigma(A)$,} is the smallest $\sigma$-algebra over $\Omega$ containing $A$, that is: $$\sigma(A)=\{\varnothing,\Omega,A,A^c\}$$
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set and $\mathcal{C}\subseteq\mathcal{P}(\Omega)$ be a subset. The \textit{$\sigma$-algebra generated by $\mathcal{C}$, $\sigma(\mathcal{C})$,} is the smallest $\sigma$-algebra over $\Omega$ containing all the elements of $\mathcal{C}$. Moreover, if $\{\mathcal{A}_n:\mathcal{C}\subseteq\mathcal{A}_n,1\leq n\leq N\}$, $N\in\NN\cup\{\infty\}$, are all the $\sigma$-algebras over $\Omega$ containing $\mathcal{C}$, then:
    $$\sigma(\mathcal{C})=\bigcap_{n=1}^N\mathcal{A}_n$$
  \end{definition}
  \begin{theorem}
    Let $\Omega$ be a set and $\mathcal{C},\mathcal{B}\subseteq\mathcal{P}(\Omega)$ be subsets. Suppose:
    \begin{enumerate}
      \item $\mathcal{B}$ is a $\sigma$-algebra.
      \item $\mathcal{C}\subseteq\mathcal{B}$.
    \end{enumerate}
    Then, $\sigma(\mathcal{C})\subseteq\mathcal{B}$.
  \end{theorem}
  \begin{definition}
    Let $(\Omega,\mathcal{T})$ be a topological space. The \textit{Borel $\sigma$-algebra over $(\Omega,\mathcal{T})$}, $\mathcal{B}((\Omega,\mathcal{T}))$, is the $\sigma$-algebra generated by the open sets of $(\Omega,\mathcal{T})$: $$\mathcal{B}((\Omega,\mathcal{T})):=\sigma(\mathcal{T})$$
    In particular, the Borel $\sigma$-algebra over $\RR$ (together with the usual topology) is: $$\mathcal{B}(\RR):=\sigma(\{U\subseteq\RR:U\text{ is open}\})$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{T})$ be a topological space. Then: $$\mathcal{B}((\Omega,\mathcal{T}))=\sigma(\{C\subseteq\Omega:C\text{ is closed}\})$$
  \end{prop}
  \begin{prop}
    Consider the Borel $\sigma$-algebra over $\RR$, $\mathcal{B}(\RR)$. Then:
    \begin{enumerate}
      \item $\mathcal{B}(\RR)=\sigma(\{(a,b)\subset\RR: a,b\in\RR, a<b\})$
      \item $\mathcal{B}(\RR)=\sigma(\{[a,b]\subset\RR: a,b\in\RR, a<b\})$
      \item $\mathcal{B}(\RR)=\sigma(\{[a,b)\subset\RR: a,b\in\RR, a<b\})$
      \item $\mathcal{B}(\RR)=\sigma(\{(a,b]\subset\RR: a,b\in\RR, a<b\})$
      \item $\mathcal{B}(\RR)=\sigma(\{(a,\infty)\subset\RR: a\in\RR\})$
      \item $\mathcal{B}(\RR)=\sigma(\{(-\infty,a)\subset\RR: a\in\RR\})$
      \item $\mathcal{B}(\RR)=\sigma(\{[a,\infty)\subset\RR: a\in\RR\})$
      \item $\mathcal{B}(\RR)=\sigma(\{(-\infty,a]\subset\RR: a\in\RR\})$
    \end{enumerate}
  \end{prop}
  \subsubsection{Probability}
  \begin{definition}[Sample space]
    The \textit{sample space} $\Omega$ of an experiment is the set of all possible outcomes of that experiment.
  \end{definition}
  \begin{definition}[Kolmogorov axioms]
    Let $\Omega$ be a set and $\mathcal{A}$ be a $\sigma$-algebra over $\Omega$. A \textit{probability} is any function $$\Prob:\mathcal{A}\longrightarrow[0,\infty)$$ satisfying the following properties:
    \begin{itemize}
      \item $\Prob(\Omega)=1$.
      \item (\textit{$\sigma$-additivity}) If $\{A_n,n\geq1\}\subset\mathcal{A}$ are pairwise disjoint, then: $$\Prob\left(\bigsqcup_{n=1}^\infty A_n\right)=\sum_{n=1}^\infty \Prob(A_n)$$
    \end{itemize}
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set and $\mathcal{A}$ be a $\sigma$-algebra over $\Omega$. An \textit{event $A\in\mathcal{A}$} is a subset of $\Omega$ for which we want to calculate the probability.
  \end{definition}
  \begin{definition}
    A \textit{probability space} is a triplet $(\Omega,\mathcal{A},\Prob)$ where $\Omega$ is any set, $\mathcal{A}$ is a $\sigma$-algebra over $\Omega$ and $\Prob$ is a probability over $\mathcal{A}$.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A,B\in\mathcal{A}$. Then, we have the following properties:
    \begin{enumerate}
      \item $\Prob(\varnothing)=0$.
      \item If $A_i\in\mathcal{A}$, $i=1,\ldots,n$, is a finite set of pairwise disjoint events, then: $$\Prob\left(\bigsqcup_{i=1}^n A_i\right)=\sum_{i=1}^n \Prob(A_i)$$
      \item $\Prob(A\setminus B) =\Prob(A)-\Prob(A\cap B)$.
      \item If $B\subset A$, then $\Prob(A\setminus B)=\Prob(A)-\Prob(B)$.
      \item If $B\subset A$, then $\Prob(B)\leq \Prob(A)$.
      \item $\Prob(A)\leq 1$.
      \item $\Prob(A^c)=1-\Prob(A)$.
      \item $\Prob(A\cup B) =\Prob(A)+\Prob(B)-\Prob(A\cap B)$.
      \item If $A_1,\ldots,A_n\in\mathcal{A}$, then:
            \begin{multline*}
              \Prob\left(\bigcup_{i=1}^n A_i\right)=\sum_{i=1}^n \Prob(A_i)-\\-\sum_{\substack{i,j=1\\i<j}}^n\Prob(A_i\cap A_j)+\sum_{\substack{i,j,k=1\\i<j<k}}^n\Prob(A_i\cap A_j\cap A_k)-\cdots+\\+{(-1)}^{n+1}\Prob(A_1\cap\cdots\cap A_n)
            \end{multline*}
      \item If $A_1,\ldots,A_n\in\mathcal{A}$, then:
            \begin{multline*}
              \Prob\left(\bigcap_{i=1}^n A_i\right)=\sum_{i=1}^n \Prob(A_i)-\\-\sum_{\substack{i,j=1\\i<j}}^n\Prob(A_i\cup A_j)+\sum_{\substack{i,j,k=1\\i<j<k}}^n\Prob(A_i\cup A_j\cap A_k)-\cdots+\\+{(-1)}^{n+1}\Prob(A_1\cup\cdots\cup A_n)
            \end{multline*}
      \item If $A_i,\ldots,A_n\in\mathcal{A}$, then: $$\Prob\left(\bigcup_{i=1}^n A_i\right)\leq\sum_{i=1}^n \Prob(A_i)$$
    \end{enumerate}
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space such that $\Omega$ is finite and all its elements are equiprobable. Let $A\in\mathcal{A}$ be an event. Then: $$\Prob(A)=\frac{|A|}{|\Omega|}$$
  \end{prop}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:n\geq 1\}\subset\mathcal{A}$ be an increasing sequence of events, that is: $$A_1\subseteq A_2\subseteq\cdots\subseteq A_n\subseteq\cdots$$ Let $A:=\bigcup_{n=1}^\infty A_n$. Then: $$\Prob(A):=\lim_{n\to\infty}\Prob(A_n)$$
  \end{theorem}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:n\geq 1\}\subset\mathcal{A}$ be an decreasing sequence of events, that is: $$A_1\supseteq A_2\supseteq\cdots\supseteq A_n\supseteq\cdots$$ Let $A:=\bigcap_{n=1}^\infty A_n$. Then: $$\Prob(A):=\lim_{n\to\infty}\Prob(A_n)$$
  \end{corollary}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:n\geq 1\}\subset\mathcal{A}$ be a sequence of events. Then: $$\Prob\left(\bigcup_{n=1}^\infty A_n\right)\leq\sum_{n=1}^\infty \Prob(A_n)$$
  \end{prop}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:n\geq 1\}\subset\mathcal{A}$ be a sequence of events with probability 0. Then: $$\Prob\left(\bigcup_{n=1}^\infty A_n\right)=0$$
  \end{corollary}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:n\geq 1\}\subset\mathcal{A}$ be a sequence of events with probability 1. Then: $$\Prob\left(\bigcap_{n=1}^\infty A_n\right)=1$$
  \end{corollary}
  \subsubsection{Conditional probability}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$ be an event such that $\Prob(A)>0$. The \textit{conditional probability that $B\in\mathcal{A}$ occurs given that $A$ occurs} is defined as: $$\Prob(B\mid A):=\frac{\Prob(A\cap B)}{\Prob(A)}$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$ be an event such that $\Prob(A)>0$. Then, the function
    $$
      \function{\Prob(\cdot\mid A)}{\mathcal{A}}{[0,\infty]}{B}{\Prob(B\mid A)}
    $$
    is a probability.
  \end{prop}
  \begin{prop}[Compound probability formula]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$ be an event such that $\Prob(A)>0$. Then, $\forall B\in\mathcal{A}$: $$\Prob(A\cap B)=\Prob(B\mid A)\Prob(A)$$
  \end{prop}
  \begin{prop}[Generalized compound probability formula]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A_1,\ldots,A_n\in\mathcal{A}$, $n\geq 2$, be events such that $\Prob(A_1\cap\cdots\cap A_{n-1})>0$. Then:
    \begin{multline*}
      \Prob(A_1\cap\cdots\cap A_n)=\Prob(A_1)\Prob(A_2\mid A_1)\Prob(A_3\mid A_2\cap A_1)\cdots\\\cdots \Prob(A_n\mid A_1\cap\cdots\cap A_{n-1})
    \end{multline*}
  \end{prop}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A=\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be a collection of events. We say that $A$ is a \textit{partition} of $\Omega$ if: $$\Omega=\bigsqcup_{n=1}^NA_n$$
  \end{definition}
  \begin{prop}[Total probability formula]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be a partition of $\Omega$ such that $\Prob(A_n)>0$ for all $1\leq n\leq N$. Then, $\forall A\in\mathcal{A}$: $$\Prob(A)=\sum_{n=1}^N\Prob(A_n)\Prob(A\mid A_n)$$
  \end{prop}
  \begin{prop}[Bayes' formula]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be a partition of $\Omega$ such that $\Prob(A_n)>0$ for all $1\leq n\leq N$. Let $A\in\mathcal{A}$ with $\Prob(A)>0$. Then, $\forall k\leq N$: $$\Prob(A_k\mid A)=\frac{\Prob(A_k)\Prob(A\mid A_k)}{\sum_{n=1}^N\Prob(A_n)\Prob(A\mid A_n)}$$
  \end{prop}
  \subsubsection{Independence of events}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Then, $A,B\in\mathcal{A}$ are \textit{independent events} if $$\Prob(A\cap B)=\Prob(A)\Prob(B)$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Then:
    \begin{enumerate}
      \item $\varnothing$ and $\Omega$ are independent of any event.
      \item If $A\in\mathcal{A}$ satisfies either $\Prob(A)=0$ or $\Prob(A)=1$, then $A$ is independent of any other event $B\in\mathcal{A}$.
      \item If an event $A\in\mathcal{A}$ is independent of itself, then either $\Prob(A)=0$ or $\Prob(A)=1$.
    \end{enumerate}
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A,B\in\mathcal{A}$ be two events. The following statements are equivalent:
    \begin{itemize}
      \item $A$ and $B$ are independent.
      \item $A^c$ and $B$ are independent.
      \item $A$ and $B^c$ are independent.
      \item $A^c$ and $B^c$ are independent.
    \end{itemize}
  \end{prop}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $n\in\NN$. We say that $A_1,\ldots,A_n\in\mathcal{A}$ are \textit{independent events} if for any $i_1,\ldots,i_k\in\{1,\ldots,n\}$, we have: $$\Prob\left(\bigcap_{r=1}^kA_{i_r}\right)=\prod_{r=1}^k\Prob(A_{i_r})$$
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $I$ be an arbitrary index set. We say that $\{A_i:i\in I\}\subset\mathcal{A}$ are \textit{independent events} if for any finite subset $\{A_{i_1},\ldots, A_{i_k}:i_r\in I\text{ for }r=1,\ldots,k\}$ of different events, we have: $$\Prob\left(\bigcap_{r=1}^kA_{i_r}\right)=\prod_{r=1}^k\Prob(A_{i_r})$$
  \end{definition}
  \subsection{Lebesgue integration}
  \begin{definition}
    Let $A\subset\RR^n$ be a subset. Then, $A$ is a \textit{null set} (or a \textit{set of zero-content}) if $\forall \varepsilon>0$ there exists a collection $\{R_k\subset\RR^n:R_k\text{ is a rectangle }\forall k\in\NN\}$ of rectangles such that:
    $$A\subset\bigcup_{k=1}^\infty R_k\qquad\text{and}\qquad\sum_{k=1}^\infty\text{vol}(R_k)<\varepsilon$$
  \end{definition}
  \begin{definition}
    Let $E$ be a set and $\mathcal{E}$ be a $\sigma$-algebra over $E$. We say that the function:
    $$
      \function{\mu}{\mathcal{E}}{[0,\infty]}{A}{\mu(A)}
    $$
    is a \textit{measure} if:
    \begin{enumerate}
      \item There exists $A\in\mathcal{E}$ such that $\mu(A)<\infty$.
      \item If $\{A_n\in\mathcal{E}:n\in\NN\}$ is a collection of pairwise disjoint sets, then: $$\mu\left(\bigcup_{n=1}^\infty A_n\right)=\sum_{n=1}^\infty \mu(A_n)$$
    \end{enumerate}
    The triplet $(E,\mathcal{E},\mu)$ is called a \textit{measurable space}.
  \end{definition}
  \begin{definition}
    The \textit{$\sigma$-algebra of all Lebesgue measurable sets in $\RR^n$}, $\mathcal{L}_n\subset\mathcal{P}(\RR^n)$, is defined as:
    \begin{multline*}
      \mathcal{L}_n:=\{A\subseteq\RR^n:A=B\cup N\text{ with }B\in\mathcal{B}(\RR^n)\\\text{ and $N$ is a null set}\}
    \end{multline*}
  \end{definition}
  \begin{theorem}
    We can extend the concept of volume on rectangles in $\RR^n$ to all the elements in $\mathcal{L}_n$. This extension is called \textit{Lebesgue measure} (or simply \textit{volume}) \textit{in $\RR^n$}.
  \end{theorem}
  \begin{definition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space and $f:E\rightarrow\RR$ be a function. We say that $f$ is \textit{measurable} if $\forall B\in\mathcal{B}(\RR)$ we have $f^{-1}(B)\in\mathcal{E}$. The \textit{Lebesgue integral of $f$ over $E$ with respect to $\mu$} is denoted by: $$\int_Ef\dd\mu$$
  \end{definition}
  \begin{prop}
    Let $(E,\mathcal{E},\mu)$ be a measurable space and $f:E\rightarrow\RR$ be a measurable function such that $f(x)\geq 0$ $\forall x\in E$. Then, we can always define the integral $$\int_Ef\dd\mu$$ taking into account that may be $+\infty$.
  \end{prop}
  \begin{definition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space and $f:E\rightarrow\RR$ be a measurable function. We say that $f$ is \textit{Lebesgue integrable with respect to $\mu$} if: $$\int_E|f|\dd\mu<\infty$$
    Moreover if $G\in\mathcal{E}$, we define: $$\int_Gf\dd\mu:=\int_Ef\vectorfunction{1}_G\dd\mu$$
  \end{definition}
  \begin{prop}
    Consider the measurable space $(\RR^n,\mathcal{L}_n,m_n)$, where $m_n$ is the volume in $\RR^n$. Let $f:\RR^n\rightarrow\RR$ be a Riemann integrable function satisfying: $$\int_{\RR^n}|f(x_1,\ldots,x_n)|\dd x_1\cdots\dd x_n<\infty$$
    Then, $f$ is Lebesgue integrable and: $$\int_{\RR^n}|f(x_1,\ldots,x_n)|\dd x_1\cdots\dd x_n=\int_{\RR^n}f\dd m_n$$
  \end{prop}
  \begin{theorem}[Tonelli's theorem]
    Let $f:\RR^2\rightarrow\RR$ be a non-negative Lebesgue measurable function. Then:
    \begin{multline*}
      \int_{\RR^2}f(x,y)\dd x\dd y=\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}f(x,y)\dd x\right)\dd y=\\=\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}f(x,y)\dd y\right)\dd x
    \end{multline*}
  \end{theorem}
  \begin{theorem}[Fubini's theorem]
    Let $f:\RR^2\rightarrow\RR$ be a Lebesgue measurable function such that at least one of the following integrals is finite.
    \begin{gather*}
      \int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}|f(x,y)|\dd x\right)\dd y\\
      \int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}|f(x,y)|\dd y\right)\dd x
    \end{gather*}
    Then, $f$ is Lebesgue integrable and:
    \begin{multline*}
      \int_{\RR^2}f(x,y)\dd x\dd y=\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}f(x,y)\dd x\right)\dd y=\\=\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty}f(x,y)\dd y\right)\dd x
    \end{multline*}
  \end{theorem}
  \subsection{Random variables and random vectors}
  \subsubsection{Random variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. A \textit{random variable $X$} is a function $X:\Omega\rightarrow\RR$ satisfying for all $B\in\mathcal{B}(\RR)$: $$X^{-1}(B)=\{\omega\in\Omega:X(\omega)\in B\}\in\mathcal{A}$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\mathcal{C}$ be a collection of subsets of $\RR$ such that $\mathcal{B}(\RR)=\sigma(\mathcal{C})$ and let $X:\Omega\rightarrow\RR$ be a function. Then, $X$ is a random variable if and only if $X^{-1}(B)\in\mathcal{A}$, $\forall B\in \mathcal{C}$.
  \end{prop}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $a,b\in\RR$ and $B\in\mathcal{B}(\RR)$. We define the following set:
    $$\{X\in B\}:=\{\omega\in\Omega:X(\omega)\in B\}=X^{-1}(B)$$
    In particular:
    \begin{align*}
      \{X\leq a\}   & :=\{\omega\in\Omega:X(\omega)\leq a\}=X^{-1}((-\infty,a]) \\
      \{X> b\}      & :=\{\omega\in\Omega:X(\omega)>b\}=X^{-1}((b,\infty))      \\
      \{a<X\leq b\} & :=\{\omega\in\Omega:a< X(\omega)\leq b\}=X^{-1}([b,a))    \\
      \{X=a\}       & :=\{\omega\in\Omega:X(\omega)=a\}=X^{-1}(\{a\})
    \end{align*}
  \end{definition}
  \begin{prop}
    Let $S=(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$, $Y$ be random variables defined on $S$ and $a\in\RR$. Then:
    \begin{enumerate}
      \item $X+Y$ is also a random variable.
      \item $aX$ is also a random variable.
      \item $XY$ is also a random variable.
      \item $\frac{1}{X}$ is also a random variable if $X(\omega)\ne 0$ $\forall \omega\in\Omega$.
    \end{enumerate}
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)_{n\geq 1}$ be a sequence of random variables. Then, the following quantities are also random variables provided that they are finite for all $\omega\in\Omega$:
    \begin{enumerate}
      \item $\sup\{X_n:n\geq 1\}$
      \item $\inf\{X_n:n\geq 1\}$
      \item $\displaystyle\limsup_{n\to\infty}X_n$
      \item $\displaystyle\liminf_{n\to\infty} X_n$
    \end{enumerate}
  \end{prop}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X_n)_{n\geq 1}$ be a sequence of random variables such that $\forall\omega\in\Omega$ the following limit exists and it is finite: $$X(\omega):=\lim_{n\to\infty}X_n(\omega)$$
    Then, $X$ is a random variable.
  \end{corollary}
  \subsubsection{Distribution of a random variable}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. The \textit{distribution of a random variable $X$} is the function:
    $$
      \function{\Prob_X}{\mathcal{B}(\RR)}{[0,1]}{B}{\Prob(\{X\in B\})\footnotemark}
    $$
  \end{definition}
  \begin{prop}\footnotetext{From now on, in order to simplify the notation, we will write $\Prob(X\in B):=\Prob(\{X\in B\})$.}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Then, for any random variable $X$, the function $\Prob_X$ is a probability over $\mathcal{B}(\RR)$. Hence, $(\RR,\mathcal{B}(\RR),\Prob_X)$ is a probability space.
  \end{prop}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that two random variables $X$, $Y$ are \textit{equal in distribution} (denoted by $X\overset{\text{d}}{=}Y$) if they satisfy: $$\Prob_X(B)=\Prob_Y(B)\qquad\forall B\in\mathcal{B}(\RR)$$ That is, $X\overset{\text{d}}{=}Y$ if they have the same distribution functions.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that two random variables $X$, $Y$ are \textit{equal almost surely} (denoted by $X\overset{\text{a.s.}}{=}Y$) if $\Prob(X=Y)=1$, or equivalently, if $\Prob(X\ne Y)=0$.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$, $Y$ be two random variables such that $X\overset{\text{a.s.}}{=}Y$. Then, $X\overset{\text{d}}{=}Y$.
  \end{prop}
  \begin{definition}[Cumulative distribution function]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable. We define the \textit{cumulative distribution function (cdf)} as:
    $$
      \function{F_X}{\RR}{[0,1]}{x}{\Prob(X\leq x)=\Prob_X((-\infty,x])}
    $$
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a random variable and $F_X$ be its cdf. Then:
    \begin{enumerate}
      \item If $x<y$, then $F_X(x)\leq F_X(y)$.
      \item $F_X$ is \textit{càdlàg}\footnote{From French ``continue à droite, limite à gauche'' (right continuous with left limits).}.
      \item $\displaystyle\lim_{x\to -\infty}F_X(x)=0$ and $\displaystyle\lim_{x\to \infty}F_X(x)=1$.
    \end{enumerate}
    Reciprocally, if there is a function $F$ satisfying these properties\footnote{Such kind of functions are called \textit{distribution functions}.}, then there exists a random variable $X$ on $(\Omega,\mathcal{A},\Prob)$ such that $F$ is its cdf.
  \end{theorem}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a random variable and $F_X$ be its cdf. Then:
    \begin{enumerate}
      \item $F_X$ has at most a countable number of discontinuities.
      \item $\forall s,t\in\RR$ such that $s<t$, we have: $$\Prob(s<X\leq t)=F(t)-F(s)$$
      \item $\displaystyle\forall x\in\RR$, $\displaystyle \Prob(X<x)=\lim_{t\to x^-}F_X(t)$.
      \item For all $x\in\RR$: $$\Prob(X=x)=F_X(x)-\lim_{t\to x^-}F_X(t)$$ Hence, $F_X$ is discontinuous at $x\iff\Prob(X=x)>0$.
    \end{enumerate}
  \end{prop}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Then, the cdf completely determine a distribution of a random variable $X$. That is, if $X$ and $Y$ are random variables such that $F_X(t)=F_Y(t)$ $\forall t\in\RR$, then $X\overset{\text{d}}{=}Y$.
  \end{theorem}
  \subsubsection{Discrete random  variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that a random variable $X$ is \textit{discrete} if there exists a finite or countable set $S\subset\RR$ such that $\Prob(X\in S)=1$. In that case, $S$ is called the \textit{support of $X$}\footnote{By agreement, we will suppose that $S$ only contains points $x$ such that $\Prob(X=x)>0$.}.
  \end{definition}
  \begin{definition}[Probability mass function]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a discrete random variable with support points $S=\{x_i:i\in I\}$, where $I\subseteq\NN$. The \textit{probability mass function (pmf)} of the random variable $X$ is:
    $$
      \function{p_X}{S}{[0,1]}{x_i}{\Prob(X=x_i)}
    $$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a discrete random variable with support points $S=\{x_i:i\in I\}$ and $p_X$ be its pmf. Then:
    \begin{enumerate}
      \item $p_X(x_i)>0$ $\forall i\in I$.
      \item $\displaystyle\sum_{i\in I}p_X(x_i)=1$.
      \item $\forall B\in\mathcal{B}(\RR)$, we have: $$\Prob(X\in B)=\sum_{i\in I:x_i\in B}p_X(x_i)$$
    \end{enumerate}
  \end{prop}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a discrete random variable with support points $S=\{x_i:i\in I\}$, $F_X$ be its cdf and $p_X$ be its pmf. Then $\forall x\in\RR$ we have: $$F_X(x)=\Prob(X\leq x)=\sum_{i\in I:x_i\leq x}p_X(x_i)$$
  \end{corollary}
  \begin{definition}[Degenerated distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. The \textit{degenerated distribution} consists in taking a constant random variable $X$ so that $$\Prob(X=a)=1$$ for some $a\in\RR$. Here we have $S=\{a\}$.
  \end{definition}
  \begin{definition}[Bernoulli distribution]
    Let\\ $(\Omega,\mathcal{A},\Prob)$ be a probability space. The \textit{Bernoulli distribution} is the one that can only take two values (1 and 0)\footnote{Also called \textit{success}/\textit{true} and \textit{failure}/\textit{false} respectively.} with probabilities $p$ and $q:=1-p$: $$\Prob(X=0)=p\qquad \Prob(X=1)=q$$ Here we have $S=\{0,1\}$. If $X$ follows a Bernoulli distribution of parameter $p$, we will write $X\sim \text{Ber}(p)$.
  \end{definition}
  \begin{definition}[Discrete uniform distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. The \textit{discrete uniform distribution} is the one whose random variable $X$ takes values on $S=\{x_1,\ldots,x_n\}$ each of these with probability $1/n$: $$\Prob(X=x_i)=\frac{1}{n}\qquad \forall i=1,\ldots,n$$ If $X$ follows a discrete uniform distribution, we will write $X\sim U\{x_1,\ldots,x_n\}$. The probability space $(S,\mathcal{P}(S),\Prob_X)$ is an \textit{equiprobable space}.
  \end{definition}
  \begin{definition}[Binomial distribution]
    Let\\ $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$. Suppose $\Prob(A)=p$. The \textit{binomial distribution} is the one whose random variable $X$ is the number of successes of $A$ in a sequence of $n$ repetitions. Thus, $S=\{0,1,\ldots,n\}$ and: $$\Prob(X=k)=\binom{n}{k}p^k{(1-p)}^{n-k}\qquad \forall k=0,1,\ldots,n$$ If $X$ follows a binomial distribution of parameters $n$ and $p$, we will write $X\sim \text{B}(n,p)$\footnote{Note that, a Bernoulli distribution of parameter $p$ may be considered as a Binomial distribution of parameters $n=1$ and $p$. Hence, $\text{Ber}(p)=\text{B}(1,p)$.}.
  \end{definition}
  \begin{definition}[Poisson distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\lambda\in\RR_{>0}$. The \textit{Poisson distribution of parameter $\lambda$} is the one whose random variable has support $S=\NN\cup\{0\}$ and: $$\Prob(X=k)=\exp{-\lambda}\frac{\lambda^k}{k!}\qquad \forall k\in\NN\cup\{0\}$$ If $X$ follows a Poisson distribution of parameter $\lambda$, we will write $X\sim \text{Pois}(\lambda)$.
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Let $(p_n)\subset(0,1)$ be a sequence such that: $$\lim_{n\to\infty}np_n=\lambda>0$$
    For each $n\geq 1$, consider $X_n\sim \text{B}(n,p_n)$. Then, $\forall k\in\NN\cup\{0\}$ we have: $$\lim_{n\to\infty}\Prob(X_n=k)=\lim_{n\to\infty}\binom{n}{k}{p_n}^k{(1-p_n)}^{n-k}=\exp{-\lambda}\frac{\lambda^k}{k!}$$
  \end{theorem}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and suppose $n\in\NN$ and $p\in (0,1)$ are such that $n\gg 1$ and $p\ll 1$. Then, $\text{B}(n,p)\simeq\text{Pois}(np)$.
  \end{corollary}
  \begin{definition}[Geometric distribution]
    Let\\ $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$. Suppose $\Prob(A)=p$. The \textit{geometric distribution} is the one whose random variable $X$ is the number of trials needed to get one success. Thus, $S=\NN$ and: $$\Prob(X=k)={(1-p)}^{k-1}p\qquad \forall k\in\NN$$ If $X$ follows a geometric distribution of parameter $p$, we will write $X\sim \text{Geo}(p)$.
  \end{definition}
  \begin{definition}[Discrete memorylessness property]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a discrete random variable whose support is $\NN$ and such that $\Prob(X >m)>0$ $\forall m\in\NN$. The distribution of $X$ is \textit{memoryless} if $\forall m,n\in\NN$, we have: $$\Prob(X>m+n\mid X >m)=\Prob(X>n)$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be a discrete random variable and $p\in(0,1)$. Then, $X\sim\text{Geo}(p)$ if and only if the distribution of $X$ is memoryless.
  \end{prop}
  \begin{definition}[Hypergeometric distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. Suppose we have a population of size $N$ of whom $K$ have a special feature (success). Let $X$ be the random variable that counts the number of successes that we have obtained in $n$ draws (without replacement). Thus, the support of $X$ is: $$S=\{\max\{n+K-N,0\},\ldots,\min\{n,K\}\}$$ And the pmf is given by: $$\Prob(X=k)=\frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}$$ This type of distribution is called \textit{hypergeometric distribution} and it is denoted by $X\sim \text{HG}(N,p,n)$, where $p=\frac{K}{N}$ is the proportion of successes in the population.
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X\sim \text{HG}(N,p,n)$ such that when $N\to\infty$, $p$ remains constant. Then:
    $$\lim_{N\to\infty}\Prob(X=k)=\lim_{N\to\infty}\frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}=\binom{n}{k}p^k{(1-p)}^{n-k}$$ which is the pmf of a binomial distribution $\text{B}(n,p)$.
  \end{theorem}
  \begin{definition}[Negative binomial distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A\in\mathcal{A}$. Suppose $\Prob(A)=p$. The \textit{negative binomial distribution} is the one whose random variable $X$ is the number of trials needed to get $r\geq 1$ successes. Thus, $S=\{r,r+1,\ldots\}$ and: $$\Prob(X=k)=\binom{k-1}{r-1}p^r{(1-p)}^{k-r}\qquad \forall k\geq r$$ If $X$ follows a negative binomial distribution of parameters $r$ and $p$, we will write $X\sim \text{NB}(r,p)$.
  \end{definition}
  \subsubsection{Absolutely continuous random variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that a random variable $X$ is \textit{absolutely continuous} if there exists a function $f:\RR\rightarrow\RR$ satisfying:
    \begin{enumerate}
      \item $f(x)\geq 0$, $\forall x\in\RR$.
      \item $f$ is integrable over $\RR$ and: $$\int_{-\infty}^{+\infty}f(x)\dd x=1$$
      \item For all $a,b\in\RR\cup\{\pm\infty\}$ with $a\leq b$, we have: $$\Prob(a\leq X\leq b)=\int_a^bf(x)\dd x$$
    \end{enumerate}
    The function $f$, denoted by $f_X$, is called \textit{probability density function (pdf)} of $X$. In general, a function satisfying the first two properties is called a \textit{density function}.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be an absolutely continuous random variable and $F_X$ be its cdf. Then:
    \begin{enumerate}
      \item $\Prob(X=a)=0,\quad\forall a\in \RR$.
      \item $\displaystyle \Prob(X\in B)=\int_Bf_X(x)\dd x,\quad\forall B\in\mathcal{B}(\RR)$.
      \item $\displaystyle F_X(b)=\Prob(X\leq b)=\int_{-\infty}^bf_X(x)\dd x,\quad\forall b\in\RR$.
      \item $F_X$ is continuous on $\RR$.
      \item If $a,b\in\RR$ are such that $a<b$, then:
            \begin{multline*}
              \Prob(a<X<b)=\Prob(a\leq X<b)=\\=\Prob(a<X\leq b)=\Prob(a\leq X\leq b)
            \end{multline*}
    \end{enumerate}
  \end{prop}
  \begin{definition}[Continuous uniform distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that an absolutely continuous random variable $X$ follows a \textit{continuous uniform distribution on $(a,b)$} (also $[a,b]$), and we denoted it by $X\sim U(a,b)$, if $X$ has the pdf $$f_X(x)=\frac{1}{b-a}\vectorfunction{1}_{(a,b)}(x)$$ where $\vectorfunction{1}_{(a,b)}$ is the indicator function. Therefore, its cdf is:
    \begin{multline*}
      F_X(x)=\left\{
      \begin{array}{ccl}
        0                            & \text{if} & x\leq a \\
        \displaystyle\frac{x-a}{b-a} & \text{if} & a<x<b   \\
        1                            & \text{if} & x\geq b \\
      \end{array}
      \right.=\\=\frac{x-a}{b-a}\vectorfunction{1}_{(a,b)}(x)+\vectorfunction{1}_{[b,\infty)}(x)
    \end{multline*}
  \end{definition}
  \begin{definition}[Exponential distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that an absolutely continuous random variable $X$ follows an \textit{exponential distribution of parameter $\lambda>0$}, and we denoted it by $X\sim \text{Exp}(\lambda)$, if $X$ has the pdf: $$f_X(x)=\lambda\exp{-\lambda x}\vectorfunction{1}_{(0,\infty)}$$ Furthermore, its cdf is:
    $$F_X(x)=(1-\exp{-\lambda x})\vectorfunction{1}_{(0,\infty)}(x)$$
  \end{definition}
  \begin{definition}[Continuous memorylessness property]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be an absolutely continuous random variable such that $\Prob(X >s)>0$ $\forall s\in\RR_{\geq 0}$. The distribution of $X$ is \textit{memoryless} if $\forall s,t\in\RR_{\geq 0}$, we have: $$\Prob(X>s+t\mid X >s)=\Prob(X>t)$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be an absolutely continuous random variable and $\lambda\in\RR_{>0}$. Then, $X\sim\text{Exp}(\lambda)$ if and only if the distribution of $X$ is memoryless.
  \end{prop}
  \begin{definition}[Standard normal distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that an absolutely continuous random variable $Z$ follows a \textit{standard normal distribution}, and we denoted it by $Z\sim N(0,1)$, if $Z$ has the pdf: $$f_X(x)=\frac{1}{\sqrt{2\pi}}\exp{\frac{-x^2}{2}}$$
  \end{definition}
  \begin{definition}[Normal distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\mu\in\RR$ and $\sigma\in\RR_{>0}$. We say that an absolutely continuous random variable $X$ follows a \textit{normal distribution}, and we denoted it by $X\sim N(\mu,\sigma^2)$, if $X$ has the pdf: $$f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\frac{-{(x-\mu)}^2}{2\sigma^2}}$$ $\mu$ is called the \textit{mean} or \textit{expectation of $X$}; $\sigma^2$, its \textit{variance}, and $\sigma$, its \textit{standard deviation}.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X\sim N(\mu,\sigma^2)$ and $Z\sim N(0,1)$ be absolutely continuous random variables. Then: $$\mu+\sigma Z\overset{\text{d}}{=} X$$ Therefore, $\forall x\in\RR$ we have: $$\Prob(X\leq x)=\Prob\left(Z\leq\frac{x-\mu}{\sigma}\right)$$ In this case, $Z$ is called the \textit{standardized form of $X$}.
  \end{prop}
  \begin{definition}[Gamma distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that an absolutely continuous random variable $X$ follows a \textit{gamma distribution of parameters $r,\alpha\in\RR_{>0}$}, and we denoted it by $X\sim \text{Gamma}(r,\alpha)$, if $X$ has the pdf: $$f_X(x)=\frac{\alpha^r}{\Gamma(r)}x^{r-1}\exp{-\alpha x}\vectorfunction{1}_{(0,\infty)}(x)$$
  \end{definition}
  \begin{definition}
    Let $a,b\in\RR_{>0}$. The \textit{beta function} is defined as: $$\text{B}(a,b):=\int_0^1x^{a-1}{(1-x)}^{b-1}\dd x$$
  \end{definition}
  \begin{prop}
    For all $a,b\in\RR_{>0}$, we have: $$\text{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$$
  \end{prop}
  \begin{definition}[Beta distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. We say that an absolutely continuous random variable $X$ follows a \textit{beta distribution of parameters $a,b\in\RR_{>0}$}, and we denoted it by $X\sim \text{B}(a,b)$, if $X$ has the pdf: $$f_X(x)=\frac{1}{\text{B}(a,b)}x^{a-1}{(1-x)}^{b-1}\vectorfunction{1}_{(0,1)}(x)$$
  \end{definition}
  \begin{definition}
    A\textit{ mixed random variable} is a random variable whose cdf is neither piecewise-constant (a discrete random variable) nor absolutely  continuous.
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X$ be a random variable with cdf $F_X$. Suppose that:
    \begin{enumerate}
      \item $F_X$ is continuous.
      \item $F_X$ is differentiable at any point except for, maybe, a finite number of points.
      \item $F_X$ is continuously differentiable at any point except for, maybe, a finite number of points.
    \end{enumerate}
    Then: $$F_X(x)=\int_{-\infty}^xF'(t)\dd t\qquad\forall x\in\RR$$
    That is, ${F_X}'$ is the pdf of $X$.
  \end{theorem}
  \subsubsection{Transformations of random variables}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be an absolutely continuous random variable with pdf $f_X$ and $U$, $V$ be open sets such that $\Prob(X\in U)=1$. Let $h:U\rightarrow V$ be a diffeomorphism of class $\mathcal{C}^1$. Then, $Y:=h(X)$ is also an absolutely continuous random variable and: $$f_Y(y)=f_X(h^{-1}(y))|(h^{-1})'(y)|\vectorfunction{1}_V(y)$$
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X$ be an absolutely continuous random variable with pdf $f_X$, $U_1,\ldots, U_k$ be pairwise disjoint open intervals such that $\Prob(X\in U_1\sqcup \cdots\sqcup U_k)=1$. Let $h:U_1\sqcup \cdots\sqcup U_k\rightarrow \RR$ and denote $h_i=h|_{U_i}$. Then, if $h_i:U_i\rightarrow V_i$ are  diffeomorphisms of class $\mathcal{C}^1$ for $i=1,\ldots,k$, then $Y:=h(X)$ is also an absolutely continuous random variable and: $$f_Y(y)=\sum_{i=1}^kf_X({h_i}^{-1}(y))|({h_i}^{-1})'(y)|\vectorfunction{1}_{V_i}(y)$$
  \end{prop}
  \subsubsection{Random vectors}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. A \textit{random vector $\vectorfunction{X}$} is a function $\vectorfunction{X}=(X_1,\ldots,X_n):\Omega\rightarrow\RR^n$ satisfying for all $B\in\mathcal{B}(\RR^n)$: $$\{\vectorfunction{X}\in B\}=\{\omega\in\Omega:\vectorfunction{X}(\omega)\in B\}\in\mathcal{A}$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space. $\vectorfunction{X}=(X_1,\ldots,X_n):\Omega\rightarrow\RR^n$ is a random vector if and only if $X_i:\Omega\rightarrow\RR$ is a random variable for $i=1,\ldots,n$.
  \end{prop}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a random vector. For all $B_1\times\cdots\times B_n\in\mathcal{B}(\RR^n)$, we have that: $$\{\vectorfunction{X}\in B_1\times\cdots\times B_n\}=\{X_1\in B_1\}\cap\cdots\cap\{X_n\in B_n\}$$ We will denote: $$\{X_1\in B_1,\ldots,X_n\in B_n\}:=\{X_1\in B_1\}\cap\cdots\cap\{X_n\in B_n\}$$
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a random vector. Then, the \textit{distribution of $\vectorfunction{X}$} is the function:
    $$
      \function{\Prob_{\vectorfunction{X}}}{\mathcal{B}(\RR^n)}{[0,1]}{B}{\Prob(\vectorfunction{X}\in B)}
    $$
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a random vector. We say that $\vectorfunction{X}$ is \textit{discrete} if there exists a finite or countable subset $S\subset\RR^n$ such that $\Prob(\vectorfunction{X}\in S)=1$\footnote{By agreement, we will suppose that $S$ only contains points $x$ such that $\Prob(\vectorfunction{X}=x)>0$.}. In that case, $S$ is called the \textit{support of $\vectorfunction{X}$}\footnote{In general we will denote $S$ by $S_{\vectorfunction{X}}$. Moreover, note that $S_{\vectorfunction{X}}=S_{X_1}\times\cdots\times S_{X_n}$, where $S_{X_i}$ is the support of the random variable $X_i$.}.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a random vector. Then, $\vectorfunction{X}$ is discrete if and only if $X_i$ is a discrete random variable for $i=1,\ldots,n$.
  \end{prop}
  \begin{definition}[Joint probability mass function]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a discrete random vector. Then, the \textit{joint probability mass function (joint pmf)} of $\vectorfunction{X}$ is:
    $$
      \function{p_{\vectorfunction{X}}}{S_{X_1}\times\cdots\times S_{X_n}}{[0,1]}{(x_1,\ldots,x_n)}{\Prob(X_1=x_1,\ldots,X_n=x_n)}
    $$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}$ be a discrete random vector. Then, the joint pmf of $\vectorfunction{X}$ determines the distribution of $\vectorfunction{X}$.
  \end{prop}
  \begin{definition}[Marginal probability mass functions]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a discrete random vector with support $S_{\vectorfunction{X}}=S_{X_1}\times\cdots\times S_{X_n}$. Then, the \textit{marginal probability mass functions (marginal pmf)} of $\vectorfunction{X}$ are:
    \begin{multline*}
      p_{X_i}(x_i)=\Prob(X_i=x_i)=\\=\sum_{\substack{y_j\in S_{X_j}\\j\ne i}}p_{\vectorfunction{X}}(y_1,\ldots,y_{i-1},x_i,y_{i+1},\ldots,y_n)
    \end{multline*}
    for $i=1,\ldots,n$.
  \end{definition}
  \begin{definition}[Multinomial distrbution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $A_1,\ldots,A_r\in\mathcal{A}$. Suppose $\Prob(A_i)=p_i$ for $i=1,\ldots,r$ such that $p_1+\cdots+p_r=1$. The \textit{multinomial distribution} is the one whose $i$-th random variable $X_i$ is the number of successes of $A_i$ in a sequence of $n$ repetitions, for $i=1,\ldots,r$, that is, $X_i\sim\text{B}(n,p_i)$. Thus, for all $n_1,\ldots,n_r\in\{0,1,\ldots,n\}$ such that $n_1+\cdots+n_r=n$ we have: $$\Prob(X_1=n_1,\ldots,X_r=n_r)=\frac{n!}{n_1!\cdots n_r!} {p_1}^{n_1}\cdots{p_r}^{n_r}$$ If $\vectorfunction{X}=(X_1,\ldots,X_r)$ follows a multinomial distribution of parameters $n$ and $p_1,\ldots,p_r$, we will write $\vectorfunction{X}\sim \text{Mult}(n;p_1,\ldots,p_r)$.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a random vector. We say that $\vectorfunction{X}$ is \textit{absolutely continuous} if exists a function $f:\RR^n\rightarrow\RR$ such that:
    \begin{enumerate}
      \item $f(x)\geq 0$, $\forall x\in\RR^n$.
      \item \hfill $$\int_{-\infty}^{+\infty}\overset{(n)}{\cdots}\int_{-\infty}^{+\infty} f(x_1,\ldots,x_n)\dd x_1\cdots\dd x_n=1$$
      \item For all $B\in\mathcal{B}(\RR^n)$ we have: $$\Prob(\vectorfunction{X}\in B)=\int\overset{(n)}{\cdots}\int_Bf(x)\dd x$$
    \end{enumerate}
    The function $f$, denoted by $f_{\vectorfunction{X}}$, is called \textit{joint probability density function (joint pdf)} of $\vectorfunction{X}$.
  \end{definition}
  \begin{prop}[Marginal probability density functions]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector with density $f_{\vectorfunction{X}}$. Then, $X_i$ is an absolutely random variable with pdf:
    \begin{multline*}
      f_{X_i}(x_i)=\int_{-\infty}^{+\infty}\overset{(n-1)}{\cdots}\int_{-\infty}^{+\infty} f(x_1,\ldots,x_n)\dd x_1\cdots\dd x_{i-1}\cdot\\\cdot\dd x_{i+1}\cdots\dd x_n
    \end{multline*}
    for $i=1,\ldots, n$. These functions $f_{X_i}$ are called \textit{marginal probability density functions (marginal pdf)} of $\vectorfunction{X}$.
  \end{prop}
  \begin{definition}[Multivariate standard normal distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector. We say that $\vectorfunction{X}$ follows a \textit{multivariate normal distribution}, denoted by $\vectorfunction{X}\sim N(0,1)$, if $\vectorfunction{X}$ has a joint pdf: $$f_{\vectorfunction{X}}(x)=\frac{1}{{(2\pi)}^{\frac{n}{2}}}\exp{-\frac{{x_1}^2+\cdots+{x_n}^2}{2}}$$ Moreover, $X_i\sim N(0,1)$ for $i=1,\ldots,n$.
  \end{definition}
  \begin{definition}[Multivariate uniform distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector. We say that $\vectorfunction{X}$ has a \textit{multivariate uniform distribution over $B\in\mathcal{B}(\RR^n)$}, with $\vol(B)<\infty$, if it has joint pdf: $$f_{\vectorfunction{X}}(x)=\frac{1}{\vol(B)}\vectorfunction{1}_B(x)$$ If $\vectorfunction{X}=(X_1,\ldots,X_r)$ follows a multivariate uniform, we will write $\vectorfunction{X}\sim U(B)$.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a random vector. The \textit{multivariate cumulative distribution function (multivariate cdf)} of $\vectorfunction{X}$ is defined as: $$F_{\vectorfunction{X}}(x_1,\ldots,x_n):=\Prob(X_1\leq x_1,\ldots,X_n\leq x_n)$$ for $(x_1,\ldots,x_n)\in\RR^n$.
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}$ be a random vector. Then, $F_{\vectorfunction{X}}$ determines the distribution of $\vectorfunction{X}$.
  \end{theorem}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a random vector. Then, the multivariate cdf $F_{\vectorfunction{X}}$ of $\vectorfunction{X}$ has the following properties:
    \begin{enumerate}
      \item It is monotonically increasing in each of its variables\footnote{For the 2-dimensional case, we have that for all $x<x'$ and $y<y'$: $$F_{\vectorfunction{X}}(x',y')-F_{\vectorfunction{X}}(x,y')-F_{\vectorfunction{X}}(x',y)+F_{\vectorfunction{X}}(x,y)\geq 0$$ This positive quantity is called \textit{increment of $F_{\vectorfunction{X}}$ in the rectangle $(x,x']\times(y,y']$}. In general a function $f:\RR^2\rightarrow\RR$ satisfying $$f(x',y')-f(x,y')-f(x',y)+f(x,y)\geq 0\qquad\forall x<x'\text{ and }\forall y<y'$$ is said to be \textit{increasing}.}.
      \item It is right-continuous in each of its variables.
      \item For all $i=1,\ldots,n$ we have:
            \begin{gather*}
              \lim_{x_i\to-\infty}F_{\vectorfunction{X}}(x_1,\ldots,x_n)=0\\
              \lim_{x_1,\ldots,x_n\to+\infty}F_{\vectorfunction{X}}(x_1,\ldots,x_n)=1
            \end{gather*}
      \item For all $i=1,\ldots,n$ we have: $$\lim_{x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_n\to+\infty}F_{\vectorfunction{X}}(x_1,\ldots,x_n)=F_{X_i}(x_i)$$
      \item If $\vectorfunction{X}$ is absolutely continuous, then:
            \begin{multline*}
              F_{\vectorfunction{X}}(x_1,\ldots,x_n)=\\=\int_{-\infty}^{x_1}\cdots\int_{-\infty}^{x_n} f_{\vectorfunction{X}}(s_1,\ldots,s_n)\dd s_1\cdots\dd s_n
            \end{multline*}
      \item If $\vectorfunction{X}$ is absolutely continuous, then:
            \begin{multline*}
              \Prob(a_1<X_1<b_1,\ldots,a_n<X_n<b_n)=\\=\int_{a_1}^{b_1}\cdots\int_{a_n}^{b_n} f_{\vectorfunction{X}}(s_1,\ldots,s_n)\dd s_1\cdots\dd s_n
            \end{multline*}
    \end{enumerate}
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector. If $\vectorfunction{X}$ has a continuous joint pdf, then: $$f_{\vectorfunction{X}}(x_1,\ldots,x_n)=\frac{\partial^nF_{\vectorfunction{X}}}{\partial x_1\cdots\partial x_n}(x_1,\ldots,x_n)$$
  \end{prop}
  \subsubsection{Transformations of random vectors}
  \begin{definition}
    We say that a function $\vectorfunction{h}:\RR^n\rightarrow\RR^m$ is \textit{Borel measurable} if $\forall B\in\mathcal{B}(\RR^m)$ we have: $$\vectorfunction{h}^{-1}(B)\in\mathcal{B}(\RR^n)$$
  \end{definition}
  \begin{prop}
    Let $\vectorfunction{h}:\RR^n\rightarrow\RR^m$ be a continuous function. Then, $\vectorfunction{h}$ is Borel measurable.
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a random vector and $\vectorfunction{h}:\RR^n\rightarrow\RR^m$ be a Borel measurable function. Then, $\vectorfunction{Y}:=\vectorfunction{h}(\vectorfunction{X})$ is also a random vector.
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $U,V\subseteq\RR^n$ be open sets and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector with joint pdf $f_{\vectorfunction{X}}$ such that $\Prob(\vectorfunction{X}\in U)=1$. Let $\vectorfunction{h}:U\rightarrow V$ be a diffeomorphism of class $\mathcal{C}^1$. Then, $\vectorfunction{Y}:=\vectorfunction{h}(\vectorfunction{X})$ is absolutely continuous and $$f_{\vectorfunction{Y}}(y)=f_{\vectorfunction{X}}(\vectorfunction{h}^{-1}(y))|J\vectorfunction{h}^{-1}(y)|\vectorfunction{1}_V(y)$$
    where $J\vectorfunction{h}^{-1}(y)=\det\vectorfunction{D}\vectorfunction{h}^{-1}(y)$ is the Jacobian of $\vectorfunction{h}^{-1}$ evaluated at $y$.
  \end{prop}
  \begin{definition}[Multivariate normal distribution]
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $\vectorfunction{X}\sim N(0,1)$ be an absolutely continuous random vector and $\vectorfunction{h}:\RR^n\rightarrow\RR^n$ be a function defined as $$\vectorfunction{h}(\vectorfunction{x})=\vectorfunction{A}\vectorfunction{x}+\vectorfunction{b}\qquad\vectorfunction{x}\in\RR^n\footnote{Here, we have thought $(x_1,\ldots,x_n)$ as the vector $\vectorfunction{x}$ in $\RR^n$.}$$ where $\vectorfunction{A}\in\GL_n(\RR)$ and $\vectorfunction{b}\in\RR^n$. Then, $\vectorfunction{Y}=\vectorfunction{h}(\vectorfunction{X})$ is an absolutely continuous random vector and it has joint pdf: $$f_{\vectorfunction{Y}}(\vectorfunction{y})=\frac{1}{{(2\pi)}^{\frac{n}{2}}}\frac{1}{\det \vectorfunction{A}}\exp{-\frac{{\|\vectorfunction{A}^{-1}(\vectorfunction{y}-\vectorfunction{b})\|}^2}{2}}$$
    In this case, we write $\vectorfunction{Y}\sim N(\vectorfunction{b},\vectorfunction{A}\transpose{A})$. The vector $\vectorfunction{b}$ is called \textit{mean vector} and the matrix $\vectorfunction{A}\transpose{A}$, \textit{covariance matrix}.
  \end{definition}
  \subsubsection{Independent random variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be random variables. We say that they are \textit{independent} if $\forall B_1,\ldots,B_n\in\mathcal{B}(\RR)$, we have: $$\Prob(X_1\in B_1,\ldots,X_n\in B_n)=\prod_{i=1}^n\Prob(X_i\in B_i)$$ That is, the events $\{X_1\in B_1\},\ldots,\{X_n\in B_n\}$ are independent.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $X_1,\ldots,X_n$ be independent random variables and $g_1,\ldots,g_n:\RR\rightarrow\RR$ be Borel measurable functions. Then, $Y_1=g_1(X_1),\ldots,Y_n=g_n(X_n)$ are also independent random variables.
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be random variables. Then, $X_1,\ldots,X_n$ are independent if and only if $$F_{(X_1,\ldots,X_n)}(x_1,\ldots,x_n)=F_{X_1}(x_1)\cdots F_{X_n}(x_n)$$ for all $(x_1,\ldots,x_n)\in\RR^n$.
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be a discrete random vector with support $S_{\vectorfunction{X}}$. Then, $X_1,\ldots,X_n$ are independent if and only if $$p_{\vectorfunction{X}}(x_1,\ldots,x_n)=p_{X_1}(x_1)\cdots p_{X_n}(x_n)$$ for all $(x_1,\ldots,x_n)\in S_{\vectorfunction{X}}$.
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $\vectorfunction{X}=(X_1,\ldots,X_n)$ be an absolutely continuous random vector. Then, $X_1,\ldots,X_n$ are independent if and only if $$f_{\vectorfunction{X}}(x_1,\ldots,x_n)=f_{X_1}(x_1)\cdots f_{X_n}(x_n)$$ for all $(x_1,\ldots,x_n)\in \RR^n$, except for, maybe, a null set.
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $X_1,\ldots,X_n$ be absolutely continuous and independent random variables. Then, $\vectorfunction{X}:=(X_1,\ldots,X_n)$ is an absolutely continuous random vector and $$f_{\vectorfunction{X}}(x_1,\ldots,x_n)=f_{X_1}(x_1)\cdots f_{X_n}(x_n)$$ for all $(x_1,\ldots,x_n)\in \RR^n$, except for, maybe, a null set.
  \end{prop}
  \subsubsection{Conditional distributions}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X,Y)$ be a discrete random vector with support $S_X\times S_Y$ and $y\in S_Y$. The \textit{conditional probability mass function of $X$ given $Y=y$} is defined as: $$p_{X\mid Y}(x\mid y):=\Prob(X=x\mid Y=y)=\frac{p_{(X,Y)}(x,y)}{p_Y(y)}$$ for all $x\in S_X$.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X,Y)$ be a discrete random vector with support $S_X\times S_Y$. Then, the pmf of $Y$ together with the pmf of $X$ conditioned to $Y=y$ determine the pmf of $X$ in the following way: $$\Prob(X=x)=\sum_{y\in S_Y}p_{X\mid Y}(x\mid y)p_Y(y)\quad\forall x\in S_X$$
  \end{prop}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X,Y)$ be an absolutely continuous random vector and $y\in S_Y$. The \textit{conditional probability density function of $X$ given $Y=y$} is defined as: $$f_{X\mid Y}(x\mid y):=\left\{
      \begin{array}{ccc}
        \frac{f_{(X,Y)}(x,y)}{f_Y(y)} & \text{if} & f_Y(y)>0 \\
        a                             & \text{if} & f_Y(y)=0
      \end{array}\right.
    $$ where $x\in\RR$ and $a\in\RR$ is an arbitrary value\footnote{Usually chosen equal to 0.}.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space, $(X,Y)$ be an absolutely continuous random vector, $y\in S_Y$ and $a,b\in\RR\cup\{\pm\infty\}$ such that $a<b$. Then: $$\Prob(X\in(a,b)\mid Y=y)=\int_a^bf_{X\mid Y}(x\mid y)\dd x$$
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space and $(X,Y)$ be an absolutely continuous random vector. Then, the pdf of $Y$ together with the pdf of $X$ conditioned to $Y=y$ determine the pdf of $X$ in the following way: $$f_X(x)=\int_{-\infty}^{+\infty}f_{(X,Y)}(x,y)\dd y=\int_{-\infty}^{+\infty}f_{X\mid Y}(x\mid y)f_Y(y)\dd y$$ for all $x\in\RR$.
  \end{prop}
  \subsection{Expectation}
  %ATENCIÓ: absolutely continuous =? everywhere continuous
\end{multicols}
\end{document}