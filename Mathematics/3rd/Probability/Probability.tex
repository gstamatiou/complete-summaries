\documentclass[../../../main.tex]{subfiles}

% 3 line brake in Bernoulli, Binomial and Geometric distributions.

\begin{document}
\begin{multicols}{2}[\section{Probability}]
  \subsection{Probabilistic models}
  \subsubsection*{Probability}
  \begin{definition}[Sample space]
    The \textit{sample space} $\Omega$ of an experiment is the set of all possible outcomes of that experiment.
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set. An \textit{event} is a subset of $\mathcal{P}(\Omega)$ for which we want to calculate the probability.
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set and $\mathcal{A}\subset\mathcal{P}(\Omega)$. We say that $\mathcal{A}$ is an \textit{algebra over $\Omega$} if:
    \begin{enumerate}
      \item $\Omega\in\mathcal{A}$.
      \item If $A\in\mathcal{A}$, then $A^c\in\mathcal{A}$.
      \item If $A,B\in\mathcal{A}$, then $A\cup B\in\mathcal{A}$.
    \end{enumerate}
  \end{definition}
  \begin{prop}
    Let $\mathcal{A}$ be an algebra over a set $\Omega$. Then:
    \begin{enumerate}
      \item $\varnothing\in\mathcal{A}$.
      \item If $A,B\in\mathcal{A}$, then $A\cap B\in\mathcal{A}$.
      \item For all $n\in\NN$, if $A_1,\ldots,A_n\in\mathcal{A}$, then: $$\bigcup_{i=1}^nA_i\in\mathcal{A}\quad\text{and}\quad\bigcap_{i=1}^nA_i\in\mathcal{A}$$
    \end{enumerate}
  \end{prop}
  \begin{definition}
    Let $\Omega$ be a set and $\mathcal{A}\subset\mathcal{P}(\Omega)$. We say that $\mathcal{A}$ is an \textit{$\sigma$-algebra over $\Omega$} if:
    \begin{enumerate}
      \item $\Omega\in\mathcal{A}$.
      \item If $A\in\mathcal{A}$, then $A^c\in\mathcal{A}$.
      \item If $A_1,A_2,\ldots\in\mathcal{A}$, then: $$\bigcup_{n=1}^\infty A_n\in\mathcal{A}$$
    \end{enumerate}
  \end{definition}
  \begin{prop}
    Let $\mathcal{A}$ be an $\sigma$-algebra over a set $\Omega$. Then:
    \begin{enumerate}
      \item $\varnothing\in\mathcal{A}$.
      \item If $A_1,A_2,\ldots\in\mathcal{A}$, then: $$\bigcap_{n=1}^\infty A_n\in\mathcal{A}$$
      \item For all $n\in\NN$, if $A_1,\ldots,A_n\in\mathcal{A}$, then: $$\bigcup_{i=1}^nA_i\in\mathcal{A}\quad\text{and}\quad\bigcap_{i=1}^nA_i\in\mathcal{A}$$
    \end{enumerate}
  \end{prop}
  \begin{definition}[Kolmogorov axioms]
    Let $\Omega$ be a set and $\mathcal{A}$ be a $\sigma$-algebra over $\Omega$. A \textit{probability} is any function $$P:\mathcal{A}\longrightarrow[0,\infty)$$ satisfying the following properties:
    \begin{itemize}
      \item $P(\Omega)=1$.
      \item (\textit{$\sigma$-additive set function}) If $\{A_n,n\geq1\}\subset\mathcal{A}$ are pairwise disjoint, then: $$P\left(\bigcup_{n=1}^\infty A_n\right)=\sum_{n=1}^\infty P(A_n)$$
    \end{itemize}
  \end{definition}
  \begin{definition}
    A \textit{probability space} is a triplet $(\Omega,\mathcal{A},P)$ where $\Omega$ is any set, $\mathcal{A}$ is a $\sigma$-algebra over $\Omega$ and $P$ is a probability over $\mathcal{A}$.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $A,B\in\mathcal{A}$. Then, we have the following properties:
    \begin{enumerate}
      \item $P(\varnothing)=0$.
      \item If $A_i\in\mathcal{A}$, $i=1,\ldots,n$, is a finite set of pairwise disjoint events, then: $$P\left(\bigsqcup_{i=1}^n A_i\right)=\sum_{i=1}^n P(A_i)$$
      \item $P(A\setminus B) =P(A)-P(A\cap B)$.
      \item If $B\subset A$, then $P(A\setminus B)=P(A)-P(B)$.
      \item If $B\subset A$, then $P(B)\leq P(A)$.
      \item $P(A)\leq 1$.
      \item $P(A^c)=1-P(A)$.
      \item $P(A\cup B) =P(A)+P(B)-P(A\cap B)$.
      \item If $A_i,\ldots,A_n\in\mathcal{A}$, then:
            \begin{multline*}
              P\left(\bigcup_{i=1}^n A_i\right)=\sum_{i=1}^n P(A_i)-\\-\sum_{\substack{i,j=1\\i<j}}^nP(A_i\cap A_j)+\sum_{\substack{i,j,k=1\\i<j<k}}^nP(A_i\cap A_j\cap A_k)-\cdots+\\+{(-1)}^{n+1}P(A_1\cap\cdots\cap A_n)
            \end{multline*}
      \item If $A_i,\ldots,A_n\in\mathcal{A}$, then: $$P\left(\bigcup_{i=1}^n A_i\right)\leq\sum_{i=1}^n P(A_i)$$
    \end{enumerate}
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space such that $\Omega$ is finite and all its elements are equiprobable. Let $A\in\mathcal{A}$ be an event. Then: $$P(A)=\frac{|A|}{|\Omega|}$$
  \end{prop}
  \begin{definition}
    Let $\Omega$ be a set. The \textit{trivial $\sigma$-algebra} is the smallest $\sigma$-algebra over $\Omega$, that is, $\{\varnothing,\Omega\}$.
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set and $A\subseteq\Omega$ be a subset. The \textit{$\sigma$-algebra generated by $A$, $\sigma(A)$,} is the smallest $\sigma$-algebra over $\Omega$ containing $A$, that is: $$\sigma(A)=\{\varnothing,\Omega,A,A^c\}$$
  \end{definition}
  \begin{definition}
    Let $\Omega$ be a set and $\mathcal{C}\subseteq\mathcal{P}(\Omega)$ be a subset. The \textit{$\sigma$-algebra generated by $\mathcal{C}$, $\sigma(\mathcal{C})$,} is the smallest $\sigma$-algebra over $\Omega$ containing all the elements of $\mathcal{C}$. Moreover, if $\{\mathcal{A}_n:\mathcal{C}\subseteq\mathcal{A}_n,1\leq n\leq N\}$, $N\in\NN\cup\{\infty\}$, are all the $\sigma$-algebras over $\Omega$ containing $\mathcal{C}$, then:
    $$\sigma(\mathcal{C})=\bigcap_{n=1}^N\mathcal{A}_n$$
  \end{definition}
  \begin{definition}
    The \textit{Borel $\sigma$-algebra over $\RR$, $\mathcal{B}(\RR)$,} is the $\sigma$-algebra generated by the open sets of $\RR$: $$\mathcal{B}(\RR):=\sigma(\{U\subseteq\RR:U\text{ is open}\})$$
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $\{A_n:n\geq 1\}\subset\mathcal{A}$ be an increasing sequence of events, so that: $$A_1\subset A_2\subset\cdots\subset A_n\subset\cdots$$ Let $A:=\bigcup_{n=1}^\infty A_n$. Then: $$P(A):=\lim_{n\to\infty}P(A_n)$$
  \end{theorem}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $\{A_n:n\geq 1\}\subset\mathcal{A}$ be an decreasing sequence of events, so that: $$A_1\supset A_2\supset\cdots\supset A_n\supset\cdots$$ Let $A:=\bigcap_{n=1}^\infty A_n$. Then: $$P(A):=\lim_{n\to\infty}P(A_n)$$
  \end{corollary}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $\{A_n:n\geq 1\}\subset\mathcal{A}$ be a sequence of events. Then: $$P\left(\bigcup_{n=1}^\infty A_n\right)\leq\sum_{n=1}^\infty P(A_n)$$
  \end{prop}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $\{A_n:n\geq 1\}\subset\mathcal{A}$ be a sequence of events with probability 0. Then: $$P\left(\bigcup_{n=1}^\infty A_n\right)=0$$
  \end{corollary}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $\{A_n:n\geq 1\}\subset\mathcal{A}$ be a sequence of events with probability 1. Then: $$P\left(\bigcap_{n=1}^\infty A_n\right)=1$$
  \end{corollary}
  \subsubsection*{Conditional probability}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $A\in\mathcal{A}$ be an event such that $P(A)>0$. The \textit{conditional probability that $B\in\mathcal{A}$ occurs given that $A$ occurs} is defined as: $$P(B\mid A):=\frac{P(A\cap B)}{P(A)}$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $A\in\mathcal{A}$ be an event such that $P(A)>0$. Then, the function
    \begin{align*}
      P(\cdot\mid A):\mathcal{A} & \longrightarrow [0,\infty] \\
      B                          & \longmapsto P(B\mid A)
    \end{align*}
    is a probability.
  \end{prop}
  \begin{prop}[Compound probability formula]
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $A\in\mathcal{A}$ be an event such that $P(A)>0$. Then, $\forall B\in\mathcal{A}$: $$P(A\cap B)=P(B\mid A)P(A)$$
  \end{prop}
  \begin{prop}[Generalized compound probability formula]
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $A_1,\ldots,A_n\in\mathcal{A}$, $n\geq 2$ be events such that $P(A_1\cap\cdots\cap A_{n-1})>0$. Then:
    \begin{multline*}
      P(A_1\cap\cdots\cap A_n)=P(A_1)P(A_2\mid A_1)P(A_3\mid A_2\cap A_1)\cdots\\\cdots P(A_n\mid A_1\cap\cdots\cap A_{n-1})
    \end{multline*}
  \end{prop}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $A=\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be a collection of events. We say that $A$ is a \textit{partition} of $\Omega$ if: $$\Omega=\bigsqcup_{n=1}^NA_n$$
  \end{definition}
  \begin{prop}[Total probability formula]
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be a partition of $\Omega$ such that $P(A_n)>0$ for all $1\leq n\leq N$. Then, $\forall A\in\mathcal{A}$: $$P(A)=\sum_{n=1}^NP(A_n)P(A\mid A_n)$$
  \end{prop}
  \begin{prop}[Bayes' formula]
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $\{A_n:1\leq n\leq N\}\subset\mathcal{A}$, $N\in\NN\cup\{\infty\}$, be a partition of $\Omega$ such that $P(A_n)>0$ for all $1\leq n\leq N$. Let $A\in\mathcal{A}$ with $P(A)>0$. Then, $\forall k\leq N$: $$P(A_k\mid A)=\frac{P(A_k)P(A\mid A_k)}{\sum_{n=1}^NP(A_n)P(A\mid A_n)}$$
  \end{prop}
  \subsubsection*{Independence of events}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. Then, $A,B\in\mathcal{A}$ are \textit{independent events} if $$P(A\cap B)=P(A)P(B)$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. Then:
    \begin{enumerate}
      \item $\varnothing$ and $\Omega$ are independent of any event.
      \item If $A\in\mathcal{A}$ satisfies either $P(A)=0$ or $P(A)=1$, then $A$ is independent of any other event $B\in\mathcal{A}$.
      \item If an event $A\in\mathcal{A}$ is independent of itself, then either $P(A)=0$ or $P(A)=1$.
    \end{enumerate}
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $A,B\in\mathcal{A}$ be two events. The following statements are equivalent:
    \begin{itemize}
      \item $A$ and $B$ are independent.
      \item $A^c$ and $B$ are independent.
      \item $A$ and $B^c$ are independent.
      \item $A^c$ and $B^c$ are independent.
    \end{itemize}
  \end{prop}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $n\in\NN$. We say that $A_1,\ldots,A_n\in\mathcal{A}$ are \textit{independent events} if for any $i_1,\ldots,i_k\in\{1,\ldots,n\}$, we have: $$P\left(\bigcap_{r=1}^kA_{i_r}\right)=\prod_{r=1}^kP(A_{i_r})$$
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $I$ be an arbitrary index set. We say that $\{A_i:i\in I\}\subset\mathcal{A}$ are \textit{independent events} if for any finite subset $A_{i_1},\ldots A_{i_r}$ of different events, we have: $$P\left(\bigcap_{r=1}^kA_{i_r}\right)=\prod_{r=1}^kP(A_{i_r})$$
  \end{definition}
  \subsection{Random variables and random vectors}
  \subsubsection*{Random variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. A \textit{random variable $X$} is a function $X:\Omega\rightarrow\RR$ satisfying for all $B\in\mathcal{B}(\RR)$: $$X^{-1}(B)=\{\omega\in\Omega:X(\omega)\in B\}\subset\mathcal{A}$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space, $\mathcal{C}$ be a collection of subsets of $\RR$ such that $\mathcal{B}(\RR)=\sigma(\mathcal{C})$ and let $X:\Omega\rightarrow\RR$ be a function. Then, $X$ is a random variable if and only if $X^{-1}(B)\in\mathcal{A}$, $\forall B\in C$.
  \end{prop}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space, $a,b\in\RR$ and $B\in\mathcal{B}(\RR)$. We define the following sets:
    \begin{enumerate}
      \item $\displaystyle\{X\in B\}:=\{\omega\in\Omega:X(\omega)\in B\}=X^{-1}(B)$
      \item $\displaystyle\{X\leq a\}:=\{\omega\in\Omega:X(\omega)\leq a\}=X^{-1}((-\infty,a])$
      \item $\displaystyle\{X> b\}:=\{\omega\in\Omega:X(\omega)>b\}=X^{-1}((b,\infty))$
      \item $\displaystyle\{X=a\}:=\{\omega\in\Omega:X(\omega)=a\}=X^{-1}(\{a\})$
    \end{enumerate}
  \end{definition}
  \begin{prop}
    Let $S=(\Omega,\mathcal{A},P)$ be a probability space, $X,Y$ be random variables defined on $S$ and $a\in\RR$. Then:
    \begin{enumerate}
      \item $X+Y$ is also a random variable.
      \item $aX$ is also a random variable.
      \item $XY$ is also a random variable.
      \item $\frac{1}{X}$ is also a random variable if $X(\omega)\ne 0$ $\forall \omega\in\Omega$.
    \end{enumerate}
  \end{prop}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $(X_n)_{n\geq 1}$ be a sequence of random variables. Then:
    \begin{enumerate}
      \item $\sup\{X_n:n\geq 1\}$ is also a random variable.
      \item $\inf\{X_n:n\geq 1\}$ is also a random variable.
      \item $\limsup\{X_n:n\geq 1\}$ is also a random variable.
      \item $\liminf\{X_n:n\geq 1\}$ is also a random variable.
    \end{enumerate}
    provided that these quantities are finite for all $\omega\in\Omega$.
  \end{prop}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $(X_n)_{n\geq 1}$ be a sequence of random variables such that $\forall\omega\in\Omega$ the following limit exists and it is finite: $$X(\omega):=\lim_{n\to\infty}X_n(\omega)$$
    Then, $X$ is a random variable.
  \end{corollary}
  \subsubsection*{Distribution of a random variable}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. The \textit{distribution of a random variable $X$} is the function:
    \begin{align*}
      P_X:\mathcal{B}(\RR) & \longrightarrow [0,1]            \\
      B                    & \longmapsto P_X(B):=P(X^{-1}(B))
    \end{align*}
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. For any random variable $X$, the function $P_X$ defines a probability over $\RR$, with the Borel $\sigma$-algebra $\mathcal{B}(\RR)$.
  \end{prop}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space, $X$ be a random variable and $B\in\mathcal{B}(\RR)$. We define: $$P(X\in B):=P(\{X\in B\})$$
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. We say that two random variables $X,Y$ are \textit{equal in distribution} (denoted by $X\overset{\text{d}}{=}Y$) if they satisfy: $$P_X(B)=P_Y(B)\qquad\forall B\in\mathcal{B}(\RR)$$ That is, $X\overset{\text{d}}{=}Y$ if they have the same distribution functions.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. We say that two random variables $X,Y$ are \textit{equal almost surely} (denoted by $X\overset{\text{a.s.}}{=}Y$) if $P(X=Y)=1$, or equivalently, if $P(X\ne Y)=0$.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $X,Y$ be two random variables such that $X\overset{\text{a.s.}}{=}Y$. Then, we have $X\overset{\text{d}}{=}Y$
  \end{prop}
  \begin{definition}[Cumulative distribution function]
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $X$ be a random variable. We define the \textit{cumulative distribution function (cdf)} as: \begin{align*}
      F_X:\RR & \longrightarrow [0,1]          \\
      x       & \longmapsto F_X(x):=P(X\leq x)
    \end{align*}
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space, $X$ be a random variable and $F_X$ be its cdf. Then:
    \begin{enumerate}
      \item\label{P_1} If $x<y$, then $F_X(x)\leq F_X(y)$.
      \item\label{P_2} $F_X$ is \textit{càdàg}\footnote{From French ``continue à droite, limite à gauche'' (right continuous with left limits).}.
      \item\label{P_3} $\displaystyle\lim_{x\to -\infty}F_X(x)=0$ and $\displaystyle\lim_{x\to \infty}F_X(x)=1$.
      \item $F_X$ has at most a countable number of discontinuities.
      \item $\forall s,t\in\RR$ such that $s<t$, we have: $$P(s<X\leq t)=F(t)-F(s)$$
      \item $\displaystyle\forall x\in\RR$, $\displaystyle P(X<x)=\lim_{t\to x^-}F_X(t)$.
      \item For all $x\in\RR$: $$P(X=x)=F_X(x)-\lim_{t\to x^-}F_X(t)$$ That is, $F_X$ is discontinues at $x$ if and only if $P(X=x)>0$.
    \end{enumerate}
  \end{prop}
  \begin{theorem}
    Let $F$ be a function satisfying properties \ref{P_1}, \ref{P_2} and \ref{P_3}. Then, there exists a random variable $X$ on a a probability space $(\Omega,\mathcal{A},P)$ such that $F$ is its distribution property.
  \end{theorem}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. Then, the cdf completely determine a distribution of a random variable $X$. That is, if $X$ and $Y$ are random variables such that $F_X(t)=F_Y(t)$ $\forall t\in\RR$, then $P_X(B)=P_Y(B)$ $\forall B\in\mathcal{B}(\RR)$.
  \end{theorem}
  \subsubsection*{Discrete random  variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. We say that a random variable $X$ is \textit{discrete} if there exists a finite or countable set $S\subset\RR$ such that $P(X\in S)=1$. This set is called \textit{support of $X$}\footnote{By agreement, we will suppose that $S$ only contains points $x$ such that $P(X=x)>0$.}.
  \end{definition}
  \begin{definition}[Probability mass function]
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $X$ be a discrete random variable with support points $S=\{x_i:i\in I\}$, where $I\subseteq\NN$. The \textit{probability mass function (pmf)} of the random variable $X$ is:
    \begin{align*}
      p_X:S & \longrightarrow [0,1]          \\
      x_i   & \longmapsto p_X(x_i):=P(X=x_i)
    \end{align*}
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space, $X$ be a discrete random variable with support points $S=\{x_i:i\in I\}$ and $p_X$ be its pmf. Then:
    \begin{enumerate}
      \item $p_X(x_i)>0$ $\forall i\in I$.
      \item $\displaystyle\sum_{i\in I}p_X(x_i)=1$.
      \item $\forall B\in\mathcal{B}(\RR)$, we have: $$P(X\in B)=\sum_{i\in I:x_i\in B}p_X(x_i)$$
    \end{enumerate}
  \end{prop}
  \begin{corollary}
    Let $(\Omega,\mathcal{A},P)$ be a probability space, $X$ be a discrete random variable with support points $S=\{x_i:i\in I\}$, $F_X$ be its cdf and $p_X$ be its pmf. Then $\forall x\in\RR$ we have: $$F_X(x)=P(X\leq x)=\sum_{i\in I:x_i\leq x}p_X(x_i)$$
  \end{corollary}
  \begin{definition}[Degenerated distribution]
    Let $(\Omega,\mathcal{A},P)$ be a probability space. The \textit{degenerated distribution} consists in taking a constant random variable $X$ so that $$P(X=a)=1$$ for some $a\in\RR$. Here we have $S=\{a\}$.
  \end{definition}
  \begin{definition}[Bernoulli distribution]
    Let\\ $(\Omega,\mathcal{A},P)$ be a probability space. The \textit{Bernoulli distribution} is the one that can only take two values (1 and 0)\footnote{Also called \textit{success}/\textit{true} and \textit{failure}/\textit{false} respectively.} with probabilities $p$ and $q:=1-p$: $$P(X=0)=p\qquad P(X=1)=q$$ for some $a\in\RR$. Here we have $S=\{0,1\}$. If $X$ follows a Bernoulli distribution, we will write $X\sim \text{B}(p)$.
  \end{definition}
  \begin{definition}[Discrete uniform distribution]
    Let $(\Omega,\mathcal{A},P)$ be a probability space. The \textit{discrete uniform distribution} is the one whose random variable $X$ takes values on $S=\{x_1,\ldots,x_n\}$ each of these with probability $1/n$: $$P(X=x_i)=\frac{1}{n}\qquad \forall i=1,\ldots,n$$ If $X$ follows a discrete uniform distribution, we will write $X\sim \mathcal{U}\{x_1,\ldots,x_n\}$.
  \end{definition}
  \begin{definition}[Binomial distribution]
    Let\\ $(\Omega,\mathcal{A},P)$ be a probability space and $A\in\mathcal{A}$. Suppose $P(A)=p$. The \textit{binomial distribution} is the one whose random variable $X$ is the number of successes of $A$ in a sequence of $n$ repetitions. Thus, $S=\{0,1,\ldots,n\}$ and: $$P(X=k)=\binom{n}{k}p^k{(1-p)}^{n-k}\qquad \forall k=0,1,\ldots,n$$ If $X$ follows a binomial distribution of parameters $n$ and $p$, we will write $X\sim B(n,p)$.
  \end{definition}
  \begin{definition}[Poisson distribution]
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $\lambda>0$. The \textit{Poisson distribution with parameter $\lambda$} is the one whose random variable has support $S=\NN\cup\{0\}$ and: $$P(X=k)=\exp{-\lambda}\frac{\lambda^k}{k!}\qquad \forall k\in\NN\cup\{0\}$$ If $X$ follows a Poisson distribution of parameter $\lambda$, we will write $X\sim \text{Pois}(\lambda)$.
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. Let $(p_n)\subset(0,1)$ be a sequence such that: $$\lim_{n\to\infty}np_n=\lambda>0$$
    For each $n\geq 1$, consider $X_n\sim B(n,p_n)$. Then, $\forall k\in\NN\cup\{0\}$ we have: $$\lim_{n\to\infty}P(X_n=k)=\lim_{n\to\infty}\binom{n}{k}{p_n}^k{(1-p_n)}^{n-k}=\exp{-\lambda}\frac{\lambda^k}{k!}$$
  \end{theorem}
  \begin{definition}[Geometric distribution]
    Let\\ $(\Omega,\mathcal{A},P)$ be a probability space and $A\in\mathcal{A}$. Suppose $P(A)=p$. The \textit{geometric distribution} is the one whose random variable $X$ is the number of trials needed to get one success. Thus, $S=\NN$ and: $$P(X=k)={(1-p)}^{k-1}p\qquad \forall k\in\NN$$ If $X$ follows a geometric distribution of parameters $p$, we will write $X\sim \text{Geo}(n,p)$.
  \end{definition}
  \begin{definition}[Discrete memorylessness property]
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $X$ be a discrete random variable whose support is $\NN\cup\{0\}$ and such that $P(X >m)>0$ $\forall m\in\NN\cup\{0\}$. The \textit{distribution of $X$ is memoryless} if $\forall m,n\in\NN\cup\{0\}$, we have: $$P(X>m+n\mid X >m)=P(X>n)$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space, $X$ be a discrete random variable and $p\in(0,1)$. Then, $X\sim\text{Geo}(p)$ if and only if the distribution of $X$ is memoryless.
  \end{prop}
  \begin{definition}[Hypergeometric distribution]
    Let $(\Omega,\mathcal{A},P)$ be a probability space. Suppose we have a population of size $N$ of whom $K$ have a special feature (success). Let $X$ be the random variable that counts the number of successes that we have obtained in $n$ draws (without replacement). Thus, the support if $X$ is: $$S=\{\max\{n+K-N,0\},\ldots,\min\{n,K\}\}$$ And the probability is given by: $$P(X=j)=\frac{\binom{K}{j}\binom{N-K}{n-j}}{\binom{N}{n}}$$ This type of distribution is called \textit{hypergeometric distribution} and it is denoted by $X\sim \text{HG}(N,p,n)$ where $p=\frac{K}{N}$ is the proportion of successes in the population.
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $X\sim \text{HG}(N,p,n)$ such that when $N\to\infty$, $p\frac{K}{N}$ remains constant. Then:
    $$\lim_{N\to\infty}P(X=j)=\lim_{N\to\infty}\frac{\binom{K}{j}\binom{N-K}{n-j}}{\binom{N}{n}}=\binom{n}{j}p^j{(1-p)}^{n-j}$$ which is the probability function of a binomial distribution $B(n,p)$.
  \end{theorem}
  \begin{definition}[Negative binomial distribution]
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $A\in\mathcal{A}$. Suppose $P(A)=p$. The \textit{negative binomial distribution} is the one whose random variable $X$ is the number of trials needed to get one $r\geq 1$ successes. Thus, $S=\{r,r+1,\ldots\}$ and: $$P(X=k)=\binom{k-1}{r-1}p^r{(1-p)}^{k-r}\qquad \forall k\geq r$$ If $X$ follows a negative binomial distribution of parameters $r$ and $p$, we will write $X\sim NB(r,p)$.
  \end{definition}
  \subsubsection*{Absolutely continuous random variables}
  \begin{definition}
    Let $(\Omega,\mathcal{A},P)$ be a probability space. We say that a random variable $X$ is \textit{absolutely continuous} if there exists a function $f:\RR\rightarrow\RR$ satisfying:
    \begin{enumerate}
      \item $f(x)\geq 0$, $\forall x\in\RR$.
      \item $f$ is integrable over $\RR$ and: $$\int_{-\infty}^{+\infty}f(x)\dd x=1$$
      \item For all $a,b\in\RR\cup\{\pm\infty\}$ with $a\leq b$, we have: $$P(a\leq X\leq b)=\int_a^bf(x)\dd x$$
    \end{enumerate}
    The function $f$ is called \textit{probability density function (pdf)} of $X$.
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space, $X$ be an absolutely continuous random variable and $F_X$ be its cdf. Then:
    \begin{enumerate}
      \item $P(X=a)=0\quad\forall a\in \RR$
      \item $\displaystyle F_X(b)=P(X\leq b)=\int_{-\infty}^bf(x)\dd x\quad\forall b\in\RR$
      \item $F_X$ is continuous on $\RR$.
      \item If $a,b\in\RR$ are such that $a<b$, then:
            \begin{multline*}
              P(a<X<b)=P(a\leq X<b)=\\=P(a<X\leq b)=P(a\leq X\leq b)
            \end{multline*}
    \end{enumerate}
  \end{prop}
  \begin{definition}[Continuous uniform distribution]
    Let $(\Omega,\mathcal{A},P)$ be a probability space. We say that an absolutely continuous random variable $X$ follows a \textit{continuous uniform distribution on $(a,b)$} (also $[a,b]$), and we denoted it by $X\sim U(a,b)$, if $X$ has the pdf $$f(x)=\frac{1}{b-a}\vectorfunction{1}_{(a,b)}(x)$$ where $\vectorfunction{1}_{(a,b)}$ is the indicator function. Its cdf is:
    \begin{multline*}
      F_X(x)=\left\{
      \begin{array}{ccl}
        0                            & \text{if} & x\leq a \\
        \displaystyle\frac{x-a}{b-a} & \text{if} & a<x<b   \\
        1                            & \text{if} & x\geq b \\
      \end{array}
      \right.=\\=\frac{x-a}{b-a}\vectorfunction{1}_{(a,b)}(x)+\vectorfunction{1}_{[b,\infty)}(x)
    \end{multline*}
  \end{definition}
  \begin{definition}[Exponential distribution]
    Let $(\Omega,\mathcal{A},P)$ be a probability space. We say that an absolutely continuous random variable $X$ follows an \textit{exponential distribution of parameter $\lambda>0$}, and we denoted it by $X\sim \text{Exp}(\lambda)$, if $X$ has the pdf $$f(x)=\lambda\exp{-\lambda x}\vectorfunction{1}_{(0,\infty)}$$ Furthermore, its cdf is:
    $$F_X(x)=(1-\exp{-\lambda x})\vectorfunction{1}_{(0,\infty)}(x)$$
  \end{definition}
  \begin{definition}[Continuous memorylessness property]
    Let $(\Omega,\mathcal{A},P)$ be a probability space and $X$ be an absolutely continuous random variable such that $P(X >s)>0$ $\forall s\in\RR_{\geq 0}$. The \textit{distribution of $X$ is memoryless} if $\forall s,t\in\RR_{\geq 0}$, we have: $$P(X>s+t\mid X >s)=P(X>t)$$
  \end{definition}
  \begin{prop}
    Let $(\Omega,\mathcal{A},P)$ be a probability space, $X$ be an absolutely continuous random variable and $\lambda\in\RR_{>0}$. Then, $X\sim\text{Exp}(\lambda)$ if and only if the distribution of $X$ is memoryless.
  \end{prop}
\end{multicols}
\end{document}