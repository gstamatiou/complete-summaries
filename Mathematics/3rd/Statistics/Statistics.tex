\documentclass[../../../main.tex]{subfiles}
% break in parametric statistical model

\begin{document}
\begin{multicols}{2}[\section{Statistics}]
  \subsection{Point estimation}
  \subsubsection{Stadistical models}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space\footnote{From now on we will assume that the random variables are defined always in the same probability space $(\Omega,\mathcal{A},\Prob)$, so we will omit to say that.}, $\Theta$ be a set, $n\in\NN$ and $x_1,\ldots,x_n$ be a collection of data that we may assume that they are the outcomes of a random vector $\vf{X}_n=(X_1,\ldots,X_n)$ defined on $(\Omega,\mathcal{A},\Prob)$. Suppose, moreover, that the outcomes of $\vf{X}_n$ are in a set $\mathcal{X}\subseteq\RR^n$, the law $\vf{X}_n$ is one in the set $\mathcal{P}=\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\}$ and $\mathcal{F}$ is a $\sigma$-algebra over $\mathcal{X}$\footnote{That is, $\mathcal{P}$ denotes a family of probability distributions of $\vf{X}_n$ in $(\mathcal{X},\mathcal{F})$, indeXed by $\theta\in\Theta$. Note that we denote that distribution of $\vf{X}_n$ by $\Prob^{\vf{X}_n}$ to distinguish it from the probability distribution $\Prob_{\vf{X}_n}$ in $(\Omega,\mathcal{A},\Prob)$.}. We define a \emph{statistical model} as the triplet $(\mathcal{X},\mathcal{F},\mathcal{P})$\footnote{Often we will take $\mathcal{F}=\mathcal{B}(\mathcal{X})$.}. The set $\mathcal{X}$ is called \emph{sample space}, and the set $\Theta$, \emph{parameter space}. The random vector $\vf{X}_n$ is called \emph{random sample}. If, moreover, $X_1,\ldots,X_n$ are i.i.d. random variables, $\vf{X}_n$ is called a \emph{simple random sample}. The value $(x_1,\ldots,x_n)\in\mathcal{X}$ is called a \emph{realization} of $(X_1,\ldots,X_n)$.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We say $\mathcal{P}=\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ is \emph{identificable} if the function $$\function{}{\Theta}{\mathcal{P}}{\theta}{\Prob_\theta^{\vf{X}_n}}$$ is injective\footnote{From now on, we will suppose that all the sets $\mathcal{P}$ are always identificable.}.
  \end{definition}
  \begin{definition}
    A statistical model $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ is said to be \emph{parametric} if $\Theta\subseteq \RR^d$ for some $d\in\NN$\footnote{There are cases where $\Theta$ is not a subset of $\RR^d$. For example, we could have $\Theta=\{f:\RR\rightarrow\RR_{\geq 0} : \int_{-\infty}^{+\infty}f(x)\dd{x}=1\}$.}.
  \end{definition}
  \subsubsection{Stadistics and estimators}
  \begin{definition}[Statistic]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define a \emph{statistic} as a Borel measurable function. That is, $\vf{T}$ can be written as $\vf{T}=\vf{h}(X_1,\ldots,X_n)$, where $\vf{h}:\mathcal{X}\rightarrow\RR^m$ is a Borel measurable function. Hence, $\vf{T}$ is a random vector. The value $m$ is the \emph{dimension} of the statistic.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define the \emph{sample mean} as the statistic: $$T(X_1,\ldots,X_n)=\frac{1}{n}\sum_{i=1}^nX_i=:\overline{X}_n$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define the \emph{sample variance} as the statistic: $$T(X_1,\ldots,X_n)=\frac{1}{n}\sum_{i=1}^n{(X_i-\overline{X}_n)}^2:={s_n}^2$$ We define the \emph{corrected sample variance} as the statistic:
    $$T(X_1,\ldots,X_n)=\frac{1}{n-1}\sum_{i=1}^n{(X_i-\overline{X}_n)}^2=:\tilde{s}_n{}^2$$
  \end{definition}
  \begin{proposition}
    Let $X_1,\ldots,X_n$ be random variables. Then: $${s_n}^2=\frac{1}{n}\sum_{i=1}^n{X_i}^2-{\overline{X}_n}^2$$
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model and $\vf{\theta} \in\Theta$. An \emph{estimator} of $\vf{\theta}$ is a statistic $\vf{T_\theta}$ whose outcomes are in $\Theta$ and does not depend on any unknown parameter. It is used to give an estimation of the (supposedly unknown) parameter $\vf{\theta}$.
  \end{definition}
  \subsubsection{Properties of estimators}
  \begin{definition}[Bias]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model and $\vf{T_\theta}$ be an integrable estimator of $\vf{\theta}\in\Theta$. We define the \emph{bias} of $\vf{T_\theta}$ with respect to $\vf{\theta}$ as: $$\bias(\vf{T_\theta}):=\Exp(\vf{T_\theta})-\vf{\theta}$$ We say that $\vf{T_\theta}$ is an \emph{unbiased estimator} of $\vf{\theta}$ if $\bias(\vf{T_\theta})=0$. Otherwise, we say that it is a \emph{biased estimator} of $\vf{\theta}$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are square-integrable\footnote{That is, with finite 2nd moments.} i.i.d. random variables with mean $\mu$ and variance $\sigma^2$. Then: $$\Exp(\overline{X}_n)=\mu\quad\text{and}\quad\Var{(\overline{X}_n)}=\frac{\sigma^2}{n}$$
    Hence, the estimator $\overline{X}_n$ of $\mu$ is unbiased.
  \end{proposition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are square-integrable i.i.d. random variables with mean $\mu$ and variance $\sigma^2$. Then: $$\Exp({s_n}^2)=\frac{n-1}{n}\sigma^2\quad\text{and}\quad\Exp(\tilde{s}_n{}^2)=\sigma^2$$
    Hence, the estimator $\tilde{s}_n{}^2$ of $\sigma^2$ is unbiased whereas the estimator ${s_n}^2$ of $\sigma^2$ is biased.
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model and $\vf{T_\theta}$ be an square integrable estimator of $\vf{\theta}\in\Theta$. The \emph{mean squared error} (\emph{m.s.e.}) of $\vf{T_\theta}$ is the function: $$\MSE(\vf{T_\theta}):=\Exp\left({(\vf{T_\theta}-\vf{\theta})}^2\right)$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model and $\vf{T_\theta}$ be an square integrable estimator of $\vf{\theta}\in\Theta$. Then: $$\MSE(\vf{T_\theta})=\Var(\vf{T_\theta})+{(\bias(\vf{T_\theta}))}^2$$
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model and $\vf{T_\theta}$ be an estimator of $\vf{\theta}\in\Theta$. We say that $\vf{T_\theta}$ is \emph{consistent} if $$\vf{T_\theta}\overset{\Prob}{\longrightarrow}\vf{\theta}$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model and $\vf{T_\theta}$, $\vf{U_\theta}$ be estimators of $\vf{\theta}\in\Theta$. We say that $\vf{T_\theta}$ is \emph{more efficient than} $\vf{U_\theta}$ if $$\Var{(\vf{T_\theta})}<\Var{(\vf{U_\theta})}\quad\forall\vf{\theta}\in\Theta$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model and $\vf{T_\theta}$ be an square integrable estimator of $\vf{\theta}\in\Theta$. We say that $\vf{T_\theta}$ is a \emph{minimum-variance unbiased estimator} (\emph{mvue}) if an unbiased estimator that has lower variance than any other unbiased estimator $\forall \vf{\theta}\in\Theta$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model. Then, the mvue is unique almost surely.
  \end{proposition}
  \subsubsection{Sufficient statistics}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $\vf{T}$ be an statistic. We say that $\vf{T}$ is \emph{sufficient} for $\theta\in\Theta$ if the joint conditional distribution of $(X_1,\ldots,X_n)$ given $\vf{T}(X_1,\ldots,X_n)=\vf{t}$ does not depend on $\theta$.
  \end{definition}
  \subsubsection{Methods of estimation}
  \begin{definition}[Method of moments]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are i.i.d. random variables, and $\mu_k$ be $k$-th moment of each of them. Suppose $\vf{\theta}=(\theta_1,\ldots,\theta_d)$. Then, given a realization $\vf{x}=(x_1,\ldots,x_n)\in\mathcal{X}$ of $\vf{X}_n$, a estimator $\tilde{\vf{\theta}}(\vf{x})=(\tilde{\theta}_1(\vf{x}),\ldots,\tilde{\theta}_d(\vf{x}))$ is given by the solution of the following system:
    $$
      \left\{
      \begin{aligned}
        \frac{1}{n}\sum_{i=1}^nx_i     & =\mu_1(\theta_1,\ldots,\theta_d) \\
        \frac{1}{n}\sum_{i=1}^n{x_i}^2 & =\mu_2(\theta_1,\ldots,\theta_d) \\
                                       & \;\;\vdots                       \\
        \frac{1}{n}\sum_{i=1}^n{x_i}^d & =\mu_d(\theta_1,\ldots,\theta_d) \\
      \end{aligned}
      \right.
    $$
  \end{definition}
  \begin{definition}[Likelihood]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model, $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$ and assume that $f_{\vf{X}_n,\vf{\theta}}$ is the pdf\footnote{We will do the study in the continuous case. For the discrete case, we shall replace the pdf of $\vf{X}_n$ by the pmf of $\vf{X}_n$.} of $\Prob_{\vf\theta}^{\vf{X}_n}$. We define the \emph{likelihood function} as the function:
    $$\function{L(\cdot;\vf{x})}{\Theta}{\RR}{\vf\theta}{f_{\vf{X}_n,\vf{\theta}}(\vf{x})}$$
  \end{definition}
  \begin{definition}[Maximum likelihood method]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model and $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$. A \emph{maximum likelihood estimator} (\emph{MLE}) of $\vf{\theta}\in\Theta$ is the estimator $\hat{\vf{\theta}}$ such that: $$L(\hat{\vf{\theta}}(\vf{x});\vf{x})=\sup\{L(\vf\theta;\vf{x}):\vf\theta\in\Theta\}$$
  \end{definition}
  \subsection{Distributions relating \texorpdfstring{$N(\mu,\sigma^2)$}{N(mu,sigma2)}}
  \subsubsection{Standard normal distribution}
  \begin{definition}
    We denote by $\Phi(t)$ the cdf of a standard normal distribution $N(0,1)$.
  \end{definition}
  \begin{definition}[Quantile]
    We define quantile function $Q(p)$ of a distribution as the inverse of the cmf. In particular, we denote the quantile of a standard normal distribution as $z_p:=Q(p)$.
  \end{definition}
  \subsubsection{\texorpdfstring{$\chi^2$}{chi2}-distribution}
  \begin{definition}
    Let $n\in\NN$ and $X_1,\ldots,X_n$ be independent random variables such that $X_i\sim \text{Gamma}(\alpha_i,\beta)$ for $i=1,\ldots,n$. Then: $$\sum_{i=1}^nX_i\sim\text{Gamma}\left(\sum_{i=1}^n\alpha_i,\beta\right)$$
  \end{definition}
  \begin{corollary}
    Let $n\in\NN$ and $Z_1,\ldots,Z_n$ be i.i.d. random variable with standard normal distribution. Then $${Z_1}^2+\cdots+{Z_n}^2\sim\text{Gamma}\left(\frac{n}{2},\frac{1}{2}\right)$$
  \end{corollary}
  \begin{definition}
    We define the \emph{chi-squared distribution with $n$ degrees of freedom}, denoted as ${\chi_n}^2$, as the distribution $${\chi_n}^2:=\text{Gamma}\left(\frac{n}{2},\frac{1}{2}\right)$$ which is the distribution of ${Z_1}^2+\cdots+{Z_n}^2$, where $Z_1,\ldots,Z_n\sim N(0,1)$ are i.i.d. random variables. Its pdf is:
    $$f_{{\chi_n}^2}(x)=\frac{1}{2^\frac{n}{2}\Gamma\left(\frac{n}{2}\right)}x^{\frac{n}{2}-1}\exp{-\frac{x}{2}}\vf{1}_{(0,\infty)}(x)$$
  \end{definition}
  \subsubsection{Student's \texorpdfstring{$t$}{t}-distribution}
  \begin{definition}
    Let $n\in\NN$ and $Z\sim N(0,1)$ and $Y\sim{\chi_n}^2$ be independent random variables. We define the \emph{Student's $t$-distribution with $n$ degrees of freedom} as the distribution of: $$\frac{Z}{\sqrt{Y/n}}$$
  \end{definition}
  \begin{proposition}
    Let $n\in\NN$. Then, the pdf of $t_n$ is: $$f_{t_n}(x)=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{\pi n}\Gamma\left(\frac{n}{2}\right)}{\left(1+\frac{x^2}{n}\right)}^{-\frac{n+1}{2}}$$
  \end{proposition}
  \subsubsection{Fisher's theorem}
  \begin{theorem}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and suppose $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ are i.i.d. random variables. Then:
    \begin{enumerate}
      \item $\overline{X}_n\sim N\left(\mu,\frac{\sigma^2}{n}\right)$
      \item $\tilde{s}_n{}^2\sim\frac{\sigma^2}{n-1}{\chi_{n-1}}^2$
      \item $\overline{X}_n$ and $\tilde{s}_n{}^2$ are independent.
    \end{enumerate}
  \end{theorem}
  \begin{corollary}
    Let $n\in\NN$ and $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ be i.i.d. random variables. Then: $$\frac{\overline{X}_n-\mu}{\frac{\tilde{s}_n}{\sqrt{n}}}\sim t_{n-1}$$
  \end{corollary}
  \begin{corollary}
    Let $n\in\NN$ and $X\sim t_n$ be a random variable. Then: $$X\overset{\text{d}}{\longrightarrow }N\left(0,1\right)$$
    Hence, $N(0,1)=t_\infty$.
  \end{corollary}
  \begin{corollary}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and suppose $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ are i.i.d. random variables. Then, the estimators $\overline{X}_n$ of $\mu$ and $\tilde{s}_n{}^2$ of $\sigma^2$ are unbiased and consistent.
  \end{corollary}
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \includestandalone[mode=image|tex,width=0.95\linewidth]{Images/student-normal}
      \captionof{figure}{Probability density function of 4 Student's $t$-distribution together with a standard normal $N(0,1)=t_{\infty}$.}
    \end{minipage}
  \end{center}
\end{multicols}
\end{document}