\documentclass[../../../main.tex]{subfiles}
% break in parametric statistical model

\begin{document}
\begin{multicols}{2}[\section{Statistics}]
  \subsection{Point estimation}
  \subsubsection{Stadistical models}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space\footnote{From now on we will assume that the random variables are defined always in the same probability space $(\Omega,\mathcal{A},\Prob)$, so we will omit to say that.}, $\Theta$ be a set, $n\in\NN$ and $x_1,\ldots,x_n$ be a collection of data that we may assume that they are the outcomes of a random vector $\vf{X}_n=(X_1,\ldots,X_n)$ defined on $(\Omega,\mathcal{A},\Prob)$. Suppose, moreover, that the outcomes of $\vf{X}_n$ are in a set $\mathcal{X}\subseteq\RR^n$, the law $\vf{X}_n$ is one in the set $\mathcal{P}=\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\}$ and $\mathcal{F}$ is a $\sigma$-algebra over $\mathcal{X}$\footnote{That is, $\mathcal{P}$ denotes a family of probability distributions of $\vf{X}_n$ in $(\mathcal{X},\mathcal{F})$, indeXed by $\theta\in\Theta$. Note that we denote that distribution of $\vf{X}_n$ by $\Prob^{\vf{X}_n}$ to distinguish it from the probability distribution $\Prob_{\vf{X}_n}$ in $(\Omega,\mathcal{A},\Prob)$.}. We define a \emph{statistical model} as the triplet $(\mathcal{X},\mathcal{F},\mathcal{P})$\footnote{Often we will take $\mathcal{F}=\mathcal{B}(\mathcal{X})$.}. The set $\mathcal{X}$ is called \emph{sample space}, and the set $\Theta$, \emph{parameter space}. The outcomes of $\vf{X}_n$ are called \emph{random samples} and they are usually denoted as $x_1,\ldots,x_n$. If, moreover, $X_1,\ldots,X_n$ are i.i.d. random variables, $x_1,\ldots,x_n$ is called a \emph{simple random sample}.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We say $\mathcal{P}=\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ is \emph{identificable} if the function $$\function{}{\Theta}{\mathcal{P}}{\theta}{\Prob_\theta^{\vf{X}_n}}$$ is injective\footnote{From now on, we will suppose that all the sets $\mathcal{P}$ are identificable.}.
  \end{definition}
  \begin{definition}
    A statistical model $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ is said to be \emph{parametric} if $\Theta\subseteq \RR^k$ for some $k\in\NN$\footnote{There are cases where $\Theta$ is not a subset of $\RR^k$. For example, we could have $\Theta=\{f:\RR\rightarrow\RR_{\geq 0} : \int_{-\infty}^{+\infty}f(x)\dd{x}=1\}$.}.
  \end{definition}
  \subsubsection{Stadistics and estimators}
  \begin{definition}[Statistic]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define a \emph{statistic} as a Borel measurable function. That is, $\vf{T}$ can be written as $\vf{T}=\vf{h}(X_1,\ldots,X_n)$, where $\vf{h}:\mathcal{X}\rightarrow\RR^m$ is a Borel measurable function. Hence, $\vf{T}$ is a random vector. The value $m$ is the \emph{dimension} of the statistic.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define the \emph{sample mean} as the statistic: $$T(X_1,\ldots,X_n)=\frac{1}{n}\sum_{i=1}^nX_i=:\overline{X}_n$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define the \emph{sample variance} as the statistic: $$T(X_1,\ldots,X_n)=\frac{1}{n}\sum_{i=1}^n{(X_i-\overline{X}_n)}^2:={s_n}^2$$ We define the \emph{corrected sample variance} as the statistic:
    $$T(X_1,\ldots,X_n)=\frac{1}{n-1}\sum_{i=1}^n{(X_i-\overline{X}_n)}^2=:\tilde{s}_n{}^2$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and $\theta \in\Theta$. An \emph{estimator} of $\theta$ is a statistic $\vf{T}_\theta$ whose outcomes are in $\Theta$ and does not depend on any unknown parameter. It is used to give an estimation of the unknown parameter $\theta$.
  \end{definition}
  \subsubsection{Properties of estimators}
  \begin{definition}[Bias]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and $\vf{T}_\theta$ be an integrable estimator of $\theta\in\Theta$. We define the \emph{bias} of $\vf{T}_\theta$ with respect to $\theta$ as: $$\bias(\vf{T}_\theta):=\Exp(\vf{T}_\theta)-\theta$$ We say that $\vf{T}_\theta$ is an \emph{unbiased estimator} of $\theta$ if $\bias{(\vf{T}_\theta)}=0$ $\theta\in\Theta$. Otherwise, we say that it is a \emph{biased estimator} of $\theta$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are square-integrable\footnote{That is, with finite 2nd moments.} i.i.d. random variables with mean $\mu$ and variance $\sigma^2$. Then: $$\Exp(\overline{X}_n)=\mu\quad\text{and}\quad\Var{(\overline{X}_n)}=\frac{\sigma^2}{n}$$
    Hence, the estimator $\overline{X}_n$ of $\mu$ is unbiased.
  \end{proposition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are square-integrable i.i.d. random variables with mean $\mu$ and variance $\sigma^2$. Then: $$\Exp({s_n}^2)=\frac{n-1}{n}\sigma^2\quad\text{and}\quad\Exp(\tilde{s}_n{}^2)=\sigma^2$$
    Hence, the estimator $\tilde{s}_n{}^2$ of $\sigma^2$ is unbiased whereas the estimator ${s_n}^2$ of $\sigma^2$ is biased.
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and $\vf{T}_\theta$ be an square integrable estimator of $\theta\in\Theta$. The \emph{mean squared error} (\emph{m.s.e.}) of $\vf{T}_\theta$ is the function: $$\MSE(\vf{T}_\theta):=\Exp\left({(\vf{T}_\theta-\theta)}^2\right)$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and $\vf{T}_\theta$ be an square integrable estimator of $\theta\in\Theta$. Then: $$\MSE(\vf{T}_\theta)=\Var(\vf{T}_\theta)+{(\bias(\vf{T}_\theta))}^2$$
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and $\vf{T}_\theta$ be an estimator of $\theta\in\Theta$. We say that $\vf{T}_\theta$ is \emph{consistent} if $$\vf{T}_\theta\overset{\Prob}{\longrightarrow}\theta$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and $\vf{T}_\theta$, $\vf{U}_\theta$ be estimators of $\theta\in\Theta$. We say that $\vf{T}_\theta$ is \emph{more efficient than} $\vf{U}_\theta$ if $$\Var{(\vf{T}_\theta)}<\Var{(\vf{U}_\theta)}\quad\forall\theta\in\Theta$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and $\vf{T}_\theta$ be an square integrable estimator of $\theta\in\Theta$. We say that $\vf{T}_\theta$ is a \emph{minimum-variance unbiased estimator} (\emph{mvue}) if an unbiased estimator that has lower variance than any other unbiased estimator $\forall \theta\in\Theta$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model. Then, the mvue is unique almost surely.
  \end{proposition}
  \subsubsection{Sufficient statistics}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and $\vf{T}$ be an statistic. We say that $\vf{T}$ is \emph{sufficient} for $\theta\in\Theta$ if the joint conditional distribution of $(X_1,\ldots,X_n)$ given $\vf{T}=t$ does not depend on $\theta$.
  \end{definition}
  \subsection{Distributions relating \texorpdfstring{$N(\mu,\sigma^2)$}{N(mu,sigma2)}}
  \subsubsection{\texorpdfstring{$\chi^2$}{chi2}-distribution}
  \begin{definition}
    Let $n\in\NN$ and $X_1,\ldots,X_n$ be independent random variables such that $X_i\sim \text{Gamma}(\alpha_i,\beta)$ for $i=1,\ldots,n$. Then: $$\sum_{i=1}^nX_i\sim\text{Gamma}\left(\sum_{i=1}^n\alpha_i,\beta\right)$$
  \end{definition}
  \begin{corollary}
    Let $n\in\NN$ and $Z_1,\ldots,Z_n$ be i.i.d. random variable with standard normal distribution. Then $${Z_1}^2+\cdots+{Z_n}^2\sim\text{Gamma}\left(\frac{n}{2},\frac{1}{2}\right)$$
  \end{corollary}
  \begin{definition}
    We define the \emph{chi-squared distribution with $n$ degrees of freedom}, denoted as ${\chi_n}^2$, as the distribution $${\chi_n}^2:=\text{Gamma}\left(\frac{n}{2},\frac{1}{2}\right)$$ which is the distribution of ${Z_1}^2+\cdots+{Z_n}^2$, where $Z_1,\ldots,Z_n\sim N(0,1)$ are i.i.d. random variables. Its pdf is:
    $$f_{{\chi_n}^2}(x)=\frac{1}{2^\frac{n}{2}\Gamma\left(\frac{n}{2}\right)}x^{\frac{n}{2}-1}\exp{-\frac{x}{2}}\vf{1}_{(0,\infty)}(x)$$
  \end{definition}
  \subsubsection{Student's \texorpdfstring{$t$}{t}-distribution}
  \begin{definition}
    Let $n\in\NN$ and $Z\sim N(0,1)$ and $Y\sim{\chi_n}^2$ be independent random variables. We define the \emph{Student's $t$-distribution with $n$ degrees of freedom} as the distribution of: $$\frac{Z}{\sqrt{Y/n}}$$
  \end{definition}
  \begin{proposition}
    Let $n\in\NN$. Then, the pdf of $t_n$ is: $$f_{t_n}(x)=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{\pi n}\Gamma\left(\frac{n}{2}\right)}{\left(1+\frac{x^2}{n}\right)}^{-\frac{n+1}{2}}$$
  \end{proposition}
  \subsubsection{Fisher's theorem}
  \begin{theorem}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and suppose $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ are i.i.d. random variables. Then:
    \begin{enumerate}
      \item $\overline{X}_n\sim N\left(\mu,\frac{\sigma^2}{n}\right)$
      \item $\tilde{s}_n{}^2\sim\frac{\sigma^2}{n-1}{\chi_{n-1}}^2$
      \item $\overline{X}_n$ and $\tilde{s}_n{}^2$ are independent.
    \end{enumerate}
  \end{theorem}
  \begin{corollary}
    Let $n\in\NN$ and $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ be i.i.d. random variables. Then: $$\frac{\overline{X}_n-\mu}{\frac{\tilde{s}_n}{\sqrt{n}}}\sim t_{n-1}$$
  \end{corollary}
  \begin{corollary}
    Let $n\in\NN$ and $X\sim t_n$ be a random variable. Then: $$X\overset{\text{d}}{\longrightarrow }N\left(0,1\right)$$
    Hence, $N(0,1)=t_\infty$.
  \end{corollary}
  \begin{corollary}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and suppose $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ are i.i.d. random variables. Then, the estimators $\overline{X}_n$ of $\mu$ and $\tilde{s}_n{}^2$ of $\sigma^2$ are unbiased and consistent.
  \end{corollary}
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \includestandalone[mode=image|tex,width=0.95\linewidth]{Images/student-normal}
      \captionof{figure}{Probability density function of 4 Student's $t$-distribution together with a standard normal $N(0,1)=t_{\infty}$.}
    \end{minipage}
  \end{center}
\end{multicols}
\end{document}