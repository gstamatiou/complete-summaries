\documentclass[../../../main.tex]{subfiles}

\begin{document}
\begin{multicols}{2}[\section{Statistics}]
  \subsection{Estimation statistics}
  \subsubsection{Stadistics and point estimation}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space\footnote{From now on we will assume that the random variables are defined in the same probability space $(\Omega,\mathcal{A},\Prob)$, so we will omit to say that.}, $n\in\NN$ and $X_1,\ldots,X_n$ be $n$ random variables. We denominate such set of random variables as a \emph{random sample} (or simply \emph{sample}) of size $n$. If, moreover, the random variables are i.i.d., we say they are a \emph{simple random sample} of size $n$.
  \end{definition}
  \begin{definition}
    Let $n\in\NN$, $X_1,\ldots,X_n$ be a simple random sample. We define a \emph{statistic} $T_n$ of $X_1,\ldots,X_n$ as a Borel measurable function $T_n:\RR^n\rightarrow\RR$. Hence, $T_n(X_1,\ldots,X_n)$ is a random variable.
  \end{definition}
  \begin{definition}
    Let $n\in\NN$ and $X_1,\ldots,X_n$ be a simple random sample. We define the statistic \emph{sample mean} as: $$\overline{X}_n:=\frac{1}{n}S_n=\frac{1}{n}\sum_{i=1}^nX_i$$
  \end{definition}
  \begin{definition}
    Let $n\in\NN$ and $X_1,\ldots,X_n$ be a simple random sample. We define the statistic \emph{sample variance} as: $${{\hat{\sigma}}_n}^2:=\frac{1}{n}\sum_{i=1}^n{(X_i-\overline{X}_n)}^2$$ We define the statistic \emph{corrected sample variance} as: $${S_n}^2:=\frac{1}{n-1}\sum_{i=1}^n{(X_i-\overline{X}_n)}^2$$
  \end{definition}
  \begin{definition}
    Let $\Theta\subseteq\CC^p$ be a set and $F:\RR\times\Theta\rightarrow[0,\infty)$ be a function a such that $\forall \theta\in\Theta$, $F(\cdot,\theta)$ is a cdf. In these conditions, we say that $(F(\cdot,\theta))_{\theta\in\Theta}$ is a \emph{family of cdfs} and $\Theta$ is the \emph{parameter space}.
  \end{definition}
  \begin{definition}
    Let $\Theta$ be a parameter space and $\theta\in\Theta$. An \emph{estimator} of $\theta$ is a statistic $T$ which does not depend on any unknown parameter.
  \end{definition}
  \begin{prop}
    Let $n\in\NN$ and $X_1,\ldots,X_n$ be a simple random sample with mean $\mu$ and variance $\sigma^2$. Then: $$\Exp(\overline{X}_n)=\mu\quad\text{and}\quad\Var(\overline{X}_n)=\sigma^2$$
  \end{prop}
  \begin{prop}
    Let $n\in\NN$ and $X_1,\ldots,X_n$ be a simple random sample with variance $\sigma^2$. Then: $$\Exp({S_n}^2)=\sigma^2$$
  \end{prop}
  \begin{definition}
    Let $\Theta$ be a parameter space and $T$ be an estimator of $\theta\in\Theta$. We define the \emph{bias} of $T$ with respect to $\theta$ as: $$\bias_\theta(T):=\Exp_\theta(T)-\theta$$ We say that $T$ is an \emph{unbiased estimator} of $\theta$ if $\bias_\theta(T)=0$. Otherwise, we say that it is a \emph{biased estimator} of $\theta$.
  \end{definition}
  \begin{definition}
    Let $\Theta$ be a parameter space and $T$ be an estimator of $\theta\in\Theta$. The \emph{mean square error} (\emph{m.s.e.}) of $T$ is the function: $$\text{MSE}_\theta(T):=\Exp_\theta((T-\theta)^2)$$
  \end{definition}
  % \begin{definition}
  %   Let $n\in\NN$, $X_1,\ldots,X_n$ be a simple random sample and $\vf{g}:\RR^n\rightarrow\RR^m$ be a Borel measurable function. Then, we say that $\vf{T}_n=\vf{g}(X_1,\ldots,X_n)$ is a \emph{vectorial statistic}.
  % \end{definition}
  % \begin{definition}
  %   Let $n\in\NN$, $X_1,\ldots,X_n$ be a simple random sample and $\vf{T}$ be a vectorial statistic. We say that $\vf{T}$ is \emph{sufficient} for $\theta\in\Theta$ if the conditional cdf of $(X_1,\ldots,X_n)$ given $\vf{T}=\vf{t}$, for some $\vf{t}\in \im \vf{T}$, does not depend on $\theta$.
  % \end{definition}
\end{multicols}
\end{document}