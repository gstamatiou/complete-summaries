\documentclass[../../../main_math.tex]{subfiles}

\begin{document}
\changecolor{AP}
\begin{multicols}{2}[\section{Advanced probability}]
  These summaries aims to review the basic notions of probability theory in a more abstract setting. We will not prove any result here as most of them are from previous courses. Furthermore, we will skip some elementary definitions already defined in other summaries.
  \subsection{Basics of measure theory and integration}
  \begin{definition}[$\sigma$-algebra]
    Let $E$ be a set. A \emph{$\sigma$-algebra} $\mathcal{E}$ on $E$ is a collection of subsets of $E$ such that:
    \begin{enumerate}
      \item $\varnothing\in\mathcal{E}$.
      \item $\forall A\in\mathcal{E}$, $A^c\in\mathcal{E}$.
      \item $\forall (A_n)_{n\in\NN}\subseteq \mathcal{E}$, $\bigcup_{n\in\NN}{A_n}\in\mathcal{E}$.
    \end{enumerate}
    The pair $(E,\mathcal{E})$ is called a \emph{measurable space}.
  \end{definition}
  \begin{definition}
    Let $E$ be a set and $\mathcal{F}$ be a collection of subsets of $E$. The \emph{$\sigma$-algebra generated by $\mathcal{F}$} is the smallest $\sigma$-algebra containing $\mathcal{F}$, i.e.:
    $$
      \sigma(\mathcal{F}):=\bigcap_{\substack{\mathcal{E}\text{ is a }\sigma\text{-algebra}\\\mathcal{F}\subseteq \mathcal{E}}}{\mathcal{E}}
    $$
  \end{definition}
  \begin{definition}
    Let $(E,\mathcal{E})$, $(F,\mathcal{F})$ be measurable spaces. A function $f:E\to F$ is said to be \emph{measurable} if $\forall A\in\mathcal{F}$, $f^{-1}(A)\in\mathcal{E}$.
  \end{definition}
  \begin{definition}[Measure]
    Let $(E,\mathcal{E})$ be a measurable space. A function $\mu:\mathcal{E}\to [0,\infty]$ is said to be a \emph{measure} if:
    \begin{enumerate}
      \item $\mu(\varnothing)=0$.
      \item $\mu$ is $\sigma$-additive, i.e. $\forall {(A_n)}_{n\in\NN}\subseteq \mathcal{E}$ pairwise disjoint, we have:
            $$
              \mu\left(\bigcup_{n\in\NN}{A_n}\right)=\sum_{n\in\NN}{\mu(A_n)}
            $$
    \end{enumerate}
    The triple $(E,\mathcal{E},\mu)$ is called a \emph{measure space}.
  \end{definition}
  \begin{definition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space and $f:E\to [0,\infty]$ be a measurable function. We define the \emph{integral of $f$ with respect to $\mu$} as:
    $$
      \int_E{f\dd{\mu}}:=\sup\left\{\int_E{g\dd{\mu}}:g\leq f, g\text{ simple}\right\}
    $$
  \end{definition}
  \begin{definition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space and $f:E\to \RR$ be a measurable function. Suppose that $\int_E{\abs{f}\dd{\mu}}<\infty$. Then, we define the \emph{integral of $f$ with respect to $\mu$} as:
    $$
      \int_E{f\dd{\mu}}:=\int_E{f^+\dd{\mu}}-\int_E{f^-\dd{\mu}}
    $$
  \end{definition}
  \begin{theorem}[Monotone convergence theorem]
    Let $(E,\mathcal{E},\mu)$ be a measurable space and ${(f_n)}_{n\in\NN}$ be a sequence of measurable functions $f_n:E\to [0,\infty]$ such that $\forall n\in\NN$, $f_n\leq f_{n+1}$. Then:
    $$
      \int_E{\lim_{n\to\infty}{f_n}\dd{\mu}}=\lim_{n\to\infty}{\int_E{f_n\dd{\mu}}}
    $$
  \end{theorem}
  \begin{theorem}[Fatou's lemma]
    Let $(E,\mathcal{E},\mu)$ be a measurable space and ${(f_n)}_{n\in\NN}$ be a sequence of measurable functions $f_n:E\to [0,\infty]$. Then:
    $$
      \int_E{\liminf_{n\to\infty}{f_n}\dd{\mu}}\leq \liminf_{n\to\infty}{\int_E{f_n\dd{\mu}}}
    $$
  \end{theorem}
  \begin{theorem}[Dominated convergence theorem]
    Let $(E,\mathcal{E},\mu)$ be a measurable space and ${(f_n)}_{n\in\NN}$ be a sequence of measurable functions $f_n:E\to \RR$ such that $\forall n\in\NN$, $\abs{f_n}\leq g$ for some $g:E\to [0,\infty]$ integrable. Then:
    $$
      \int_E{\lim_{n\to\infty}{f_n}\dd{\mu}}=\lim_{n\to\infty}{\int_E{f_n\dd{\mu}}}
    $$
  \end{theorem}
  \begin{proposition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space and $f:E\times I\rightarrow \RR$ be a measurable function, where $I\subseteq \RR$ is an interval. Assume that $\forall \lambda\in I$, $f(\cdot,\lambda)$ is integrable and that for some $k\in \NN\cup\{0\}$ and $\forall x\in E$ we have $f(x,\cdot)\in \mathcal{C}^k(I)$ and $\abs{\partial_\lambda^k f(x,\lambda)}\leq g(x)$ for some $g:E\to [0,\infty]$ integrable. Then, the function $F:I\to \RR$ defined by:
    $$
      F(\lambda):=\int_E{f(x,\lambda)\dd{\mu(x)}}
    $$
    is in $\mathcal{C}^k(I)$ and $\forall j\in\{0,\ldots,k\}$ we have:
    $$
      \partial_\lambda^j F(\lambda)=\int_E{\partial_\lambda^j f(x,\lambda)\dd{\mu(x)}}
    $$
  \end{proposition}
  \begin{definition}[Product measure]
    Let $(E,\mathcal{E},\mu)$ and $(F,\mathcal{F},\nu)$ be two measurable spaces. We define the \emph{product measure} $\mu\otimes\nu$ on $(E\times F,\mathcal{E}\otimes\mathcal{F})$ as:
    $$
      \forall A\in\mathcal{E}, B\in\mathcal{F}, \quad \mu\otimes\nu(A\times B):=\mu(A)\nu(B)
    $$
  \end{definition}
  \begin{definition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space. We say that $\mu$ is \emph{$\sigma$-finite} if there exists a sequence ${(E_n)}_{n\in\NN}\subseteq \mathcal{E}$ such that $\forall n\in\NN$, $\mu(E_n)<\infty$ and $\bigcup_{n\in\NN}{E_n}=E$.
  \end{definition}
  \begin{theorem}[Fubini]
    Let $(E,\mathcal{E},\mu)$ and $(F,\mathcal{F},\nu)$ be two $\sigma$-finite measurable spaces and $f:E\times F\to \RR$ be a measurable function. Then, the following are equivalent:
    \begin{enumerate}
      \item $f$ is integrable with respect to $\mu\otimes\nu$.
      \item $\displaystyle\int_E{\left(\int_F{\abs{f(x,y)}\dd{\nu(y)}}\right)\dd{\mu(x)}}<\infty$.
      \item $\displaystyle\int_F{\left(\int_E{\abs{f(x,y)}\dd{\mu(x)}}\right)\dd{\nu(y)}}<\infty$.
    \end{enumerate}
    And if any of the above holds, then:
    \begin{align*}
      \int_{E\times F}{f\dd{(\mu\otimes\nu)}} & =\int_E{\left(\int_F{f(x,y)\dd{\nu(y)}}\right)\dd{\mu(x)}} \\
                                              & =\int_F{\left(\int_E{f(x,y)\dd{\mu(x)}}\right)\dd{\nu(y)}}
    \end{align*}
  \end{theorem}
  \begin{definition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space and $f:E\to [0,\infty]$ be a measurable function. We define the measure $\nu$ on $(E,\mathcal{E})$ as $\forall A\in\mathcal{E}$:
    $$
      \nu(A):=\int_A{f\dd{\mu}}
    $$
    In that case, we say that $f$ is the \emph{density} of $\nu$ with respect to $\mu$, also denoted by $\dv{\nu}{\mu}=f$.
  \end{definition}
  \begin{definition}
    Let $(E,\mathcal{E},\mu)$ be a measurable space. A measure $\nu$ on $(E,\mathcal{E})$ is said to be \emph{absolutely continuous} with respect to $\mu$ if $\forall A\in\mathcal{E}$ such that $\mu(A)=0$, we have $\nu(A)=0$.
  \end{definition}
  \begin{theorem}[Radon-Nikodym]
    Let $\mu$, $\nu$ be two $\sigma$-finite on  a measurable space $(E,\mathcal{E})$ such that $\nu$ is absolutely continuous with respect to $\mu$. Then, $\nu$ admits a density $f$ with respect to $\mu$.
  \end{theorem}
  \subsection{Probability spaces and random variables}
  \begin{definition}
    A \emph{probability space} is a triple $(\Omega,\mathcal{F},\Prob)$ where $\Omega$ is a set, $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$ and $\Prob$ is a measure on $(\Omega,\mathcal{F})$ such that $\Prob(\Omega)=1$. In this context, the elements if $\mathcal{F}$ are called \emph{events}.
  \end{definition}
  \begin{definition}[Random variable]
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space and $(E,\mathcal{E})$ be a measurable space. An \emph{$E$-valued random variable} is a measurable function from $(\Omega,\mathcal{F})$ to $(E,\mathcal{E})$\footnote{When $E$ is not specified, we will assume that $E=\RR$.}.
  \end{definition}
  \begin{definition}[Expectation]
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space and $X$ be a random variable. We define the \emph{expectation of $X$} as:
    $$
      \Exp(X):=\int_\Omega{X\dd{\Prob}}
    $$
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space, $(E,\mathcal{E})$ be a measurable space and $X$ be a $E$-valued random variable. We define the \emph{law of $X$} as the measure image on $E$, defined for all $A\in\mathcal{E}$ as:
    $$
      \mathcal{L}^X(A):=\Prob\circ X^{-1}(A)=\Prob(X\in A)
    $$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space, $X$ be a random variable and $h:\RR\to \RR$ be a measurable function such that $h(X)$ is integrable. Then:
    $$
      \Exp(h(X))=\int_\RR{h(x)\dd{\mathcal{L}^X(x)}}
    $$
    In particular, if the law of $X$ admits a density $f$ with respect to the Lebesgue measure, then:
    $$
      \Exp(h(X))=\int_\RR{h(x)f(x)\dd{x}}
    $$
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space and $X$ be a random variable. We define the \emph{$\sigma$-algebra generated by $X$} as the smallest $\sigma$-algebra containing $X$, i.e.:
    $$
      \sigma(X):=\sigma(X^{-1}(A):A\in\mathcal{E})
    $$
  \end{definition}
  \begin{proposition}
    Let $X$ be a $(E,\mathcal{E})$-valued random variable and $Y$ be a $\sigma(X)$-measurable random variable. Then, there exists a measurable function $f:E\to \RR$ such that $Y=f(X)$.
  \end{proposition}
  \begin{proposition}[Jensen's inequality]
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space, $X$ be a random variable and $h:\RR\to \RR$ be a convex function. Then:
    $$
      h(\Exp(X))\leq \Exp(h(X))
    $$
    as long as the expectations are well-defined.
  \end{proposition}
  \begin{proposition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space, $X$ be a random variable and $h:\RR\to \RR$ be a non-decreasing positive function. Then:
    $$
      \Prob(X\geq t)\leq \frac{\Exp(h(X))}{h(t)}
    $$
  \end{proposition}
  \subsection{Conditional expectation}
  \begin{proposition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space and $\mathcal{G}\subseteq \mathcal{F}$ be a $\sigma$-algebra. Then, for any integrable random variable $X$, there exists a unique (up to a.s.) random variable $Y$ such that:
    \begin{enumerate}
      \item $Y$ is $\mathcal{G}$-measurable.
      \item For any $Z$ $\mathcal{G}$-measurable such that $XZ$ is integrable, we have that $\Exp(XZ)=\Exp(YZ)$.
    \end{enumerate}
    We denote $Y=\Exp(X\mid \mathcal{G})$ and call it the \emph{conditional expectation of $X$ given $\mathcal{G}$}.
  \end{proposition}
  \begin{remark}
    If the variable $X$ is not integrable but it is non-negative, then the above holds for any $Z$ non-negative as well.
  \end{remark}
  \begin{remark}
    The conditional expectation, when restricted to $X\in L^2(\Omega,\mathcal{F},\Prob)$, is the orthogonal projection of $X$ onto $L^2(\Omega,\mathcal{G},\Prob)$.
  \end{remark}
  \begin{proposition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space, $\mathcal{G}\subseteq \mathcal{F}$ be a $\sigma$-algebra and $X$, $Y$ be random variables. Then, assuming that all the expectations below are well-defined, we have:
    \begin{enumerate}
      \item If $Y$, $Z$ are $\mathcal{G}$-measurable random variables, then $\Exp(XY+Z\mid \mathcal{G})=Y\Exp(X\mid \mathcal{G})+Z$.
      \item If $X\overset{\text{a.s.}}{\leq} Y$, then $\Exp(X\mid \mathcal{G})\overset{\text{a.s.}}{\leq} \Exp(Y\mid \mathcal{G})$.
      \item $\Exp(\Exp(X\mid \mathcal{G}))=\Exp(X)$.
      \item $\Exp(\abs{\Exp(X\mid \mathcal{G})})\leq \Exp(\abs{X})$.
      \item \emph{Tower property}: if $\mathcal{H}\subseteq \mathcal{G}\subseteq \mathcal{F}$ are $\sigma$-algebras, then $\Exp(\Exp(X\mid \mathcal{G})\mid \mathcal{H})=\Exp(X\mid \mathcal{H})$.
      \item If $X$ is independent of $\mathcal{G}$, then $\Exp(X\mid \mathcal{G})=\Exp(X)$.
      \item If $X$ is independent of $\mathcal{G}$ and $Y$ is $\mathcal{G}$-measurable, then for any measurable function $f$, we have that $\Exp(f(X,Y)\mid \mathcal{G})=g(Y)$, where $g(y)=\Exp(f(X,y))$. This is often written as:
            $$
              \Exp(f(X,Y)\mid \mathcal{G})=\Exp(f(X,y))|_{y=Y}
            $$
    \end{enumerate}
  \end{proposition}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space and $X$, $Y$ be random variables. We define the \emph{conditional expectation of $X$ given $Y$} as:
    $$
      \Exp(X\mid Y):=\Exp(X\mid \sigma(Y))
    $$
  \end{definition}
  \begin{remark}
    It can be seen that this definition coincides with the one given by:
    $$
      \Exp(X\mid Y)=\sum_{y\in \supp(Y)} \Exp(X\mid Y=y)\indi{Y=y}
    $$
  \end{remark}
  \begin{proposition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space and $X$, $Y$ be random variables. Assume that $(X,Y)$ has a law which admits a density $f=f(x,y)$ (which for simplicity we may think it with respect to $\dd{x}\dd{y}$). Then, for any function $h$ such that $\Exp(h(X))$ makes sense:
    $$
      \Exp(h(X)\mid Y)\overset{\text{a.s}}{=}\frac{\int_\RR{h(x)f(x,Y)\dd{x}}}{\int_\RR{f(x,Y)\dd{x}}}
    $$
  \end{proposition}
  \begin{definition}
    A \emph{probability kernel} on $(\RR, \mathcal{B}(\RR))$ is a function $K:\RR\times \mathcal{B}(\RR)\to [0,1]$ such that:
    \begin{enumerate}
      \item $\forall y\in \RR$, $K(y,\cdot)$ is a probability measure on $(\RR,\mathcal{B}(\RR))$.
      \item $\forall A\in \mathcal{B}(\RR)$, $K(\cdot,A)$ is measurable.
    \end{enumerate}
  \end{definition}
  \begin{theorem}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space and $X$, $Y$ be random variables. Then, there exists a probability kernel $\mathcal{L}^{X\mid Y}$, called the \emph{conditional law of $X$ given $Y$}, such that for any bounded measurable function $f$ we have:
    $$
      \Exp(f(X)\mid Y)=\int_\RR{f(x)\dd{\mathcal{L}^{X\mid Y}(Y,x)}}
    $$
  \end{theorem}
  \subsection{Martingales}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space. A \emph{filtration} is a sequence of sub-$\sigma$-algebras ${(F_n)}_{n\in\NN}$ such that $\forall n\in\NN$, $F_n\subseteq F_{n+1}$. The tuple $(\Omega,\mathcal{F},\Prob,{(F_n)}_{n\in\NN})$ is called a \emph{filtered probability space}.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob,{(F_n)}_{n\in\NN})$ be a filtered probability space. A stochastic process ${(X_n)}_{n\in\NN}$ is \emph{adapted} to ${(F_n)}_{n\in\NN}$ if $\forall n\in\NN$, $X_n$ is $F_n$-measurable.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob,{(F_n)}_{n\in\NN})$ be a filtered probability space and ${(M_n)}_{n\in\NN}$ be an adapted stochastic process. We say that ${(M_n)}_{n\in\NN}$ is a \emph{martingale} if $\forall n\in\NN$, $\Exp(\abs{M_n})<\infty$ and $\Exp(M_{n+1}\mid F_n)=M_n$.
  \end{definition}
  \begin{remark}
    A \emph{submartingale} and \emph{supermartingale} are defined similarly, but with $\Exp(M_{n+1}\mid F_n)\geq M_n$ and $\Exp(M_{n+1}\mid F_n)\leq M_n$ respectively.
  \end{remark}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob,{(F_n)}_{n\in\NN})$ be a filtered probability space. A \emph{stopping time} is a random variable $\tau:\Omega\to \NN\cup\{\infty\}$ such that $\forall n\in\NN$, $\{\tau\leq n\}\in F_n$.
  \end{definition}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob,{(F_n)}_{n\in\NN})$ be a filtered probability space, $\tau$ be a stopping time and $M:={(M_n)}_{n\in\NN}$ be a process. We define the \emph{stopped process} $M^\tau:={(M_{\tau\wedge n})}_{n\in\NN}$.
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{F},\Prob,{(F_n)}_{n\in\NN})$ be a filtered probability space, $\tau$ be a stopping time and $M:={(M_n)}_{n\in\NN}$ be a martingale. Then, $M^\tau:={(M_{\tau\wedge n})}_{n\in\NN}$ is a martingale.
  \end{proposition}
  \begin{corollary}
    Let $(\Omega,\mathcal{F},\Prob,{(F_n)}_{n\in\NN})$ be a filtered probability space, $\tau$ be a bounded stopping time and $M:={(M_n)}_{n\in\NN}$ be a martingale. Then, $\Exp(M_\tau)=\Exp(M_0)$.
  \end{corollary}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob,{(F_n)}_{n\in\NN})$ be a filtered probability space, $\tau$ be a stopping time. We define:
    $$
      \mathcal{F}_\tau:=\{A\in\mathcal{F}:\forall n\in\NN, A\cap \{\tau=n\}\in F_n\}
    $$
  \end{definition}
  \begin{remark}
    It can be seen that in the above definition, $\mathcal{F}_\tau$ is a $\sigma$-algebra.
  \end{remark}
  \begin{proposition}
    Let $(\Omega,\mathcal{F},\Prob,{(F_n)}_{n\in\NN})$ be a filtered probability space, $\rho\leq\tau$ be two bounded stopping times, and $M$ be a martingale. Then, $\Exp(M_\tau\mid \mathcal{F}_\rho)=M_\rho$.
  \end{proposition}
  \begin{theorem}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space and ${(M_n)}_{n\in\NN}$ be a martingale such that $\sup_{n\in\NN}{\Exp(\abs{M_n})}<\infty$. Then, there exists a random variable $M_\infty$ such that $M_n\overset{\text{a.s.}}{\to} M_\infty$.
  \end{theorem}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space. A family of random variables ${(X_t)}_{t\in I}$ is said to be \emph{uniformly integrable} if:
    $$
      \lim_{a\to\infty}{\sup_{t\in I}{\Exp(\abs{X_t}\indi{\abs{X_t}>a})}}=0
    $$
  \end{definition}
  \begin{proposition}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space and ${(X_n)}_{n\in\NN}$ be a family of uniformly integrable random variables such that $X_n\overset{\text{a.s.}}{\to} X$. Then, $X$ is integrable and $\Exp(X_n)\to \Exp(X)$.
  \end{proposition}
  \begin{theorem}
    Let $(\Omega,\mathcal{F},\Prob)$ be a probability space and ${(M_n)}_{n\in\NN}$ be a martingale bounded in $L^p$, $1<p<\infty$. Then, there exists a random variable $M_\infty$ such that $M_n\overset{L^p}{\to} M_\infty$ and $M_n\overset{\text{a.s.}}{\to} M_\infty$.
  \end{theorem}
\end{multicols}
\end{document}