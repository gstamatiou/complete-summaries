\documentclass[../../../main_math.tex]{subfiles}

\begin{document}
\changecolor{MM}
\begin{multicols}{2}[\section{Montecarlo methods}]
  The goal of Montecarlo methods is to compute $\Exp(X)$, where $X$ is a random variable. In dimension 1, deterministic methods are more efficient but in higher dimensions ($d\geq 4$), Montecarlo methods are more competitive.
  \subsection{Foundations}
  As always, we consider a probability space $(\Omega,\mathcal{F},\mathbb{P})$ and a random variable $Y\in L^1$.
  \subsubsection{Principle}
  \begin{definition}
    The main idea will be to approximate $\Exp(Y)$ by $\frac{1}{n}\sum_{i=1}^n Y_i:=\overline{Y}_n$, where $Y_i$ are \iid random variables with same law as $Y$. The variable $\overline{Y}_n$ is called the \emph{Montecarlo estimator} of $\Exp(Y)$.
  \end{definition}
  \begin{lemma}
    The Montecarlo estimator is consistent, i.e.\ $\overline{Y}_n\overset{\text{a.s.}}{\longrightarrow}\Exp(Y)$, and unbiased, i.e.\ $\Exp(\overline{Y}_n)=\Exp(Y)$.
  \end{lemma}
  \begin{proof}
    Use the \mnameref{P:stronglawKolmo}.
  \end{proof}
  \begin{lemma}
    Assume $Y\in L^2$ and let $\overline{Y}_n$ be the Montecarlo estimator of $\Exp(Y)$. Then:
    $$
      \norm{\overline{Y}_n-\Exp(Y)}_{2}=\sqrt{\frac{\Var(Y)}{n}}
    $$
  \end{lemma}
  \begin{proof}
    \begin{multline*}
      \norm{\overline{Y}_n-\Exp(Y)}_{2}=\sqrt{\Exp\left(\left(\overline{Y}_n-\Exp(Y)\right)^2\right)}=\\=\sqrt{\Var(\overline{Y}_n)}=\sqrt{\frac{\Var(Y)}{n}}
    \end{multline*}
  \end{proof}
  \begin{lemma}
    Let $Y\in L^2$ and $\overline{Y}_n$ be the Montecarlo estimator of $\Exp(Y)$. Then:
    $$
      \sqrt{n}(\overline{Y}_n - \Exp(Y))\overset{\text{d}}{\longrightarrow}N(0,\Var(Y))
    $$
  \end{lemma}
  \begin{proof}
    Use \mnameref{P:central_limit_thm}.
  \end{proof}
  \begin{remark}
    In practice, we do not know $\Var(Y)$, so we use an estimator of it, such as ${\overline{\sigma}_n}^2=\frac{1}{n-1}\sum_{i=1}^n {(Y_i-\overline{Y}_n)}^2$, which is a consistent unbiased estimator of $\Var(Y)$. Thus:
    $$
      \frac{\sqrt{n}}{\overline{\sigma}_n}(\overline{Y}_n - \Exp(Y))\overset{\text{d}}{\longrightarrow}N(0,1)
    $$
    by \mnameref{P:slutsky}.
  \end{remark}
  \begin{lemma}
    Let $Y\in L^2$ and $\overline{Y}_n$ be the Montecarlo estimator of $\Exp(Y)$. Then, a confidence interval for $\Exp(Y)$ of level $1-\alpha$ is:
    $$
      \text{CI}_\alpha:=\left(\overline{Y}_n-z_{1-\alpha/2}\frac{\overline{\sigma}_n}{\sqrt{n}},\overline{Y}_n+z_{1-\alpha/2}\frac{\overline{\sigma}_n}{\sqrt{n}}\right)
    $$
    where $z_{\alpha/2}$ is the quantile of order $\alpha/2$ of the standard normal distribution.
  \end{lemma}
  \subsubsection{Random number generator}
  In this chapter we will assume that we already now how to simulate sequences of \iid random variables with uniform distribution on $[0,1]$.
  \begin{remark}
    In summary, the computer generates a sequence ${(x_i)}_{0\leq i\leq m}$, with $m$ as large as possible, in the following way: $x_{i+1}=f(x_i)$ and then sets $u_i=\frac{x_i}{m}$. The value $x_0$ is called the \emph{seed} of the sequence and $f$ is chosen with periodicity as high as possible. In the early days of computers, $f(x)=ax+b\mod{m}$, which had periodicity $m\sim 2^{31}-1$. Nowadays, \emph{Mersenne Twister algorithm} is used, which has periodicity $m\sim 2^{19937}-1$.
  \end{remark}
  \subsubsection{Simulation of random variables}
  \begin{lemma}
    Let $U, {(U_i)}_{0\leq i\leq d}\sim U([0,1])$. Then:
    \begin{itemize}
      \item If $a,b\in\RR$ with $a<b$, then $a+(b-a)U\sim U([a,b])$.
      \item If $p\in (0,1)$, then $\indi{U\leq p}\sim\text{Ber}(p)$.
      \item If $p\in (0,1)$, then $\sum_{i=1}^d \indi{U_i\leq p}\sim\text{B}(d,p)$.
      \item If $(x_n),(p_n)\in\RR$ be such that $\sum_{n\geq 0} p_n=1$, then $\sum_{n\geq 0} x_n\indi{\sum_{k=0}^{n-1} p_k\leq U\leq\sum_{k=0}^n p_k}\sim U\left((x_n)\right)$.
      \item If $\prod_{i=1}^d (a_i,b_i)\in \RR^d$ with $a_i<b_i$, then $(a_i+(b_i-a_i)U_i)_{1\leq i\leq d}\sim U\left(\prod_{i=1}^d (a_i,b_i)\right)$.
    \end{itemize}
  \end{lemma}
  \begin{proposition}
    Let $X$ be a random variable with cdf $F$ and $U\sim U([0,1])$. Then,
    $$
      F^{-1}(u)=\inf\{ x\in\RR : F(x)\geq u\}
    $$
    satisfies $F^{-1}(U)\sim X$.
  \end{proposition}
  \begin{proposition}
    Let $U\sim U([0,1])$, $X$ be a random variable with cdf $F$ and $a,b\in\RR$ with $a<b$ be such that $\Prob(a< X\leq b)>0$. Then:
    $$
      F^{-1}\left(F(a)+(F(b)-F(a))U\right)\sim \mathcal{L}(X\mid a< X\leq b)
    $$
  \end{proposition}
  \begin{proposition}[Acceptance-rejection method]
    Let ${(X_i)}_{i\geq 1}$ be \iid $\RR^d$-valued random variables, $D\in \mathcal{B}(\RR^d)$ be such that $\Prob(X_1\in D)>0$ and set:
    $$
      \nu := \inf\{ i\geq 1 : X_i\in D\}
    $$
    Then, $X_\nu\sim \mathcal{L}(X_1\mid X_1\in D)$.
  \end{proposition}
  \begin{remark}
    The principle of the acceptance-rejection method is to simulate conditional distributions by rejecting samples that do not satisfy a prescribed condition.
  \end{remark}
  \begin{proposition}
    Let $f$ be a pdf of some random variable, ${(X_i)}_{i\geq 1}$ be \iid with pdf $g$ and ${(U_i)}_{i\geq 1}$ be \iid $U([0,1])$ independent of ${(X_i)}_{i\geq 1}$. Assume that $\exists c\geq 1$ such that $f(x)\almoste{\leq} cg(x)$ and set:
    $$
      \nu := \inf\{ i\geq 1 : cg(X_i)U_i\leq f(X_i)\}
    $$
    Then, $X_\nu$ admits $f$ as pdf.
  \end{proposition}
  \begin{proposition}
    Let $f$ be a pdf of some random variable and $a_1,a_2\in\RR$ with $a_2>0$ be such that
    $$
      D:=\{ (u,v)\in\RR_{>0}\times \RR:0<u^2<f\left(a_1+a_2\frac{v}{u}\right)\}
    $$
    is bounded. If $(U,V)\sim U(D)$, then $a_1+a_2\frac{V}{U}$ admits $f$ as pdf.
  \end{proposition}
  \subsubsection{Gaussian distribution}
  \begin{proposition}[Box-Muller method]
    Let $U$, $V$ be \iid $U([0,1])$ and set:
    $$
      X:=\sqrt{-2\log(U)}\cos(2\pi V)\quad Y:=\sqrt{-2\log(U)}\sin(2\pi V)
    $$
    Then, $X$ and $Y$ are \iid $N(0,1)$.
  \end{proposition}
  \begin{proof}
    Let $\varphi:\RR^2\to\RR$ be bounded and measurable. Then:
    \begin{multline*}
      \Exp(\varphi(X,Y))=\\=\!\!\!\int_{{(0,1)}^2}\!\!\!\!\varphi\left( \sqrt{-2\log u} \cos(2\pi v), \sqrt{-2\log u} \sin(2\pi v)\right)\dd{u}\dd{v}=\\=\int_{\RR^2}\varphi(x,y) \frac{1}{2\pi}\exp{-\frac{x^2+y^2}{2}}\dd{x}\dd{y}
    \end{multline*}
    by the change of variable formula. Thus, $X$ and $Y$ are \iid $N(0,1)$.
  \end{proof}
  \begin{proposition}[Polar method]
    Let $U$, $V$ be \iid $U(\DD)$, where $\DD\subset \RR^2$ is the open unit disk. Let $R^2=U^2+V^2$ and set:
    $$
      X:=U\sqrt{\frac{-2\log(R^2)}{R^2}}\quad Y:=V\sqrt{\frac{-2\log(R^2)}{R^2}}
    $$
    Then, $X$, $Y$ are \iid $N(0,1)$.
  \end{proposition}
  \begin{proposition}
    Let $\vf{X}\in N_d(0,\vf{I}_d)$, $\vf\mu\in\RR^d$ and $\vf{A}\in\mathcal{M}_d(\RR)$. Then, $\vf\mu+\vf{AX}\sim N_d(\vf\mu,\vf{A}\transpose{\vf{A}})$.
  \end{proposition}
  \begin{remark}
    To simulate $\vf{Y}\sim N_d(\vf\mu,\vf\Sigma)$, we proceed as follows:
    \begin{enumerate}
      \item Find $\vf{A}\in\mathcal{M}_d(\RR)$ such that $\vf\Sigma=\vf{A}\transpose{\vf{A}}$ (e.g.\ by Cholesky decomposition).
      \item Simulate $\vf{X}\sim N_d(0,\vf{I}_d)$.
      \item Set $\vf{Y}=\vf\mu+\vf{AX}$.
    \end{enumerate}
  \end{remark}
  \subsection{Variance reduction techniques}
  \subsubsection{Antithetic control}
  \begin{definition}
    Let $Y=g(X)$ be a random variable with $X\sim N(0,\sigma^2)$. The \emph{antithetic method} consists in using the estimator:
    $$
      \overline{Y}_n^{\text{A}}:=\frac{1}{n} \sum_{i=1}^n \frac{g(X_i)+g(-X_i)}{2}
    $$
    where ${(X_i)}_{1\leq i\leq n}$ are \iid $N(0,\sigma^2)$.
  \end{definition}
  \begin{lemma}
    The antithetic estimator $\overline{Y}_n^{\text{A}}$ is an unbiased and consistent estimator of $\Exp(Y)$. Furthermore:
    $$
      \sqrt{n}\left(\overline{Y}_n^{\text{A}}-\Exp(Y)\right)\overset{\text{d}}{\longrightarrow}N\left(0,\Var\left(\frac{g(X)+g(-X)}{2}\right)\right)
    $$
  \end{lemma}
  \begin{remark}
    In terms of computational cost, $\overline{Y}_n^{\text{A}}$ is more expensive than $\overline{Y}_n$ but cheaper than $\overline{Y}_{2n}$.
  \end{remark}
  \begin{proposition}
    We have that $\Var(\overline{Y}_n^{\text{A}})\leq \Var(\overline{Y}_n)$. Moreover, if
    $$
      \cov(g(X),g(-X))\leq 0
    $$
    then $\Var(\overline{Y}_n^{\text{A}})\leq \Var(\overline{Y}_{2n})$.
  \end{proposition}
  \begin{proof}
    \begin{align*}
      \Var(\overline{Y}_n^{\text{A}}) & =\frac{1}{2n}\left(\Var(g(X))+\cov(g(X),g(-X))\right) \\
                                      & \leq \Var(\overline{Y}_n)
    \end{align*}
    And clearly if $\cov(g(X),g(-X))\leq 0$, then $\Var(\overline{Y}_n^{\text{A}})\leq \Var(\overline{Y}_{2n})$.
  \end{proof}
  \begin{proposition}
    If $g$ is monotone, then:
    $$
      \cov(g(X),g(-X))\leq 0
    $$
  \end{proposition}
  \begin{proof}
    Let $X_1$, $X_2$ be \iid copies of $X$. Then, since $g$ is monotone:
    $$
      (g(X_1)-g(X_2))(g(-X_1)-g(-X_2))\leq 0
    $$
    Now taking expectations:
    \begin{multline*}
      0\geq\Exp((g(X_1)-g(X_2))(g(-X_1)-g(-X_2)))=\\=2 \cov(g(X),g(-X))
    \end{multline*}
  \end{proof}
  \subsubsection{Control variate}
  \begin{definition}
    The principle of the \emph{control variate} is to find a real-valued random variable $X$ such that $\Exp(X)$ is known, and a constant $b\in\RR$ such that:
    $$
      \Var(Y+b(X-\Exp(X)))\ll \Var(Y)
    $$
    We define $Y(b):=Y+b(X-\Exp(X))$.
    This suggests the following estimator:
    $$
      \overline{Y}_n(b):=\frac{1}{n}\sum_{i=1}^n [Y_i+b(X_i-\Exp(X))]
    $$
    where ${(Y_i)}_{1\leq i\leq n}$ are \iid copies of $Y$ and ${(X_i)}_{1\leq i\leq n}$ are \iid copies of $X$.
  \end{definition}
  \begin{lemma}
    The control variate estimator $\overline{Y}_n(b)$ is unbiased and consistent. Furthermore:
    $$
      \sqrt{n}\left(\overline{Y}_n(b)-\Exp(Y)\right)\overset{\text{d}}{\longrightarrow}N\left(0,\Var(Y(b))\right)
    $$
    where $Y(b):=Y+b(X-\Exp(X))$.
  \end{lemma}
  \begin{remark}
    If $b=0$, the control variate estimator $\overline{Y}_n(b)$ coincides with the classical estimator $\overline{Y}_n$. Otherwise, the computational cost of $\overline{Y}_n(b)$ is higher than $\overline{Y}_n$, but it does not depend on the choice of $b\neq 0$.
  \end{remark}
  \begin{proposition}
    The minimum of $\Var(Y(b))$ is attained for $$\hat{b}= -\frac{\cov(Y,X)}{\Var(X)}$$ and in that case, $\Var(Y(\hat{b}))=\Var(Y)(1-{\rho_{XY}}^2)$, where $\rho_{XY}$ is the correlation between $X$ and $Y$.
  \end{proposition}
  \begin{remark}
    Usually, $\hat{b}$ is unknown, but we can use an estimator of it, such as:
    $$
      \hat{b}_n:=-\frac{\sum_{i=1}^n (Y_i-\overline{Y}_n)(X_i-\overline{X}_n)}{\sum_{i=1}^n {(X_i-\overline{X}_n)}^2}
    $$
    but if we know $\Exp(X)$ and $\Var(X)$ explicitly, we can use them in the formula of $\hat{b}_n$.
  \end{remark}
  \begin{remark}
    The result of above tell us that we should pick $X$ strongly correlated to $Y$ but simple enough to know explicitly $\Exp(X)$.
  \end{remark}
  \begin{definition}
    Let $\vf{X}$ be a random vector such that $\Exp(\vf{X})$ is known, and $\vf{b}\in \RR^d$. We define the \emph{multiple control variate estimator} as:
    $$
      \overline{Y}_n(\vf{b}):=\frac{1}{n}\sum_{i=1}^n [Y_i+\transpose{\vf{b}}(\vf{X}_i-\Exp(\vf{X}))]
    $$
    where ${(Y_i)}_{1\leq i\leq n}$ are \iid copies of $Y$ and ${(\vf{X}_i)}_{1\leq i\leq n}$ are \iid copies of $\vf{X}$.
    We also define $Y(\vf{b}):= Y+\transpose{\vf{b}}(\vf{X}-\Exp(\vf{X}))$.
  \end{definition}
  \begin{lemma}
    The multiple control variate estimator $\overline{Y}_n(\vf{b})$ is unbiased and consistent. Furthermore:
    $$
      \sqrt{n}\left(\overline{Y}_n(\vf{b})-\Exp(Y)\right)\overset{\text{d}}{\longrightarrow}N\left(0,\Var(Y(\vf{b}))\right)
    $$
  \end{lemma}
  \begin{proposition}
    The minimum of $\Var(Y(\vf{b}))$ is attained for $$\hat{\vf{b}}=
      -\Var(\vf{X})^{-1}\cov(\vf{X},Y)
    $$
    and in that case, $\Var(Y(\hat{\vf{b}}))=\Var(Y)(1-R^2)$, where:
    $$
      R^2:=\frac{\transpose{\cov(\vf{X},Y)}\Var(\vf{X})^{-1}\cov(\vf{X},Y)}{\Var(Y)}
    $$
  \end{proposition}
  \subsubsection{Importance sampling}
  \begin{definition}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space. We say that a probability measure $\QQ$ is \emph{equivalent} to $\Prob$ if:
    $$
      \{A\in \mathcal{F} : \Prob(A)=0\}=\{A\in \mathcal{F} : \QQ(A)=0\}
    $$
  \end{definition}
  \begin{lemma}
    Let $\QQ$ be a probability measure equivalent to $\Prob$. Then, there exists a random variable $L>0$ such that $\forall A\in \mathcal{F}$, we have $\QQ(A)=\Exp_\Prob(L\indi{A})$. Furthermore, for all $X$ bounded:
    $$
      \Exp_\QQ(X)=\Exp_\Prob(L X)\qquad \Exp_\Prob(X)=\Exp_\QQ\left(\frac{X}{L}\right)
    $$
    Conversely, if we have a random variable $L>0$ such that $\Exp_\Prob(L)=1$, then
    $$
      \function{\QQ}{\mathcal{F}}{[0,1]}{A}{\Exp_\Prob(L\indi{A})}
    $$
    is a probability measure equivalent to $\Prob$.
  \end{lemma}
  \begin{remark}
    The principle of the importance sampling method is to change the probability measure in order to give more weight to important outcomes.
  \end{remark}
  \begin{definition}
    Let $(\QQ,L)$ be such that it defines a probability measure equivalent to $\Prob$. The \emph{importance sampling estimator} is defined as:
    $$
      \overline{Y}_n^{\QQ}:=\frac{1}{n}\sum_{i=1}^n {L_i}^{-1}Y_i
    $$
    where $(L_i,Y_i)$ are \iid copies of $(L,Y)$ (under $\QQ$).
  \end{definition}
  \begin{lemma}
    The importance sampling estimator $\overline{Y}_n^{\QQ}$ is unbiased and consistent. Furthermore:
    $$
      \sqrt{n}\left(\overline{Y}_n^{\QQ}-\Exp_\Prob(Y)\right)\overset{\text{d}}{\longrightarrow}N\left(0,\Var_{\QQ}(L^{-1}Y)\right)
    $$
  \end{lemma}
  \begin{remark}
    In terms of computational cost, the importance sampling estimator $\overline{Y}_n^{\QQ}$ is more expensive than the classical estimator $\overline{Y}_n$. In terms of precision, the best estimator is the one with the smallest variance so we have to compare $\Var(Y)$ and $\Var_{\QQ}(L^{-1}Y)$. It holds:
    $$
      \Var_{\QQ}(L^{-1}Y)=\Exp_\Prob\left(L^{-1}Y^2\right)-{\Exp_\Prob(Y)}^2
    $$
    This quantity can be larger or smaller than $\Var(Y )$ depending on the choice of $L$ and thus the success of importance sampling relies on the choice of an effective change of probability measure.
  \end{remark}
  \subsection{Simulation of diffusion processes}
  The aim of this section is to develop methods to simulate solutions to SDEs of the form:
  \begin{equation}\label{MM:SDE}
    \begin{cases}
      \dd{\vf{X}_t}=\vf{b}(\vf{X}_t)\dd{t}+\vf{\sigma}(\vf{X}_t)\dd{\vf{B}_t}
      \vf{X}_0=\vf{x}_0
    \end{cases}
  \end{equation}
  where $\vf{b}:\RR^d\to\RR^d$ and $\vf{\sigma}:\RR^d\to\mathcal{M}_{d\times d}(\RR)$ are Lipschitz continuous.
  \subsubsection{Exact simulation}
  \begin{proposition}
    To simulate a sample $(\vf{B}_{t_1},\dots,\vf{B}_{t_n})$ of a $d$-dimensional Brownian motions, we can use the following algorithm:
    \begin{enumerate}
      \item Generate $(\vf{Z}_1,\dots,\vf{Z}_n)$ \iid $N_d(0,\vf{I}_d)$.
      \item Set $\vf{B}_{t_0}:=0$ and for all $0\leq i\leq n-1$, set:
            $$
              \vf{B}_{t_{i+1}}=\vf{B}_{t_i}+\sqrt{t_{i+1}-t_i}\vf{Z}_{i+1}
            $$
    \end{enumerate}
  \end{proposition}
  \subsubsection{Euler scheme}
  \begin{definition}
    Consider the SDE of \mcref{MM:SDE} and let $h$ be a discretization step. The \emph{Euler method} consists in:
    \begin{align*}
      \vf{X}_{t+h} & =\vf{X}_t+\int_t^{t+h}\vf{b}(\vf{X}_s)\dd{s}+\int_t^{t+h}\vf{\sigma}(\vf{X}_s)\dd{\vf{B}_s} \\
                   & \approx \vf{X}_t+h\vf{b}(\vf{X}_t)+\vf{\sigma}(\vf{X}_t)(\vf{B}_{t+h}-\vf{B}_t)
    \end{align*}
    More generally, if we want to obtain the solution at $(t_1,\dots,t_n)$, we can use the following algorithm. Set $\vf{\tilde{X}}_0:=\vf{x}_0$ and for all $0\leq i\leq n-1$, set:
    $$
      \vf{\tilde{X}}_{t_{i+1}}=\vf{\tilde{X}}_{t_i}+(t_{i+1}-t_i)\vf{b}(\vf{\tilde{X}}_{t_i})+\vf{\sigma}(\vf{\tilde{X}}_{t_i})(\vf{B}_{t_{i+1}}-\vf{B}_{t_i})
    $$
  \end{definition}
  \begin{remark}
    Note that Euler scheme reduces to generating independent increments $\vf{B}_{t_{i+1}}-\vf{B}_{t_i}\sim \sqrt{t_{i+1}-t_i}N_d(0,\vf{I}_d)$.
  \end{remark}
  \begin{remark}
    Trying to build an implicit Euler scheme for SDEs is much more complicated than for ODEs, as we need to ensure that the process is still adapted.
  \end{remark}
  \begin{theorem}
    Let $\vf{X}$ be the solution to \mcref{MM:SDE} and $\vf{\tilde{X}}$ be the solution to the Euler scheme. Then, for $p\geq 1$:
    $$
      \Exp\left(\sup_{0\leq t\leq T}\norm{\vf{X}_t-\vf{\tilde{X}}_t}_p\right)\leq C_p h
    $$
  \end{theorem}
\end{multicols}
\end{document}