\documentclass[../../../main_math.tex]{subfiles}

\begin{document}
\changecolor{MM}
\begin{multicols}{2}[\section{Montecarlo methods}]
  The goal of Montecarlo methods is to compute $\Exp(X)$, where $X$ is a random variable. In dimension 1, deterministic methods are more efficient but in higher dimensions ($d\geq 4$), Montecarlo methods are more competitive.
  \subsection{Foundations}
  As always, we consider a probability space $(\Omega,\mathcal{F},\mathbb{P})$ and a random variable $Y\in L^1$.
  \subsubsection{Principle}
  \begin{definition}
    The main idea will be to approximate $\Exp(Y)$ by $\frac{1}{n}\sum_{i=1}^n Y_i:=\overline{Y}_n$, where $Y_i$ are \iid random variables with same law as $Y$. The variable $\overline{Y}_n$ is called the \emph{Montecarlo estimator} of $\Exp(Y)$.
  \end{definition}
  \begin{lemma}
    The Montecarlo estimator is consistent, i.e.\ $\overline{Y}_n\overset{\text{a.s.}}{\longrightarrow}\Exp(Y)$, and unbiased, i.e.\ $\Exp(\overline{Y}_n)=\Exp(Y)$.
  \end{lemma}
  \begin{proof}
    Use the \mnameref{P:stronglawKolmo}.
  \end{proof}
  \begin{lemma}
    Assume $Y\in L^2$ and let $\overline{Y}_n$ be the Montecarlo estimator of $\Exp(Y)$. Then:
    $$
      \norm{\overline{Y}_n-\Exp(Y)}_{2}=\sqrt{\frac{\Var(Y)}{n}}
    $$
  \end{lemma}
  \begin{proof}
    \begin{multline*}
      \norm{\overline{Y}_n-\Exp(Y)}_{2}=\sqrt{\Exp\left(\left(\overline{Y}_n-\Exp(Y)\right)^2\right)}=\\=\sqrt{\Var(\overline{Y}_n)}=\sqrt{\frac{\Var(Y)}{n}}
    \end{multline*}
  \end{proof}
  \begin{lemma}
    Let $Y\in L^2$ and $\overline{Y}_n$ be the Montecarlo estimator of $\Exp(Y)$. Then:
    $$
      \sqrt{n}(\overline{Y}_n - \Exp(Y))\overset{\text{d}}{\longrightarrow}N(0,\Var(Y))
    $$
  \end{lemma}
  \begin{proof}
    Use \mnameref{P:central_limit_thm}.
  \end{proof}
  \begin{remark}
    In practice, we do not know $\Var(Y)$, so we use an estimator of it, such as ${\overline{\sigma}_n}^2=\frac{1}{n-1}\sum_{i=1}^n {(Y_i-\overline{Y}_n)}^2$, which is a consistent unbiased estimator of $\Var(Y)$. Thus:
    $$
      \frac{\sqrt{n}}{\overline{\sigma}_n}(\overline{Y}_n - \Exp(Y))\overset{\text{d}}{\longrightarrow}N(0,1)
    $$
    by \mnameref{P:slutsky}.
  \end{remark}
  \begin{lemma}
    Let $Y\in L^2$ and $\overline{Y}_n$ be the Montecarlo estimator of $\Exp(Y)$. Then, a confidence interval for $\Exp(Y)$ of level $1-\alpha$ is:
    $$
      \text{CI}_\alpha:=\left(\overline{Y}_n-z_{1-\alpha/2}\frac{\overline{\sigma}_n}{\sqrt{n}},\overline{Y}_n+z_{1-\alpha/2}\frac{\overline{\sigma}_n}{\sqrt{n}}\right)
    $$
    where $z_{\alpha/2}$ is the quantile of order $\alpha/2$ of the standard normal distribution.
  \end{lemma}
  \subsubsection{Random number generator}
  In this chapter we will assume that we already now how to simulate sequences of \iid random variables with uniform distribution on $[0,1]$.
  \begin{remark}
    In summary, the computer generates a sequence ${(x_i)}_{0\leq i\leq m}$, with $m$ as large as possible, in the following way: $x_{i+1}=f(x_i)$ and then sets $u_i=\frac{x_i}{m}$. The value $x_0$ is called the \emph{seed} of the sequence and $f$ is chosen with periodicity as high as possible. In the early days of computers, $f(x)=ax+b\mod{m}$, which had periodicity $m\sim 2^{31}-1$. Nowadays, \emph{Mersenne Twister algorithm} is used, which has periodicity $m\sim 2^{19937}-1$.
  \end{remark}
  \subsubsection{Simulation of random variables}
  \begin{lemma}
    Let $U, {(U_i)}_{0\leq i\leq d}\sim U([0,1])$. Then:
    \begin{itemize}
      \item If $a,b\in\RR$ with $a<b$, then $a+(b-a)U\sim U([a,b])$.
      \item If $p\in (0,1)$, then $\indi{U\leq p}\sim\text{Ber}(p)$.
      \item If $p\in (0,1)$, then $\sum_{i=1}^d \indi{U_i\leq p}\sim\text{B}(d,p)$.
      \item If $(x_n),(p_n)\in\RR$ be such that $\sum_{n\geq 0} p_n=1$, then $\sum_{n\geq 0} x_n\indi{\sum_{k=0}^{n-1} p_k\leq U\leq\sum_{k=0}^n p_k}\sim U\left((x_n)\right)$.
      \item If $\prod_{i=1}^d (a_i,b_i)\in \RR^d$ with $a_i<b_i$, then $(a_i+(b_i-a_i)U_i)_{1\leq i\leq d}\sim U\left(\prod_{i=1}^d (a_i,b_i)\right)$.
    \end{itemize}
  \end{lemma}
  \begin{proposition}
    Let $X$ be a random variable with cdf $F$ and $U\sim U([0,1])$. Then,
    $$
      F^{-1}(u)=\inf\{ x\in\RR : F(x)\geq u\}
    $$
    satisfies $F^{-1}(U)\sim X$.
  \end{proposition}
  \begin{proposition}
    Let $U\sim U([0,1])$, $X$ be a random variable with cdf $F$ and $a,b\in\RR$ with $a<b$ be such that $\Prob(a< X\leq b)>0$. Then:
    $$
      F^{-1}\left(F(a)+(F(b)-F(a))U\right)\sim \mathcal{L}(X\mid a< X\leq b)
    $$
  \end{proposition}
  \begin{proposition}[Acceptance-rejection method]
    Let ${(X_i)}_{i\geq 1}$ be \iid $\RR^d$-valued random variables, $D\in \mathcal{B}(\RR^d)$ be such that $\Prob(X_1\in D)>0$ and set:
    $$
      \nu := \inf\{ i\geq 1 : X_i\in D\}
    $$
    Then, $X_\nu\sim \mathcal{L}(X_1\mid X_1\in D)$.
  \end{proposition}
  \begin{remark}
    The principle of the acceptance-rejection method is to simulate conditional distributions by rejecting samples that do not satisfy a prescribed condition.
  \end{remark}
  \begin{proposition}
    Let $f$ be a pdf of some random variable, ${(X_i)}_{i\geq 1}$ be \iid with pdf $g$ and ${(U_i)}_{i\geq 1}$ be \iid $U([0,1])$ independent of ${(X_i)}_{i\geq 1}$. Assume that $\exists c\geq 1$ such that $f(x)\almoste{\leq} cg(x)$ and set:
    $$
      \nu := \inf\{ i\geq 1 : cg(X_i)U_i\leq f(X_i)\}
    $$
    Then, $X_\nu$ admits $f$ as pdf.
  \end{proposition}
  \begin{proposition}
    Let $f$ be a pdf of some random variable and $a_1,a_2\in\RR$ with $a_2>0$ be such that
    $$
      D:=\{ (u,v)\in\RR_{>0}\times \RR:0<u^2<f\left(a_1+a_2\frac{v}{u}\right)\}
    $$
    is bounded. If $(U,V)\sim U(D)$, then $a_1+a_2\frac{V}{U}$ admits $f$ as pdf.
  \end{proposition}
  \subsubsection{Gaussian distribution}
  \begin{proposition}[Box-Muller method]
    Let $U$, $V$ be \iid $U([0,1])$ and set:
    $$
      X:=\sqrt{-2\log(U)}\cos(2\pi V)\quad Y:=\sqrt{-2\log(U)}\sin(2\pi V)
    $$
    Then, $X$ and $Y$ are \iid $N(0,1)$.
  \end{proposition}
  \begin{proof}
    Let $\varphi:\RR^2\to\RR$ be bounded and measurable. Then:
    \begin{multline*}
      \Exp(\varphi(X,Y))=\\=\!\!\!\int_{{(0,1)}^2}\!\!\!\!\varphi\left( \sqrt{-2\log u} \cos(2\pi v), \sqrt{-2\log u} \sin(2\pi v)\right)\dd{u}\dd{v}=\\=\int_{\RR^2}\varphi(x,y) \frac{1}{2\pi}\exp{-\frac{x^2+y^2}{2}}\dd{x}\dd{y}
    \end{multline*}
    by the change of variable formula. Thus, $X$ and $Y$ are \iid $N(0,1)$.
  \end{proof}
  \begin{proposition}[Polar method]
    Let $U$, $V$ be \iid $U(\DD)$, where $\DD\subset \RR^2$ is the open unit disk. Let $R^2=U^2+V^2$ and set:
    $$
      X:=U\sqrt{\frac{-2\log(R^2)}{R^2}}\quad Y:=V\sqrt{\frac{-2\log(R^2)}{R^2}}
    $$
    Then, $X$, $Y$ are \iid $N(0,1)$.
  \end{proposition}
  \begin{proposition}
    Let $\vf{X}\in N_d(0,\vf{I}_d)$, $\vf\mu\in\RR^d$ and $\vf{A}\in\mathcal{M}_d(\RR)$. Then, $\vf\mu+\vf{AX}\sim N_d(\vf\mu,\vf{A}\transpose{\vf{A}})$.
  \end{proposition}
  \begin{remark}
    To simulate $\vf{Y}\sim N_d(\vf\mu,\vf\Sigma)$, we proceed as follows:
    \begin{enumerate}
      \item Find $\vf{A}\in\mathcal{M}_d(\RR)$ such that $\vf\Sigma=\vf{A}\transpose{\vf{A}}$ (e.g.\ by Cholesky decomposition).
      \item Simulate $\vf{X}\sim N_d(0,\vf{I}_d)$.
      \item Set $\vf{Y}=\vf\mu+\vf{AX}$.
    \end{enumerate}
  \end{remark}
  \subsection{Variance reduction techniques}
  \subsubsection{Antithetic control}
  \begin{definition}
    Let $Y=g(X)$ be a random variable with $X\sim N(0,\sigma^2)$. The \emph{antithetic method} consists in using the estimator:
    $$
      \overline{Y}_n^{\text{A}}:=\frac{1}{n} \sum_{i=1}^n \frac{g(X_i)+g(-X_i)}{2}
    $$
    where ${(X_i)}_{1\leq i\leq n}$ are \iid $N(0,\sigma^2)$.
  \end{definition}
  \begin{lemma}
    The antithetic estimator $\overline{Y}_n^{\text{A}}$ is an unbiased and consistent estimator of $\Exp(Y)$. Furthermore:
    $$
      \sqrt{n}\left(\overline{Y}_n^{\text{A}}-\Exp(Y)\right)\overset{\text{d}}{\longrightarrow}N\left(0,\Var\left(\frac{g(X)+g(-X)}{2}\right)\right)
    $$
  \end{lemma}
  \begin{remark}
    In terms of computational cost, $\overline{Y}_n^{\text{A}}$ is more expensive than $\overline{Y}_n$ but cheaper than $\overline{Y}_{2n}$.
  \end{remark}
  \begin{proposition}
    We have that $\Var(\overline{Y}_n^{\text{A}})\leq \Var(\overline{Y}_n)$. Moreover, if
    $$
      \cov(g(X),g(-X))\leq 0
    $$
    then $\Var(\overline{Y}_n^{\text{A}})\leq \Var(\overline{Y}_{2n})$.
  \end{proposition}
  \begin{proof}
    \begin{align*}
      \Var(\overline{Y}_n^{\text{A}}) & =\frac{1}{2n}\left(\Var(g(X))+\cov(g(X),g(-X))\right) \\
                                      & \leq \Var(\overline{Y}_n)
    \end{align*}
    And clearly if $\cov(g(X),g(-X))\leq 0$, then $\Var(\overline{Y}_n^{\text{A}})\leq \Var(\overline{Y}_{2n})$.
  \end{proof}
  \begin{proposition}
    If $g$ is monotone, then:
    $$
      \cov(g(X),g(-X))\leq 0
    $$
  \end{proposition}
  \begin{proof}
    Let $X_1$, $X_2$ be \iid copies of $X$. Then, since $g$ is monotone:
    $$
      (g(X_1)-g(X_2))(g(-X_1)-g(-X_2))\leq 0
    $$
    Now taking expectations:
    \begin{multline*}
      0\geq\Exp((g(X_1)-g(X_2))(g(-X_1)-g(-X_2)))=\\=2 \cov(g(X),g(-X))
    \end{multline*}
  \end{proof}
  \subsubsection{Control variate}
  \begin{definition}
    The principle of the \emph{control variate} is to find a real-valued random variable $X$ such that $\Exp(X)$ is known, and a constant $b\in\RR$ such that:
    $$
      \Var(Y+b(X-\Exp(X)))\ll \Var(Y)
    $$
    We define $Y(b):=Y+b(X-\Exp(X))$.
    This suggests the following estimator:
    $$
      \overline{Y}_n(b):=\frac{1}{n}\sum_{i=1}^n [Y_i+b(X_i-\Exp(X))]
    $$
    where ${(Y_i)}_{1\leq i\leq n}$ are \iid copies of $Y$ and ${(X_i)}_{1\leq i\leq n}$ are \iid copies of $X$.
  \end{definition}
  \begin{lemma}
    The control variate estimator $\overline{Y}_n(b)$ is unbiased and consistent. Furthermore:
    $$
      \sqrt{n}\left(\overline{Y}_n(b)-\Exp(Y)\right)\overset{\text{d}}{\longrightarrow}N\left(0,\Var(Y(b))\right)
    $$
    where $Y(b):=Y+b(X-\Exp(X))$.
  \end{lemma}
  \begin{remark}
    If $b=0$, the control variate estimator $\overline{Y}_n(b)$ coincides with the classical estimator $\overline{Y}_n$. Otherwise, the computational cost of $\overline{Y}_n(b)$ is higher than $\overline{Y}_n$, but it does not depend on the choice of $b\neq 0$.
  \end{remark}
  \begin{proposition}
    The minimum of $\Var(Y(b))$ is attained for $$\hat{b}= -\frac{\cov(Y,X)}{\Var(X)}$$ and in that case, $\Var(Y(\hat{b}))=\Var(Y)(1-{\rho_{XY}}^2)$, where $\rho_{XY}$ is the correlation between $X$ and $Y$.
  \end{proposition}
  \begin{remark}
    Usually, $\hat{b}$ is unknown, but we can use an estimator of it, such as:
    $$
      \hat{b}_n:=-\frac{\sum_{i=1}^n (Y_i-\overline{Y}_n)(X_i-\overline{X}_n)}{\sum_{i=1}^n {(X_i-\overline{X}_n)}^2}
    $$
    but if we know $\Exp(X)$ and $\Var(X)$ explicitly, we can use them in the formula of $\hat{b}_n$.
  \end{remark}
  \begin{remark}
    The result of above tell us that we should pick $X$ strongly correlated to $Y$ but simple enough to know explicitly $\Exp(X)$.
  \end{remark}
  \begin{definition}
    Let $\vf{X}$ be a random vector such that $\Exp(\vf{X})$ is known, and $\vf{b}\in \RR^d$. We define the \emph{multiple control variate estimator} as:
    $$
      \overline{Y}_n(\vf{b}):=\frac{1}{n}\sum_{i=1}^n [Y_i+\transpose{\vf{b}}(\vf{X}_i-\Exp(\vf{X}))]
    $$
    where ${(Y_i)}_{1\leq i\leq n}$ are \iid copies of $Y$ and ${(\vf{X}_i)}_{1\leq i\leq n}$ are \iid copies of $\vf{X}$.
    We also define $Y(\vf{b}):= Y+\transpose{\vf{b}}(\vf{X}-\Exp(\vf{X}))$.
  \end{definition}
  \begin{lemma}
    The multiple control variate estimator $\overline{Y}_n(\vf{b})$ is unbiased and consistent. Furthermore:
    $$
      \sqrt{n}\left(\overline{Y}_n(\vf{b})-\Exp(Y)\right)\overset{\text{d}}{\longrightarrow}N\left(0,\Var(Y(\vf{b}))\right)
    $$
  \end{lemma}
  \begin{proposition}
    The minimum of $\Var(Y(\vf{b}))$ is attained for $$\hat{\vf{b}}=
      -\Var(\vf{X})^{-1}\cov(\vf{X},Y)
    $$
    and in that case, $\Var(Y(\hat{\vf{b}}))=\Var(Y)(1-R^2)$, where:
    $$
      R^2:=\frac{\transpose{\cov(\vf{X},Y)}\Var(\vf{X})^{-1}\cov(\vf{X},Y)}{\Var(Y)}
    $$
  \end{proposition}
  \subsubsection{Importance sampling}
  \begin{definition}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space. We say that a probability measure $\QQ$ is \emph{equivalent} to $\Prob$ if:
    $$
      \{A\in \mathcal{F} : \Prob(A)=0\}=\{A\in \mathcal{F} : \QQ(A)=0\}
    $$
  \end{definition}
  \begin{lemma}
    Let $\QQ$ be a probability measure equivalent to $\Prob$. Then, there exists a random variable $L>0$ such that $\forall A\in \mathcal{F}$, we have $\QQ(A)=\Exp_\Prob(L\indi{A})$. Furthermore, for all $X$ bounded:
    $$
      \Exp_\QQ(X)=\Exp_\Prob(L X)\qquad \Exp_\Prob(X)=\Exp_\QQ\left(\frac{X}{L}\right)
    $$
    Conversely, if we have a random variable $L>0$ such that $\Exp_\Prob(L)=1$, then
    $$
      \function{\QQ}{\mathcal{F}}{[0,1]}{A}{\Exp_\Prob(L\indi{A})}
    $$
    is a probability measure equivalent to $\Prob$.
  \end{lemma}
  \begin{remark}
    The principle of the importance sampling method is to change the probability measure in order to give more weight to important outcomes.
  \end{remark}
  \begin{definition}
    Let $(\QQ,L)$ be such that it defines a probability measure equivalent to $\Prob$. The \emph{importance sampling estimator} is defined as:
    $$
      \overline{Y}_n^{\QQ}:=\frac{1}{n}\sum_{i=1}^n {L_i}^{-1}Y_i
    $$
    where $(L_i,Y_i)$ are \iid copies of $(L,Y)$ (under $\QQ$).
  \end{definition}
  \begin{lemma}
    The importance sampling estimator $\overline{Y}_n^{\QQ}$ is unbiased and consistent. Furthermore:
    $$
      \sqrt{n}\left(\overline{Y}_n^{\QQ}-\Exp_\Prob(Y)\right)\overset{\text{d}}{\longrightarrow}N\left(0,\Var_{\QQ}(L^{-1}Y)\right)
    $$
  \end{lemma}
  \begin{remark}
    In terms of computational cost, the importance sampling estimator $\overline{Y}_n^{\QQ}$ is more expensive than the classical estimator $\overline{Y}_n$. In terms of precision, the best estimator is the one with the smallest variance so we have to compare $\Var(Y)$ and $\Var_{\QQ}(L^{-1}Y)$. It holds:
    $$
      \Var_{\QQ}(L^{-1}Y)=\Exp_\Prob\left(L^{-1}Y^2\right)-{\Exp_\Prob(Y)}^2
    $$
    This quantity can be larger or smaller than $\Var(Y )$ depending on the choice of $L$ and thus the success of importance sampling relies on the choice of an effective change of probability measure.
  \end{remark}
  \subsection{Simulation of diffusion processes}
  The aim of this section is to develop methods to simulate solutions to SDEs of the form:
  \begin{equation}\label{MM:SDE}
    \begin{cases}
      \dd{\vf{X}_t}=\vf{b}(\vf{X}_t)\dd{t}+\vf{\sigma}(\vf{X}_t)\dd{\vf{B}_t}
      \vf{X}_0=\vf{x}_0
    \end{cases}
  \end{equation}
  where $\vf{b}:\RR^d\to\RR^d$ and $\vf{\sigma}:\RR^d\to\mathcal{M}_{d\times d}(\RR)$ are Lipschitz continuous.
  \subsubsection{Exact simulation}
  \begin{proposition}
    To simulate a sample $(\vf{B}_{t_1},\dots,\vf{B}_{t_m})$ of a $d$-dimensional Brownian motions, we can use the following algorithm:
    \begin{enumerate}
      \item Generate $(\vf{Z}_1,\dots,\vf{Z}_m)$ \iid $N_d(0,\vf{I}_d)$.
      \item Set $\vf{B}_{t_0}:=0$ and for all $0\leq i\leq m-1$, set:
            $$
              \vf{B}_{t_{i+1}}=\vf{B}_{t_i}+\sqrt{t_{i+1}-t_i}\vf{Z}_{i+1}
            $$
    \end{enumerate}
  \end{proposition}
  \subsubsection{Euler scheme}
  \begin{definition}
    Consider the SDE of \mcref{MM:SDE} and let $h$ be a discretization step. The \emph{Euler method} consists in:
    \begin{align*}
      \vf{X}_{t+h} & =\vf{X}_t+\int_t^{t+h}\vf{b}(\vf{X}_s)\dd{s}+\int_t^{t+h}\vf{\sigma}(\vf{X}_s)\dd{\vf{B}_s} \\
                   & \approx \vf{X}_t+h\vf{b}(\vf{X}_t)+\vf{\sigma}(\vf{X}_t)(\vf{B}_{t+h}-\vf{B}_t)
    \end{align*}
    More generally, if we want to obtain the solution at $(t_1,\dots,t_n)$, we can use the following algorithm. Set $\vf{\tilde{X}}^m_0:=\vf{x}_0$ and for all $0\leq i\leq m-1$, set:
    $$
      \vf{\tilde{X}}^m_{t_{i+1}}=\vf{\tilde{X}}^m_{t_i}+(t_{i+1}-t_i)\vf{b}(\vf{\tilde{X}}^m_{t_i})+\vf{\sigma}(\vf{\tilde{X}}^m_{t_i})(\vf{B}_{t_{i+1}}-\vf{B}_{t_i})
    $$
  \end{definition}
  \begin{remark}
    Note that Euler scheme reduces to generating independent increments $\vf{B}_{t_{i+1}}-\vf{B}_{t_i}\sim \sqrt{t_{i+1}-t_i}N_d(0,\vf{I}_d)$.
  \end{remark}
  \begin{remark}
    Trying to build an implicit Euler scheme for SDEs is much more complicated than for  ODEs, as we need to ensure that the process is still adapted.
  \end{remark}
  \begin{definition}
    Let $\vf{X}$ be the solution to \mcref{MM:SDE}. We define the \emph{continuous Euler scheme} as:
    $$
      \vf{\tilde{X}}_t:=\vf{x}_0+\int_0^t\vf{b}(\vf{X}_{\phi_s})\dd{s}+\int_0^t\vf{\sigma}(\vf{X}_{\phi_s})\dd{\vf{B}_s}
    $$
    where $\phi_s:=\max\{ t_i: t_i< s\}$.
  \end{definition}
  \begin{lemma}
    Let $X$ be the solution to \mcref{MM:SDE} with $d=1$ and $\tilde{X}^m$ be the solution to the Euler scheme. Then, for $p\geq 1$:
    $$
      \sup_{n\in\NN}\Exp\left( \sup_{0\leq t\leq T}\abs{\tilde{X}^m_t}^p\right)<\infty
    $$
  \end{lemma}
  \begin{lemma}
    Let $X$ be the solution to \mcref{MM:SDE} with $d=1$ and $\tilde{X}^m$ be the solution to the Euler scheme. Then, for $p\geq 1$:
    $$
      \max_{i=0,\ldots,m-1}{\Exp\left(\sup_{t_i\leq t\leq t_{i+1}}\abs{X_t-\tilde{X}^m_{\phi_t}}^p\right)}^\frac{1}{p}\leq \frac{C}{\sqrt{m}}
    $$
  \end{lemma}
  \begin{theorem}[Strong error of the Euler scheme]
    Let $X$ be the solution to \mcref{MM:SDE} with $d=1$ and $\tilde{X}^m$ be the solution to the Euler scheme. Then, for $p\geq 1$:
    $$
      {\Exp\left(\sup_{0\leq t\leq T}\abs{X_t-\tilde{X}^m_t}^p\right)}^{1/p}\leq \frac{C}{\sqrt{m}}
    $$
  \end{theorem}
  \begin{theorem}[Weak error of the Euler scheme]
    If $\vf{b},\vf\sigma\in \mathcal{C}^\infty(\RR^d)$ with bounded derivatives and either one of the following conditions holds:
    \begin{itemize}
      \item $g\in \mathcal{C}^\infty_\text{b}(\RR^d)$
      \item $g$ is measurable with polynomial growth and $\vf\sigma$ is uniformly elliptic, i.e.\ $\exists\lambda>0$ such that $\forall \vf{x},\vf\xi\in\RR^d$ we have:
            $$
              \transpose{\vf\xi}(\vf{\sigma}\transpose{\vf{\sigma}})(\vf{x})\vf\xi\geq \lambda\norm{\vf\xi}^2
            $$
    \end{itemize}
    Then:
    $$
      \abs{\Exp(g(\tilde{X}^m_T))- \Exp(g(X_T))}\leq \frac{C}{m}
    $$
    where $T$ is the final time of the simulation.
  \end{theorem}
  \begin{remark}
    We can write the error taking in the Montecarlo estimation using the Euler scheme as:
    \begin{multline*}
      \frac{1}{n}\sum_{i=1}^n g(\tilde{X}^{m,(i)}_T)-\Exp(g(X_T))=\\=\frac{1}{n}\sum_{i=1}^n g(\tilde{X}^{m,(i)}_T)-\Exp(g(\tilde{X}^m_T))+\Exp(g(\tilde{X}^m_T))-\Exp(g(X_T))
    \end{multline*}
    where $\tilde{X}^{m,(i)}_T$ are \iid copies of $\tilde{X}^m_T$. This error consists in two terms:
    \begin{itemize}
      \item The statistical error: $$\frac{1}{n}\sum_{i=1}^n g(\tilde{X}^{m,(i)}_T)-\Exp(g(\tilde{X}^m_T))\sim \frac{1}{\sqrt{n}}$$
      \item The discretization error:$$\Exp(g(\tilde{X}^m_T))-\Exp(g(X_T))\sim \frac{1}{m}$$
    \end{itemize}
    This suggests choosing $m\sim \sqrt{n}$. However, confidence intervals are of little interest in this setting as the amplitude of the bias is similar to the length of the confidence interval. Thus, we have no control on the error a posteriori. We conclude that it is best to be in between the setting $\sqrt{n}\ll m$ to have meaningful confidence intervals and $\sqrt{n}\sim m$ for efficiency, say $n^{\frac{1}{2}+\delta}\sim m$ for $\delta>0$ small.
  \end{remark}
  \begin{corollary}[Romberg Extrapolation]
    The \emph{Romberg Extrapolation} consists in the following result:
    $$
      \Exp\left(2g(\tilde{X}^m_T)-g(\tilde{X}^m_T)\right)-\Exp(g(X_T))= \frac{C}{m^2}+\o{\frac{1}{m^2}}
    $$
    This suggests using the estimator for $\Exp(g(X_T))$:
    $$
      \frac{1}{n}\sum_{i=1}^n\left(2 g(\tilde{X}^{m,(i)}_T)-g(\tilde{X}^{m,(i)}_T)\right)
    $$
    where $\tilde{X}^{m,(i)}_T$ are \iid copies of $\tilde{X}^m_T$.
  \end{corollary}
  \subsection{Brownian bridge approach}
  \subsubsection{Brownian bridge}
  \begin{proposition}
    Let $X_t=x+\mu t+\sigma B_t$ with $\mu\in \RR$, $\sigma\ne 0$ and ${(B_t)}_{t\geq 0}$ be a Brownian motion. Then, given $(X_u,X_v)=(y,z)$, the process ${(X_t)}_{u\leq t\leq v}$ is a continuous Gaussian process, independent of ${(X_t)}_{t\leq u}$ and ${(X_t)}_{t\geq v}$, such that for all $u\leq t\leq s \leq v$:
    \begin{align*}
      \Exp(X_t\mid X_u=y, X_v=z)     & =y+\frac{t-u}{v-u}(z-y)         \\
      \cov(X_t,X_s\mid X_u=y, X_v=z) & =\frac{(t-u)(v-s)}{v-u}\sigma^2
    \end{align*}
  \end{proposition}
  \begin{corollary}
    Let $X_t=x+\mu t+\sigma B_t$ with $\mu\in \RR$, $\sigma\ne 0$ and ${(B_t)}_{t\geq 0}$ be a Brownian motion. Then, the distribution of the process ${(X_t)}_{u\leq t\leq v}$ given ${(X_t)}_{t\leq u}$ and ${(X_t)}_{t\geq v}$ is the same as the distribution of
    $$
      {(y+\sigma B_{t-u})}_{u\leq t\leq v}\quad\text{given } B_{v-u}=\frac{z-y}{\sigma}
    $$
  \end{corollary}
  \subsubsection{Exit times}
  In this section we are interested in approximate the quantity:
  $$
    \Exp\left(\indi{\tau>T}g(X_T) \right)
  $$
  where $\tau:=\inf\{ t\geq 0 : X_t\notin D\}$ and $D\subset \RR^d$ is an open connected set.
  \begin{remark}
    Due to \mcref{SC:feynman_kac}, we have that $\Exp\left(\indi{\tau>T}g(X_T) \right)$ is precisely the solution to the PDE:
    $$
      \begin{cases}
        \partial_t u+\vf{b}\cdot \grad u+\frac{1}{2}\trace(\vf{\sigma}\transpose{\vf{\sigma}}\laplacian u)=0 & \text{ in} [0,T)\times D \\
        u(T,\cdot)=g                                                                                         & \text{ in}x\in D
      \end{cases}
    $$
  \end{remark}
  \begin{proposition}
    Assume that:
    \begin{itemize}
      \item $\vf{b},\vf\sigma\in\mathcal{C}^3$ with bounded derivatives and $\vf\sigma$ uniformly elliptic.
      \item $D$ is a half-space or $\Fr{D}$ is bounded of class $\mathcal{C}^3$.
      \item $g$ is measurable with polynomial growth and vanishes at a neighborhood of $\Fr{D}$.
    \end{itemize}
    Then:
    $$
      \Exp\left(\indi{\tau>T}g(X_T) \right)-\Exp(\indi{\tilde\tau^m>T} g(\tilde{X}^m_T))=\O{\frac{1}{\sqrt{m}}}
    $$
    where $\tilde\tau^m:=\min\{ t_i: \tilde{X}^m_{t_i}\notin D\}$ and $\tilde{X}^m$ is the Euler scheme.
  \end{proposition}
  \begin{proposition}
    Assume that:
    \begin{itemize}
      \item $\vf{b},\vf\sigma\in\mathcal{C}^5$ with bounded derivatives and $\vf\sigma$ uniformly elliptic.
      \item $D$ is a half-space or $\Fr{D}$ is bounded of class $\mathcal{C}^5$.
      \item $g$ is measurable with polynomial growth and vanishes at a neighborhood of $\Fr{D}$.
    \end{itemize}
    Then:
    $$
      \Exp\left(\indi{\tau>T}g(X_T) \right)-\Exp(\indi{\tau^m>T} g(\tilde{X}^m_T))=\O{\frac{1}{m}}
    $$
  \end{proposition}
  \begin{proposition}
    Assume that $\mathcal{D}$ is a half-space with hyperplane orthogonal to $\vf\nu\in\RR^d$ passing by $\vf{z}\in\RR^d$, i.e.:
    $$
      \mathcal{D}=\{ \vf{y}\in\RR^d : \transpose{\vf\nu}(\vf{y}-\vf{z})>0\}
    $$
    If $\vf{x}_i,\vf{x}_{i+1}\in \mathcal{D}$, then:
    \begin{multline*}
      \Prob(\exists t\in [t_i,t_{i+1}], \tilde{X}_t^m\notin\mathcal{D}:\tilde{X}_{t_i}^m=\vf{x}_i, \tilde{X}_{t_{i+1}}^m=\vf{x}_{i+1})=\\=\exp{
        -2\frac{m}{T}\frac{\transpose{\vf\nu}(\vf{x}_{i}-\vf{z})\transpose{\vf\nu}(\vf{x}_{i+1}-\vf{z})}{\norm{\transpose{\vf\sigma(\vf{x}_i)}\vf\nu}^2}
      }
    \end{multline*}
  \end{proposition}
  \begin{lemma}
    Let ${(B_t)}_{t\geq 0}$ be a Brownian motion. Then, $\forall a\leq 0$ and $b\geq a$ we have:
    $$
      \Prob\left(\min_{t\in[0,h]}B_t\leq a\mid B_h=b\right)=\exp{-\frac{2}{h}a(a-b)}
    $$
  \end{lemma}
  \subsection{Computation of sensitivities}
  In this chapter we aim to construct Montecarlo methods in order to compute sensitivities of the price of an option. From the PDE point of view, we aim to compute the derivatives of the solution ${(\vf{X}_t^{\vf{x}})}_{t\geq 0}$ of the SDE:
  $$
    \begin{cases}
      \dd{\vf{X}_t}=\vf{b}(\vf{X}_t)\dd{t}+\vf{\sigma}(\vf{X}_t)\dd{\vf{B}_t} \\
      \vf{X}_0=\vf{x}
    \end{cases}
  $$
  \subsubsection{Finite difference method}
  Here we will focus on the case $d=1$.
  \begin{definition}
    Let $u(0,x):=\Exp(g(X_T^x))$ and $\tilde{u}^n(0,x):=\frac{1}{n}\sum_{i=1}^n g(\tilde{X}_T^{x,(i)})$ where $\tilde{X}_T^{x,(i)}$ are \iid copies of $\tilde{X}_T^x$. We define the \emph{finite difference estimator} as:
    \begin{align*}
      \partial_xu(0,x) & \approx \frac{u(0,x+\varepsilon) - u(0,x-\varepsilon)}{2\varepsilon}                     \\
                       & \approx \frac{\tilde{u}^n(0,x+\varepsilon) - \tilde{u}^n(0,x-\varepsilon)}{2\varepsilon}
    \end{align*}
  \end{definition}
  \begin{proposition}
    If $g$ is smooth enough, then:
    $$
      \Var\left( \frac{\tilde{u}^n(0,x+\varepsilon) - \tilde{u}^n(0,x-\varepsilon)}{2\varepsilon}\right)\approx \frac{1}{n}\Var(g'(X_T^x))
    $$
  \end{proposition}
  \begin{remark}
    When $g$ is irregular, the optimal choice of $\varepsilon$ is not clear, as the bias increases with $\varepsilon$ and the variance increases as $\varepsilon$ decreases.
  \end{remark}
  \subsubsection{Black-Scholes model}
  \begin{proposition}
    Recall the one-dimensional \emph{Black-Scholes model}:
    $$
      \dd{X}_t=r X_t\dd{t}+\sigma X_t\dd{B}_t
    $$
    Then: $$
      \partial_x \Exp(g(X_T^x))=\frac{1}{\sigma x T}\Exp(g(X_T^x)B_T)
    $$
  \end{proposition}
  \subsubsection{Pathwise differentiation}
  \begin{theorem}
    If $\vf{b},\vf\sigma\in \mathcal{C}^2$ with bounded derivatives, then the flow ${\vf{x}}\mapsto \vf{X}_t^{\vf{x}}$ is $\mathcal{C}^1$ a.s.\ and the \emph{tangent process} $\vf{DX}^{\vf{x}}$ satisfies:
    $$
      \vf{DX}^{\vf{x}}_t=\vf{I}_d+\!\int_0^t\! \vf{Db}(\vf{X}_s^{\vf{x}})\vf{DX}_s^{\vf{x}}\dd{s}+\sum_{j=1}^{d}\int_0^t\! \vf{D\sigma}_j(\vf{X}_s^{\vf{x}})\vf{DX}_s^{\vf{x}}\dd{\vf{B}_s^j}
    $$
    where $\vf{D\sigma}_j$ is the $j$-th column of $\vf{D\sigma}$.
  \end{theorem}
  \begin{remark}
    In one dimension, we have:
    $$
      \partial_xX_t^x=1+\int_0^t b'(X_s^x)\grad X_s^x\dd{s}+\int_0^t \sigma'(X_s^x)\grad X_s^x\dd{B}_s
    $$
    which yields:
    $$
      \partial_xX_t^x\!=\!\expp{\int_0^t\!\left[b'(X_s^x)-\frac{1}{2}{\sigma'(X_s^x)}^2\right]\dd{s} \!+\!\! \int_0^t \sigma'(X_s^x)\dd{B}_s\!}
    $$
  \end{remark}
  \begin{proposition}
    If $\vf{b},\vf\sigma\in \mathcal{C}^2$ and $g\in\mathcal{C}^1$ with bounded derivatives, then:
    $$
      \grad \Exp(g(\vf{X}_T^{\vf{x}}))=\Exp\left(\grad g(\vf{X}_T^{\vf{x}})\grad \vf{X}_T^{\vf{x}}\right)
    $$
  \end{proposition}
  \begin{remark}
    In practice, to find the derivative of $\vf{X}_T^{\vf{x}}$ we proceed as in the deterministic case, i.e.\ solving the coupled variational equations.
  \end{remark}
  \subsubsection{Malliavin differentiation}
  \begin{proposition}
    If $\vf{b},\vf\sigma\in \mathcal{C}^2$ with bounded derivatives, $\vf\sigma$ is uniformly elliptic and $g$ is measurable with polynomial growth, then:
    $$
      \grad \Exp(g(\vf{X}_T^{\vf{x}}))=\Exp\left[g(\vf{X}_T^{\vf{x}})\frac{1}{T}\transpose{\left(\int_0^T\transpose{(\vf\sigma^{-1}(X_t^{\vf{x}})\vf{DX}_t^{\vf{x}})}\!\dd{\vf{B}_t}\!\right)\!\!}\right]
    $$
  \end{proposition}
  \subsection{American options}
  \begin{definition}
    In a frictionless market, the \emph{price of an American option} is given by:
    $$
      v(0,x)=\sup_{\tau\in\mathcal{T}_{0,T}}\Exp\left(\exp{-r\tau}g(X_\tau)\right)
    $$
    where $r$ is the \emph{risk-free interest rate}, $\mathcal{T}_{0,T}$ is the set of stopping times with values in $[0,T]$ and:
    $$
      \dd{X_t}=r X_t\dd{t}+\sigma (X_t)\dd{B}_t\quad X_0=x
    $$
  \end{definition}
  In this section we will introduce efficient algorithms to approximate the price of an American option.
  \subsubsection{Discretization}
  \begin{definition}
    Fix a time grid ${(t_i)}_{0\leq i\leq m}$ with $t_0=0$ and $t_m=T$. The \emph{discretization method} consists in replacing:
    \begin{enumerate}
      \item $\mathcal{T}_{0,T}$ by $\tilde{\mathcal{T}}_{0,T}^m$, the set of stopping times with values in ${(t_i)}_{0\leq i\leq m}$.
      \item $X$ by $\tilde{X}^m$, the Euler scheme.
    \end{enumerate}
  \end{definition}
  \begin{proposition}
    We can compute the price of an American option using the discretization method by the following recursive formula:
    $$
      \begin{cases}
        \tilde{v}^m(t_m,\tilde{X}^m_{t_m})=g(\tilde{X}^m_{t_m}) \\
        \begin{aligned}
          \tilde{v}^m & (t_i,\tilde{X}^m_{t_i})=                                                                                                                       \\
                      & =\max\left\{ g(\tilde{X}^m_{t_i}),\exp{-\frac{rT}{m}}\Exp\left(\tilde{v}^m(t_{i+1},\tilde{X}^m_{t_{i+1}})\mid \tilde{X}^m_{t_i}\right)\right\}
        \end{aligned}
      \end{cases}
    $$
  \end{proposition}
  \begin{proposition}
    If $g$ is Lipschitz continuous, then:
    $$
      \abs{v(0,x)-\tilde{v}^m(0,x)}\leq \frac{C}{\sqrt{m}}
    $$
  \end{proposition}
  \begin{remark}
    In the sequel, we assume that $r = 0$ and we write $X$ instead of $\tilde{X}^m$ for the sake of simplicity.
  \end{remark}
  \subsubsection{Naive approach}
  \begin{definition}
    The \emph{naive approach} consists in proceeding as follows:
    \begin{enumerate}
      \item Generate ${(X_{t_1}^j)}_{1\leq j\leq n}$ \iid copies of $X_{t_1}$ given $X_0=x$ and approximate:
            $$
              \tilde{v}^m(0,x)\approx\max \left\{ g(x),\frac{1}{n}\sum_{j=1}^n \tilde{v}^m(t_1,X_{t_1}^j)\right\}
            $$
      \item For each $1\leq j\leq n$, generate ${(X_{t_2}^{j,k})}_{1\leq k\leq n}$ \iid copies of $X_{t_2}$ given $X_{t_1}=X_{t_1}^j$ and approximate:
            $$
              \tilde{v}^m(t_1,X_{t_1}^j)\approx\max \left\{ g(X_{t_1}^j),\frac{1}{n}\sum_{k=1}^n \tilde{v}^m(t_2,X_{t_2}^{j,k})\right\}
            $$
      \item For each ${(j_1,\ldots,j_{m-1})}\in{\{1,\ldots,n\}}^{m-1}$, generate ${(X_{t_m}^{j_1,\ldots,j_{m-1},k})}_{1\leq k\leq n}$ \iid copies of $X_{t_m}$ given $X_{t_{m-1}}=X_{t_{m-1}}^{j_1,\ldots,j_{m-1}}$ and approximate:
            \begin{multline*}
              \tilde{v}^m(t_{m-1},X_{t_{m-1}}^{j_1,\ldots,j_{m-1}})\approx\\\approx\max\! \left\{\! g(X_{t_{m-1}}^{j_1,\ldots,j_{m-1}}),\frac{1}{n}\sum_{k=1}^n\! \tilde{v}^m(t_m,X_{t_m}^{j_1,\ldots,j_{m-1},k})\!\right\}
            \end{multline*}
    \end{enumerate}
    \begin{remark}
      This method provides a consistent estimator. However, it requires to generate $\sum_{i=1}^mn^i\sim n^m$ random variables. So the computational cost of the method increases exponentially with the number of exercise dates and becomes prohibitive for applications to the pricing of American options.
    \end{remark}
  \end{definition}
  \subsubsection{Regression methods}
  \begin{definition}[Tsitsiklis-Van Roy method]
    The \emph{Tsitsiklis-Van Roy method} consists in approximate the conditional expectation by a projection on a finite dimensional subspace of $L^2$. Namely, it holds:
    \begin{multline*}
      \Exp\left(\tilde{v}^m(t_{i+1},X_{t_{i+1}})\mid X_{t_i}\right)=\\=\argmin_{Y\in L^2(X_{t_i})}\Exp\left(\left(\tilde{v}^m(t_{i+1},X_{t_{i+1}})-Y\right)^2\right)
    \end{multline*}
    Here $L^2(X_{t_i})$ is the collection of square-integrable $\sigma(X_{t_i})$-measurable random variables. Then, we choose a family of basis functions $\vf\varphi=(\varphi_1,\ldots,\varphi_\ell)$ and approximate:
    $$
      \Exp\left(\tilde{v}^m(t_{i+1},X_{t_{i+1}})\mid X_{t_i}\right)\approx \sum_{j=1}^\ell \alpha_j^i\varphi_j(X_{t_i})
    $$
    where:
    $$
      \vf\alpha^i=\argmin_{\vf\alpha\in\RR^\ell}\Exp\left[\left(\tilde{v}^m(t_{i+1},X_{t_{i+1}})-\sum_{j=1}^\ell \alpha_j\varphi_j(X_{t_i})\right)^2\right]
    $$
    One can check that $$
      \vf\alpha^i={\Exp(\vf\varphi(X_{t_i})\transpose{\vf\varphi(X_{t_i})})}^{-1}\Exp(\vf\varphi(X_{t_i})\tilde{v}^m(t_{i+1},X_{t_{i+1}}))
    $$
    provided that $\Exp(\vf\varphi(X_{t_i})\transpose{\vf\varphi(X_{t_i})})$ is non-degenerate.
  \end{definition}
  \begin{proposition}
    An implementation of the Tsitsiklis-Van Roy method is as follows:
    \begin{enumerate}
      \item Generate ${(X_{t_1}^j,\ldots,X_{t_m}^j)}_{1\leq j\leq n}$ \iid copies of $(X_{t_1},\ldots,X_{t_m})$.
      \item Set $V_m^j=g(X_{t_m}^j)$ for all $1\leq j\leq n$.
      \item Recursively for $i=m-1,\ldots,1$, compute:
            $$
              \vf{\tilde{\alpha}}^i=\argmin_{\vf\alpha\in\RR^n}\frac{1}{n}\sum_{j=1}^n\left(V_{i+1}^j-\sum_{k=1}^\ell\alpha_k\varphi_k(X_{t_i}^j)\right)^2
            $$
            and set:
            $$
              V_i^j=\max\left\{ g(X_{t_i}^j),\sum_{k=1}^\ell\tilde{\alpha}_k^i\varphi_k(X_{t_i}^j)\right\}
            $$
      \item Set:
            $$V_0=\max\left\{ g(x),\frac{1}{n}\sum_{j=1}^n V_1^j\right\}$$
    \end{enumerate}
  \end{proposition}
  \begin{theorem}
    If $$
      \Exp(\tilde{v}^m(t_{i+1},X_{t_{i+1}})\mid X_{t_i})=\sum_{j=1}^\ell \alpha_j^i\varphi_j(X_{t_i})
    $$
    then the Tsitsiklis-Van Roy estimator $V_0$ is consistent, i.e. $V_0\overset{\Prob}{\longrightarrow}v(0,x)$ as $n\to\infty$.
  \end{theorem}
  \begin{definition}[Longstaff-Schwartz method]
    The \emph{Longstaff-Schwartz method} consists in approximate the optimal stopping time instead of the value function itself. Recall that:
    $$
      \tilde{v}^m(0,x)=\sup_{\tau\in\tilde{\mathcal{T}}_{0,T}^m}\Exp\left(g(X_\tau)\right)=\Exp(g(X_{\tau^*}))
    $$
    where $$
      \tau^*=\inf\{t_i: g(X_{t_i})\geq \Exp(\tilde{v}^m(t_{i+1},X_{t_{i+1}})\mid X_{t_i})\}
    $$
  \end{definition}
  \begin{proposition}
    The implementation of the Longstaff-Schwartz method is as follows:
    \begin{enumerate}
      \item Generate ${(X_{t_1}^j,\ldots,X_{t_m}^j)}_{1\leq j\leq n}$ \iid copies of $(X_{t_1},\ldots,X_{t_m})$.
      \item Define the stopping rule $\tilde{\tau}_m=t_m$ and apply it to the trajectories simulated just before, i.e.\ set $V_m^j=g(X_{\tilde{\tau}_m}^j)=g(X_{t_m}^j)$ for all $1\leq j\leq n$.
      \item Recursively for $i=m-1,\ldots,1$, compute:
            $$
              \vf{\tilde\alpha}^i=\argmin_{\vf\alpha\in\RR^n}\frac{1}{n}\sum_{j=1}^n\left(V_{i+1}^j-\sum_{k=1}^\ell\alpha_k\varphi_k(X_{t_i}^j)\right)^2
            $$
            Then, define for any sample path $(X_{t_1},\ldots,X_{t_m})$, the stopping rule:
            $$
              \tilde{\tau}_i=\begin{cases}
                t_i                & \text{ if } g(X_{t_i})\geq \sum_{k=1}^\ell\tilde{\alpha}_k^i\varphi_k(X_{t_i}) \\
                \tilde{\tau}_{i+1} & \text{ otherwise}
              \end{cases}
            $$
            and apply it to the trajectories simulated just before, i.e.\ set for $1\leq j\leq n$:
            $$
              V_i^j=\begin{cases}
                g(X_{t_i}^j) & \text{ if } g(X_{t_i}^j)\geq \sum_{k=1}^\ell\tilde{\alpha}_k^i\varphi_k(X_{t_i}^j) \\
                V_{i+1}^j    & \text{ otherwise}
              \end{cases}
            $$
      \item Define the stopping rule
            $$
              \tilde{\tau}_0=\begin{cases}
                0              & \text{ if } g(x)\geq \frac{1}{n}\sum_{j=1}^n V_1^j \\
                \tilde{\tau}_1 & \text{ otherwise}
              \end{cases}
            $$
            and apply it to the trajectories simulated just before, i.e.\ set:
            $$
              V_0=\frac{1}{n} \sum_{j=1}^ng(X_{\tilde{\tau}_0}^j)=\max\left\{ g(x),\frac{1}{n}\sum_{j=1}^n V_1^j\right\}
            $$
    \end{enumerate}
  \end{proposition}
  \begin{theorem}
    If $$
      \Exp(\tilde{v}^m(t_{i+1},X_{t_{i+1}})\mid X_{t_i})=\sum_{j=1}^\ell \alpha_j^i\varphi_j(X_{t_i})
    $$
    then the Longstaff-Schwartz estimator $V_0$ is consistent, i.e. $V_0\overset{\Prob}{\longrightarrow}v(0,x)$ as $n\to\infty$. Otherwise, the limit corresponds to the value of the option under a sub-optimal stopping rule and so it underestimates the
    true price.
  \end{theorem}
  \begin{remark}
    However, when $n$ is finite, $\tilde{\tau}_0$ is not a stopping time since it uses information about the future. Thus, we should add a fifth step to the algorithm:
    \begin{enumerate}
      \setcounter{enumi}{4}
      \item Generate ${(X_{t_1}^{n+j},\ldots,X_{t_m}^{n+j})}_{1\leq j\leq \tilde{n}}$ \iid copies of $(X_{t_1},\ldots,X_{t_m})$ and apply the stopping rule $\tilde{\tau}_0$ to these new trajectories, i.e. set:
            $$
              \underline{V}_0=\frac{1}{\tilde{n}} \sum_{j=1}^{\tilde{n}}g(X_{\tilde{\tau}_0}^{n+j})
            $$
    \end{enumerate}
  \end{remark}
  \begin{lemma}[Rogers's lemma]
    We have:
    $$
      v(0,x)=\inf_{M\in\mathcal{M}_{0,T}}\Exp\left(\sup_{t\in[0,T]}g(X_t)-M_t\right)
    $$
    where $\mathcal{M}_{0,T}$ is the set of continuous martingales on $[0,T]$.
  \end{lemma}
  \begin{remark}
    Roughly speaking, we can construct a nearly optimal martingale $\tilde{M}$ of the problem above and simulate \iid copies of $(X,\tilde{M})$ to compute the Monte Carlo estimator:
    $$
      \overline{V}_0=\frac{1}{\tilde{n}} \sum_{j=1}^{\tilde{n}}\sup_{t\in[0,T]}g(X_t^{j})-\tilde{M}_t^{j}
    $$
    This provides a confidence interval for the true price given by:
    \begin{multline*}
      \Bigg[\overline{V}_0-z_{1-\frac{\alpha}{2}}\sqrt{\frac{\Var(g(X_{\tilde{\tau}_0}))}{\tilde{n}}},\\\overline{V}_0+z_{1-\frac{\alpha}{2}}\sqrt{\frac{\Var\left(\sup_{t\in[0,T]}\{g(X_t)-\tilde{M}_t\}\right)}{\tilde{n}}}\Bigg]
    \end{multline*}
  \end{remark}
\end{multicols}
\end{document}